{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOoU8apO+y29rIxn/FsWBno",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasCovarrubias/jaxecon/blob/main/Value_Function_Iteration_with_TPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Programming in TPUs Using JAX"
      ],
      "metadata": {
        "id": "9hDVZWighqhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab notebook explores best practices to parallelize code using JAX, with a focus on how to leverage TPUs. Our starting point is Sargent and Stachurski's notebook on how to use JAX and GPUS to perform heaviliy optimized vectorization https://notes.quantecon.org/submission/622ed4daf57192000f918c61. For large state spaces, we get a 10x speed gain when using TPUs. I show that in order to get such speed bump we need to, in addition to vectorize, parallelize the value function update across the 8 cores of a TPU. Also, I show that automatic vectorization using jax.vmap gets close in terms of speed to manual vectorization, while being easier to get right and more generally applicable. "
      ],
      "metadata": {
        "id": "CRO5lTwrhyCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem is to maximize the expected discounted sum\n",
        "\n",
        "$$ 𝔼\\sum_{t≥0} β^t u(c_t)$$\n",
        "\n",
        "subject to\n",
        "\n",
        "$$c_t+a_t+1≤Ra_t+yt, \\quad c_t≥0,\\quad a_t≥0$$\n",
        "\n",
        "for all $t≥0$, with $a_0$ and $y_0$ given. Here $c_t$ is consumption, $a_t$ is assets, $R$ is the gross risk-free rate of return, and $y_t$ is income. The income process follows a Markov chain with transition matrix $P$.\n",
        "\n",
        "The Bellman equation is\n",
        "\n",
        "$$v(a,y)= \\underset{0≤a′≤Ra+y}{\\max} \\{ u(Ra+y−a′)+β\\sum_{y′}v(a′,y′)P(y,y′) \\}$$\n",
        "\n",
        "where $v$ is the value function. The corresponding Bellman operator is\n",
        "\n",
        "$$Tv(a,y)= \\underset{0≤a′≤Ra+y}{\\max} \\{ u(Ra+y−a′)+β\\sum_{y′}v(a′,y′)P(y,y′) \\}$$\n",
        "\n",
        "We solve the dynamic program by value function iteration --- that is, by iterating with T.\n"
      ],
      "metadata": {
        "id": "DuQOCHhqiXl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a preview of the time (in secods) that it take to do un update of the value function under the different parallelization schemes we will review:\n",
        "\n",
        "State Space size $\\to$  |\t131,072\t|\t2,097,152\t|\t8,388,608\t|\t33,554,432\t|\t134,217,728\n",
        "-------------------|-------|-------|-------|-------|-------\n",
        "GPU, manual vectorization | 0.0040\t|\t0.1979\t|\t1.5700\t|\t12.1128\t|\t97.4787\n",
        "GPU, auto. vectorization |\t0.0048\t|\t0.2975\t|\t2.3174\t|\t20.6630\t|\t224.7583\n",
        "TPU, manual vectorization | **0.0033**\t|\t0.0988\t|\t0.7798\t|\t6.2468\t|\t49.9455\n",
        "TPU, auto. vectorization | 0.0034\t|\t0.1263\t|\t0.9292\t|\t7.4162\t|\t59.2860 \n",
        "TPU parallelization | 0.0272\t|\t**0.0562**\t|\t**0.2108**\t|\t**1.2765**\t|\t**9.7084** \n",
        "\n",
        "The header of each column represents the size of the state space. It is worth noticing that increase the size of the grid for $a$ and $y$ not only increase the number of points in the state space grid but also increase the cost of the update in each point, because the number of possible $a'$ also increase and calculating the expectation term requires more computation. \n"
      ],
      "metadata": {
        "id": "aq3TbaWeGrn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by installing quantecon and importing the libraries we will use and configuring TPU if needed."
      ],
      "metadata": {
        "id": "0VGwrYCDnece"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installs\n",
        "!pip install -U quantecon # Install quantecon in case it's missing\n",
        "\n",
        "#imports\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from numba import njit\n",
        "import quantecon as qe \n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "from jax.config import config\n",
        "config.update(\"jax_log_compiles\", 1)\n",
        "# to suppress watnings uncomment next two lines\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "#chage to False if using CPU or GPU runtime.\n",
        "use_TPU = False\n",
        "if use_TPU:\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  !nvidia-smi\n"
      ],
      "metadata": {
        "id": "LQqkG9rlkPy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5219e47d-c3f9-47da-cb7d-96da47cf8f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting quantecon\n",
            "  Downloading quantecon-0.5.3-py3-none-any.whl (179 kB)\n",
            "\u001b[K     |████████████████████████████████| 179 kB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.21.6)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from quantecon) (0.51.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from quantecon) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.4.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->quantecon) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->quantecon) (57.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (2022.6.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->quantecon) (1.2.1)\n",
            "Installing collected packages: quantecon\n",
            "Successfully installed quantecon-0.5.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul  7 18:11:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case you chose a GPU runtime, if you don't use Colab Pro, you should get something lika a Tesla K80. Since I am using Colab Pro, I got a Tesla P100."
      ],
      "metadata": {
        "id": "Lmfv1mhTJF66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting Point: Naive implementation on a small grid"
      ],
      "metadata": {
        "id": "ijfXQhUfIyqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we specify global parameters (we will then move it to local), the grids for the state and actions, and the inital value for the value function. "
      ],
      "metadata": {
        "id": "u2Joo0ilkTRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grid for assets\n",
        "#scale = 1/64\n",
        "scale = 32\n",
        "\n",
        "a_min, a_max = 0.01, 2\n",
        "ρ = 0.9\n",
        "σ = 0.1\n",
        "a_size = ap_size = int(1024*scale)\n",
        "a_grid = jnp.linspace(a_min, a_max, a_size)  # grid for a\n",
        "ap_grid = jnp.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "#grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "ρ = 0.9\n",
        "σ = 0.1\n",
        "y_size = int(128*scale)\n",
        "mc = qe.tauchen(ρ, σ, n=y_size)\n",
        "y_grid = jnp.exp(mc.state_values)\n",
        "P = jnp.array(mc.P)\n",
        "\n",
        "# Organize in dictionaries\n",
        "params = {\n",
        "    \"R\": 1.1,\n",
        "    \"beta\": 0.99,\n",
        "    \"gamma\": 2.5\n",
        "}\n",
        "\n",
        "grids = {\n",
        "  \"a\": a_grid,\n",
        "  \"y\": y_grid,\n",
        "  \"ap\": ap_grid,}\n",
        "\n",
        "model = {\"params\": params, \n",
        "        \"grids\": grids, \n",
        "        \"Trans_matrix\": P, \n",
        "        \"indices\": {\"a\": jnp.array(range(a_size)), \"y\": jnp.array(range(y_size)), \"ap\": jnp.array(range(ap_size))},\n",
        "         \"batched_grids\": {}}\n",
        "\n",
        "# initial value\n",
        "v_init = np.zeros((a_size, y_size))"
      ],
      "metadata": {
        "id": "Fz8O9k67kZxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29bf652-05ff-42c7-d134-8d5a82d21727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.0062677860260009766 sec\n",
            "WARNING:absl:Compiling _linspace (139608449852336 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.07455158233642578 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.0004894733428955078 sec\n",
            "WARNING:absl:Compiling <lambda> (139608840627216 for args (ShapedArray(float32[4096]),).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.12166714668273926 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003921985626220703 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00037741661071777344 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002968311309814453 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to start by using a nainve approach that uses for loops extensively. This structure is going t o be the base of our \"authomatically vectorized\" version."
      ],
      "metadata": {
        "id": "2sEZEyoRGinJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def T_naive(v, model):\n",
        "  params = model[\"params\"]\n",
        "  grids = model[\"grids\"]\n",
        "  P = model[\"Trans_matrix\"]\n",
        "  \n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  \"The Bellman operator.\"\n",
        "  # Allocate memory\n",
        "  v_new = np.empty_like(v)\n",
        "  # Step through all states\n",
        "  for i, a in enumerate(a_grid):\n",
        "      for j, y in enumerate(y_grid):\n",
        "          # Choose a' optimally by stepping through all possible values\n",
        "          v_max = - np.inf\n",
        "          for k, ap in enumerate(ap_grid):\n",
        "              c = params[\"R\"] * a + y - ap\n",
        "              if c > 0:  \n",
        "                  # Calculate the right hand side of the Belllman operator\n",
        "                  val = u(c) + params[\"beta\"] * np.dot(v[k, :], P[j, :])\n",
        "                  if val > v_max:\n",
        "                      v_max = val\n",
        "          v_new[i, j] = v_max\n",
        "  return v_new\n",
        "\n",
        "%time T_naive(v_init, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyjlPg9DGo7_",
        "outputId": "5125514a-304c-4e3f-b6ac-b181bbcfb003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming _unstack for jit in 0.004049062728881836 sec\n",
            "WARNING:absl:Compiling _unstack (140706578708912 for args (ShapedArray(float32[16]),).\n",
            "WARNING:absl:Finished XLA compilation of _unstack in 0.040812015533447266 sec\n",
            "WARNING:absl:Finished tracing + transforming _unstack for jit in 0.001270294189453125 sec\n",
            "WARNING:absl:Compiling _unstack (140706578708912 for args (ShapedArray(float32[2]),).\n",
            "WARNING:absl:Finished XLA compilation of _unstack in 0.02381110191345215 sec\n",
            "WARNING:absl:Finished tracing + transforming fn for jit in 0.0007047653198242188 sec\n",
            "WARNING:absl:Compiling fn (140706578708048 for args (ShapedArray(float32[]), ShapedArray(float32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of fn in 0.02022862434387207 sec\n",
            "WARNING:absl:Finished tracing + transforming fn for jit in 0.0005855560302734375 sec\n",
            "WARNING:absl:Compiling fn (140706578708144 for args (ShapedArray(float32[]), ShapedArray(float32[])).\n",
            "WARNING:absl:Finished XLA compilation of fn in 0.022016525268554688 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.0005757808685302734 sec\n",
            "WARNING:absl:Compiling <lambda> (140706575833424 for args (ShapedArray(float32[]), ShapedArray(float32[])).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.022728681564331055 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003509521484375 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00036263465881347656 sec\n",
            "WARNING:absl:Compiling prim_fun (140706575867056 for args (ShapedArray(float32[]), ShapedArray(float32[])).\n",
            "WARNING:absl:Finished XLA compilation of gt in 0.02251887321472168 sec\n",
            "WARNING:absl:Finished tracing + transforming _power for jit in 0.0029370784759521484 sec\n",
            "WARNING:absl:Compiling _power (140706575867056 for args (ShapedArray(float32[]), ShapedArray(float32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _power in 0.023710966110229492 sec\n",
            "WARNING:absl:Finished tracing + transforming true_divide for jit in 0.0009534358978271484 sec\n",
            "WARNING:absl:Compiling true_divide (140706575867056 for args (ShapedArray(float32[]), ShapedArray(float32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of true_divide in 0.021477937698364258 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005881786346435547 sec\n",
            "WARNING:absl:Compiling prim_fun (140706575796656 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of lt in 0.02163243293762207 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006127357482910156 sec\n",
            "WARNING:absl:Compiling prim_fun (140706575782640 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of add in 0.020365238189697266 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006206035614013672 sec\n",
            "WARNING:absl:Compiling prim_fun (140706575864656 for args (ShapedArray(bool[], weak_type=True), ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of select_n in 0.0206146240234375 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005395412445068359 sec\n",
            "WARNING:absl:Compiling prim_fun (140706575867632 for args (ShapedArray(int32[], weak_type=True),).\n",
            "WARNING:absl:Finished XLA compilation of convert_element_type in 0.02097296714782715 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006091594696044922 sec\n",
            "WARNING:absl:Compiling prim_fun (140706575867056 for args (ShapedArray(int32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.0194704532623291 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0008671283721923828 sec\n",
            "WARNING:absl:Compiling prim_fun (140706575796848 for args (ShapedArray(float32[2,2]), ShapedArray(int32[1])).\n",
            "WARNING:absl:Finished XLA compilation of gather in 0.02220749855041504 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005879402160644531 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004439353942871094 sec\n",
            "WARNING:absl:Compiling prim_fun (140706575867632 for args (ShapedArray(float32[], weak_type=True),).\n",
            "WARNING:absl:Finished XLA compilation of convert_element_type in 0.019938945770263672 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.63 s, sys: 11.8 s, total: 18.5 s\n",
            "Wall time: 12.7 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.86624002, -0.23726241],\n",
              "       [-1.27407837, -0.21337929],\n",
              "       [-0.93996847, -0.19325423],\n",
              "       [-0.73004103, -0.17609487],\n",
              "       [-0.58810759, -0.16135961],\n",
              "       [-0.48685074, -0.14854133],\n",
              "       [-0.41164595, -0.13733099],\n",
              "       [-0.354004  , -0.12747793],\n",
              "       [-0.30863982, -0.1187598 ],\n",
              "       [-0.27222025, -0.1109712 ],\n",
              "       [-0.24243763, -0.10400578],\n",
              "       [-0.21771047, -0.09774285],\n",
              "       [-0.19692048, -0.09208323],\n",
              "       [-0.17923257, -0.08694786],\n",
              "       [-0.164065  , -0.08227308],\n",
              "       [-0.15090549, -0.07800293]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual vectorization: Reshape grids and transition matrix\n",
        "\n",
        "\n",
        "We add dimensions to arrays so that they will be stretched along the new dimensions when placed in arithmetic operations with other arrays that have more elements along those dimensions. This stretching is done by repeating values, which is what we use to replace loops.\n",
        "\n",
        "The next code cell reshapes all arrays to be three-dimensional."
      ],
      "metadata": {
        "id": "6W6SRD3FmRD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model[\"batched_grids\"] = {\n",
        "    \"P\": jnp.reshape(P, (y_size, y_size, 1)),\n",
        "    \"a\": jnp.reshape(a_grid, (a_size, 1, 1)),\n",
        "    \"y\": jnp.reshape(y_grid, (1, y_size, 1)),\n",
        "    \"ap\": jnp.reshape(ap_grid, (1, 1, ap_size)),\n",
        "}"
      ],
      "metadata": {
        "id": "FkjoN5cfmm0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee702d0-9253-47a4-b4a4-74c498b06b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005838871002197266 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140389008 for args (ShapedArray(float32[2,2]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0044863224029541016 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005173683166503906 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140387088 for args (ShapedArray(float32[16]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003246307373046875 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003857612609863281 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140375568 for args (ShapedArray(float32[2]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003700733184814453 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004076957702636719 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140357296 for args (ShapedArray(float32[16]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0031156539916992188 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can implement a vectorized version of the Bellman operator, which calculates the same values."
      ],
      "metadata": {
        "id": "BJBjSuCQm4MY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_T_manualvec(model: dict):\n",
        "  params = model[\"params\"]\n",
        "  a = model[\"batched_grids\"][\"a\"]\n",
        "  y = model[\"batched_grids\"][\"y\"]\n",
        "  ap = model[\"batched_grids\"][\"ap\"]\n",
        "  P = model[\"batched_grids\"][\"P\"]\n",
        "  \n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_manualvec(v):\n",
        "      vp = jnp.dot(v, P) # vp has shape (a_size, y_size, 1)\n",
        "      c = params[\"R\"] * a + y - ap # c has shape (a_size, y_size, ap_size)\n",
        "      # m = jnp.where(c > 0, u(c) + β * vp, -np.inf) # m has shape (a_size, y_size, ap_size) \n",
        "      m = u(c) + params[\"beta\"] * vp # m has shape (a_size, y_size, ap_size) \n",
        "      return jnp.max(m, axis=2) # we to average over the last axis, that is, for each a and y, get the the max over ap.\n",
        "    \n",
        "  return T_manualvec\n"
      ],
      "metadata": {
        "id": "gMVrgeNim435"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now wer are gonig to precompile the T operation so JAX optimize it use of the hardware."
      ],
      "metadata": {
        "id": "N4spbHVTm___"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T_manualvec = get_T_manualvec(model)\n",
        "T_manualvec_jit = jax.jit(T_manualvec).lower(v_init).compile()\n",
        "%time T_manualvec_jit(v_init).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeDrpKwbtsgQ",
        "outputId": "d48147f9-643f-49af-dcf6-f71c98a9de67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.008809804916381836 sec\n",
            "WARNING:absl:Compiling T_manualvec (139825198784240 for args (ShapedArray(float32[16,2]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.05184221267700195 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 451 µs, sys: 3.86 ms, total: 4.31 ms\n",
            "Wall time: 2.62 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[        nan, -0.23726259],\n",
              "             [        nan, -0.21338044],\n",
              "             [        nan, -0.1932523 ],\n",
              "             [        nan, -0.17610094],\n",
              "             [        nan, -0.16134453],\n",
              "             [        nan, -0.14853972],\n",
              "             [        nan, -0.13734336],\n",
              "             [        nan, -0.12748614],\n",
              "             [        nan, -0.11875407],\n",
              "             [        nan, -0.11097524],\n",
              "             [        nan, -0.10401004],\n",
              "             [-0.21771078, -0.09774413],\n",
              "             [-0.19691947, -0.09208304],\n",
              "             [-0.17923889, -0.08694804],\n",
              "             [-0.1640543 , -0.08227319],\n",
              "             [-0.1508989 , -0.07800273]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a Tesla P100 GPU, we get 30ms. With TPU, we get 16.9"
      ],
      "metadata": {
        "id": "5iNU6avAozFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authomatic vecotrization using vmap\n",
        "\n",
        "When we calculate the value function at each point of the grid, we want to pass all the possible actions as a vector, and then calculate the maximum. In order to get the actio_value for all the possible actions, we are going to use vmap."
      ],
      "metadata": {
        "id": "KTHLO-DHMv1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we calculate the value function at each point of the grid, we want to pass all the possible actions as a vector, and then calculate the maximum. In order to get the actio_value for all the possible actions, we are going to use vmap."
      ],
      "metadata": {
        "id": "sn5sngld7KX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_T_autovec(model:dict):\n",
        "  params = model[\"params\"]\n",
        "  grids = model[\"grids\"]\n",
        "  P = model[\"Trans_matrix\"]\n",
        "  \n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_autovec(v: jnp.ndarray):\n",
        "\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:],P[y_ind, :]) ,- jnp.inf)\n",
        "    \n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = model[\"indices\"][\"a\"]\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_state_value = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    return new_state_value\n",
        "\n",
        "  return T_autovec"
      ],
      "metadata": {
        "id": "v3zJaJk89npP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T_autovec = get_T_autovec(model)\n",
        "T_autovec_jit = jax.jit(T_autovec).lower(v_init).compile()"
      ],
      "metadata": {
        "id": "27x1EKcp95xh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3907a77e-7a1e-431e-e9aa-c486a7d1ef16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming T_autovec for jit in 0.029858827590942383 sec\n",
            "WARNING:absl:Compiling T_autovec (139825186948592 for args (ShapedArray(float32[16,2]),).\n",
            "WARNING:absl:Finished XLA compilation of T_autovec in 0.05750226974487305 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time T_autovec_jit(v_init).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks6iS0OVONFR",
        "outputId": "c8f0421c-0d58-4672-90f4-bf0a7d06b611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.41 ms, sys: 3.37 ms, total: 4.78 ms\n",
            "Wall time: 2.77 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-1.8662423 , -0.23726259],\n",
              "             [-1.2739494 , -0.21338044],\n",
              "             [-0.9399282 , -0.1932523 ],\n",
              "             [-0.7300421 , -0.17610094],\n",
              "             [-0.5881006 , -0.16134453],\n",
              "             [-0.48685533, -0.14853972],\n",
              "             [-0.41165787, -0.13734336],\n",
              "             [-0.3540047 , -0.12748614],\n",
              "             [-0.30865595, -0.11875407],\n",
              "             [-0.2722252 , -0.11097524],\n",
              "             [-0.24243681, -0.10401004],\n",
              "             [-0.21771078, -0.09774413],\n",
              "             [-0.19691947, -0.09208304],\n",
              "             [-0.17923889, -0.08694804],\n",
              "             [-0.1640543 , -0.08227319],\n",
              "             [-0.1508989 , -0.07800273]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To finish the exercise off, let's iterate until convergence."
      ],
      "metadata": {
        "id": "oyMHJQ4OpAfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vfi_iterator(v_init=v_init, tol=1e-6, max_iter=1435):\n",
        "    error = tol + 1\n",
        "    i = 0\n",
        "    v = v_init\n",
        "    # while error > tol and i < max_iter:\n",
        "    while i < max_iter:\n",
        "        new_v = T_autovec_jit(v)\n",
        "        error = jnp.max(jnp.abs(new_v - v))\n",
        "        v = new_v\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "        i += 1\n",
        "\n",
        "    if i == max_iter:\n",
        "        print(f\"Warning: iteration hit upper bound {max_iter}.\")\n",
        "    else:\n",
        "        print(f\"\\nConverged at iteration {i}.\")\n",
        "    return v\n",
        "\n",
        "%time v = vfi_iterator()"
      ],
      "metadata": {
        "id": "TqgYaYNFpDNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688b9242-69b4-4a8a-9276-c6ec942e752e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0\n",
            "Iteration 100\n",
            "Iteration 200\n",
            "Iteration 300\n",
            "Iteration 400\n",
            "Iteration 500\n",
            "Iteration 600\n",
            "Iteration 700\n",
            "Iteration 800\n",
            "Iteration 900\n",
            "Iteration 1000\n",
            "Iteration 1100\n",
            "Iteration 1200\n",
            "Iteration 1300\n",
            "Iteration 1400\n",
            "Warning: iteration hit upper bound 1435.\n",
            "CPU times: user 17.3 s, sys: 1min 18s, total: 1min 35s\n",
            "Wall time: 1min 33s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelizing across TPU cores using pmaps\n",
        "\n",
        "We are going to make a very small change to our update funciton so we can give a partition of the grid of a as an input and we get can update the value function only on those states. Then, with pmap, we will vectorize that function so we give a n array with n_device partitions of a and it will update those partitions of the state in parallel across TPU_cores."
      ],
      "metadata": {
        "id": "y2_IS1SzOinP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_T_tpu(model:dict):\n",
        "\n",
        "  params = model[\"params\"]\n",
        "  grids = model[\"grids\"]\n",
        "  P = model[\"Trans_matrix\"]\n",
        "  \n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_tpu(a_partition:jnp.array, v: jnp.ndarray):\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:], model[\"Trans_matrix\"][y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = a_partition\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_state_value = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    return new_state_value\n",
        "\n",
        "  return T_tpu"
      ],
      "metadata": {
        "id": "px-H46ldOnOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, we create an array with the partitions. The dimension of that array should be (n_cores,a_size/n_cores), so the leading axis organize the different partitions."
      ],
      "metadata": {
        "id": "HVlCCZV0PaYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = jax.local_device_count()\n",
        "a_partitions = jnp.reshape(model[\"indices\"][\"a\"], (n_devices, a_size//n_devices))"
      ],
      "metadata": {
        "id": "rky2k2rIPZh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7284c169-2e6a-48fc-c159-04353967d590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005376338958740234 sec\n",
            "WARNING:absl:Compiling prim_fun (139608449492368 for args (ShapedArray(int32[32768]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.004404544830322266 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are goingh to pmap the function so instead of accepting a single partition of a as an input, it takes n array with partitions."
      ],
      "metadata": {
        "id": "MBmdVcbMRHEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfetto profiling\n",
        "T_tpu = get_T_tpu(model)\n",
        "T_tpu_jit = jax.pmap(T_tpu, in_axes = (0,None)).lower(a_partitions, v_init).compile()\n",
        "with jax.profiler.trace(\"/tmp/jax-trace\"):\n",
        "  T_tpu_jit(a_partitions,v_init).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wET5GrxCRPKc",
        "outputId": "28e41273-8f2b-49f1-d682-5e36dd4b7f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming T_tpu for pmap in 0.032935380935668945 sec\n",
            "WARNING:absl:Compiling T_tpu (139608449493328) for 1 devices with args [ShapedArray(int32[1,32768]), ShapedArray(float32[32768,4096])]. (num_replicas=1 num_partitions=1)\n",
            "WARNING:absl:Finished XLA compilation of T_tpu in 4.413908004760742 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0007562637329101562 sec\n",
            "WARNING:absl:Compiling _multi_slice (139608840514160 for args (ShapedArray(int32[1,32768]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.0034847259521484375 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! tensorboard --logdir /tmp/jax-trace\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJqj33hn30rr",
        "outputId": "d240d1e0-eba5-4db8-e514-8b7885eb896d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.8.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('jax-trace', 'zip', '/tmp/jax-trace')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Lez1TtMO46Ae",
        "outputId": "1a849652-b78e-499f-f0a6-069b142f1a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/jax-trace.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a X2 increase in throughput vs the manually vectorized solution. "
      ],
      "metadata": {
        "id": "m2bt6XWsRiRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vfi_iterator(v_init=v_init, tol=1e-6, max_iter=1435):\n",
        "    error = tol + 1\n",
        "    i = 0\n",
        "    v = v_init\n",
        "    while error > tol and i < max_iter:\n",
        "    # while i < max_iter:\n",
        "        new_v = T_tpu_jit(a_partitions,v).reshape(a_size,y_size)\n",
        "        error = jnp.max(jnp.abs(new_v - v))\n",
        "        v = new_v\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "        i += 1\n",
        "\n",
        "    if i == max_iter:\n",
        "        print(f\"Warning: iteration hit upper bound {max_iter}.\")\n",
        "    else:\n",
        "        print(f\"\\nConverged at iteration {i}.\")\n",
        "    return v\n",
        "\n",
        "%time v = vfi_iterator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_Kd-64jzk2a",
        "outputId": "74ba08ff-b1c2-448f-ce71-ef55d51c36e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0\n",
            "Iteration 100\n",
            "Iteration 200\n",
            "Iteration 300\n",
            "Iteration 400\n",
            "Iteration 500\n",
            "Iteration 600\n",
            "\n",
            "Converged at iteration 601.\n",
            "CPU times: user 27.6 s, sys: 39.4 s, total: 1min 6s\n",
            "Wall time: 48 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarks\n",
        "\n",
        "Now we are going to store our results for different levels of scales"
      ],
      "metadata": {
        "id": "FFAPDQ5I2m09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_dict = {\"Size of grid\": [], \"Manual Vectorization\": [], \"Automatic Vectorization\": [], \"TPU Parallelization\": []}\n",
        "for scale in [1, 4, 8, 16, 32]:\n",
        "  # grid for assets\n",
        "  a_size = ap_size = 1024*scale\n",
        "  a_grid = jnp.linspace(a_min, a_max, a_size)  # grid for a\n",
        "  ap_grid = jnp.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "  #grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "  ρ = 0.9\n",
        "  σ = 0.1\n",
        "  y_size = 128*scale\n",
        "  mc = qe.tauchen(ρ, σ, n=y_size)\n",
        "  y_grid = jnp.exp(mc.state_values)\n",
        "  P = jnp.array(mc.P)\n",
        "  results_dict[\"Size of grid\"].append(a_size*y_size)\n",
        "\n",
        "  params = {\n",
        "      \"R\": 1.1,\n",
        "      \"beta\": 0.99,\n",
        "      \"gamma\": 2.5\n",
        "  }\n",
        "  grids = {\n",
        "    \"a\": a_grid,\n",
        "    \"y\": y_grid,\n",
        "    \"ap\": ap_grid,}\n",
        "\n",
        "  batched_grids = {\n",
        "    \"P\": jnp.reshape(P, (y_size, y_size, 1)),\n",
        "    \"a\": jnp.reshape(a_grid, (a_size, 1, 1)),\n",
        "    \"y\": jnp.reshape(y_grid, (1, y_size, 1)),\n",
        "    \"ap\": jnp.reshape(ap_grid, (1, 1, ap_size)),\n",
        "    } \n",
        "\n",
        "  model = {\"params\": params, \n",
        "          \"grids\": grids,\n",
        "           \"batched_grids\": batched_grids,\n",
        "          \"Trans_matrix\": P, \n",
        "          \"indices\": {\"a\": jnp.array(range(a_size)), \"y\": jnp.array(range(y_size)), \"ap\": jnp.array(range(ap_size))}\n",
        "           }\n",
        "\n",
        "  # initial value\n",
        "  v_init = jnp.zeros((a_size, y_size))\n",
        "\n",
        "  # Get and compile T functions\n",
        "  T_manualvec = get_T_manualvec(model)\n",
        "  T_manualvec_jit = jax.jit(T_manualvec).lower(v_init).compile()\n",
        "  T_autovec = get_T_autovec(model)\n",
        "  T_autovec_jit = jax.jit(T_autovec).lower(v_init).compile()\n",
        "  T_tpu = get_T_tpu(model)\n",
        "  a_partitions = jnp.reshape(model[\"indices\"][\"a\"], (n_devices, a_size//n_devices))\n",
        "  T_tpu_jit = jax.pmap(T_tpu, in_axes = (0,None)).lower(a_partitions, v_init).compile()\n",
        "  # run for 10 times and time it using timeit\n",
        "\n",
        "\n",
        "\n",
        "  results_dict[\"Manual Vectorization\"].append(timeit.timeit('T_manualvec_jit(v_init).block_until_ready()', globals=globals(), number=10)/10)\n",
        "  results_dict[\"Automatic Vectorization\"].append(timeit.timeit('T_autovec_jit(v_init).block_until_ready()', globals=globals(), number=10)/10)\n",
        "  results_dict[\"TPU Parallelization\"].append(timeit.timeit('T_tpu_jit(a_partitions, v_init).block_until_ready()', globals=globals(), number=10)/10)\n",
        "\n",
        "print(results_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6mw4oTZ2Kvj",
        "outputId": "71ba15a4-1ed3-46c9-93fd-6c9685824dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.008484125137329102 sec\n",
            "WARNING:absl:Compiling _linspace (139675140387184 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.0783088207244873 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.0005445480346679688 sec\n",
            "WARNING:absl:Compiling <lambda> (139675140386992 for args (ShapedArray(float32[128]),).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.11459469795227051 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003898143768310547 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006630420684814453 sec\n",
            "WARNING:absl:Compiling prim_fun (139675137021424 for args (ShapedArray(float32[128,128]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003975391387939453 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004298686981201172 sec\n",
            "WARNING:absl:Compiling prim_fun (139675137021424 for args (ShapedArray(float32[1024]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0029718875885009766 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.000385284423828125 sec\n",
            "WARNING:absl:Compiling prim_fun (139675137021424 for args (ShapedArray(float32[128]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0029745101928710938 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004096031188964844 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140119056 for args (ShapedArray(float32[1024]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0030584335327148438 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003094673156738281 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002923011779785156 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004067420959472656 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004658699035644531 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140119056 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.06913113594055176 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.010780096054077148 sec\n",
            "WARNING:absl:Compiling T_manualvec (139675140138768 for args (ShapedArray(float32[1024,128]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 2.217419385910034 sec\n",
            "WARNING:absl:Finished tracing + transforming T_autovec for jit in 0.040338754653930664 sec\n",
            "WARNING:absl:Compiling T_autovec (139675139838416 for args (ShapedArray(float32[1024,128]),).\n",
            "WARNING:absl:Finished XLA compilation of T_autovec in 0.28353190422058105 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005974769592285156 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140169808 for args (ShapedArray(int32[1024]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.00479888916015625 sec\n",
            "WARNING:absl:Finished tracing + transforming T_tpu for pmap in 0.03531026840209961 sec\n",
            "WARNING:absl:Compiling T_tpu (139675139702960) for 1 devices with args [ShapedArray(int32[1,1024]), ShapedArray(float32[1024,128])]. (num_replicas=1 num_partitions=1)\n",
            "WARNING:absl:Finished XLA compilation of T_tpu in 0.27845168113708496 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0008800029754638672 sec\n",
            "WARNING:absl:Compiling _multi_slice (139675140119056 for args (ShapedArray(int32[1,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.00454258918762207 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0005447864532470703 sec\n",
            "WARNING:absl:Compiling _multi_slice (139675140119056 for args (ShapedArray(float32[1024,128]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.003360748291015625 sec\n",
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.007162809371948242 sec\n",
            "WARNING:absl:Compiling _linspace (139675139702960 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.07665872573852539 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.00048661231994628906 sec\n",
            "WARNING:absl:Compiling <lambda> (139675139702960 for args (ShapedArray(float32[512]),).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.11437153816223145 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005533695220947266 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004634857177734375 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140354704 for args (ShapedArray(float32[512,512]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.00411224365234375 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004515647888183594 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140354704 for args (ShapedArray(float32[4096]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003010988235473633 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003628730773925781 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140354704 for args (ShapedArray(float32[512]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.002988576889038086 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004210472106933594 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140354704 for args (ShapedArray(float32[4096]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.002948284149169922 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00034332275390625 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00035691261291503906 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00036334991455078125 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140354704 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.0720522403717041 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.009527206420898438 sec\n",
            "WARNING:absl:Compiling T_manualvec (139675139436720 for args (ShapedArray(float32[4096,512]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.31351637840270996 sec\n",
            "WARNING:absl:Finished tracing + transforming T_autovec for jit in 0.03765273094177246 sec\n",
            "WARNING:absl:Compiling T_autovec (139675139437680 for args (ShapedArray(float32[4096,512]),).\n",
            "WARNING:absl:Finished XLA compilation of T_autovec in 0.3269667625427246 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0007607936859130859 sec\n",
            "WARNING:absl:Compiling prim_fun (139675139839664 for args (ShapedArray(int32[4096]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.004094362258911133 sec\n",
            "WARNING:absl:Finished tracing + transforming T_tpu for pmap in 0.03565359115600586 sec\n",
            "WARNING:absl:Compiling T_tpu (139675139739728) for 1 devices with args [ShapedArray(int32[1,4096]), ShapedArray(float32[4096,512])]. (num_replicas=1 num_partitions=1)\n",
            "WARNING:absl:Finished XLA compilation of T_tpu in 0.33272838592529297 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0008335113525390625 sec\n",
            "WARNING:absl:Compiling _multi_slice (139675139875472 for args (ShapedArray(int32[1,4096]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.004688262939453125 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0006084442138671875 sec\n",
            "WARNING:absl:Compiling _multi_slice (139675139875472 for args (ShapedArray(float32[4096,512]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.003229856491088867 sec\n",
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.0073642730712890625 sec\n",
            "WARNING:absl:Compiling _linspace (139675139839664 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.07465672492980957 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.000579833984375 sec\n",
            "WARNING:absl:Compiling <lambda> (139675137016368 for args (ShapedArray(float32[1024]),).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.11842894554138184 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003859996795654297 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005156993865966797 sec\n",
            "WARNING:absl:Compiling prim_fun (139675139453968 for args (ShapedArray(float32[1024,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.004168510437011719 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003895759582519531 sec\n",
            "WARNING:absl:Compiling prim_fun (139675139917872 for args (ShapedArray(float32[8192]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.004051923751831055 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006871223449707031 sec\n",
            "WARNING:absl:Compiling prim_fun (139675139453968 for args (ShapedArray(float32[1024]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.002916097640991211 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00047016143798828125 sec\n",
            "WARNING:absl:Compiling prim_fun (139675137019824 for args (ShapedArray(float32[8192]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0037889480590820312 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002810955047607422 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003421306610107422 sec\n",
            "WARNING:absl:Compiling prim_fun (139675141304400 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.07501363754272461 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.009218215942382812 sec\n",
            "WARNING:absl:Compiling T_manualvec (139675136666032 for args (ShapedArray(float32[8192,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.4638974666595459 sec\n",
            "WARNING:absl:Finished tracing + transforming T_autovec for jit in 0.039070844650268555 sec\n",
            "WARNING:absl:Compiling T_autovec (139675141304400 for args (ShapedArray(float32[8192,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of T_autovec in 0.4857215881347656 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005543231964111328 sec\n",
            "WARNING:absl:Compiling prim_fun (139675139436720 for args (ShapedArray(int32[8192]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003989458084106445 sec\n",
            "WARNING:absl:Finished tracing + transforming T_tpu for pmap in 0.0389552116394043 sec\n",
            "WARNING:absl:Compiling T_tpu (139675141306416) for 1 devices with args [ShapedArray(int32[1,8192]), ShapedArray(float32[8192,1024])]. (num_replicas=1 num_partitions=1)\n",
            "WARNING:absl:Finished XLA compilation of T_tpu in 0.43026018142700195 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0008599758148193359 sec\n",
            "WARNING:absl:Compiling _multi_slice (139675139805360 for args (ShapedArray(int32[1,8192]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.0049097537994384766 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0005118846893310547 sec\n",
            "WARNING:absl:Compiling _multi_slice (139675139805360 for args (ShapedArray(float32[8192,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.003642559051513672 sec\n",
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.00774693489074707 sec\n",
            "WARNING:absl:Compiling _linspace (139675139821392 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.07459425926208496 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.0005939006805419922 sec\n",
            "WARNING:absl:Compiling <lambda> (139675139821392 for args (ShapedArray(float32[2048]),).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.1155858039855957 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006084442138671875 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0009760856628417969 sec\n",
            "WARNING:absl:Compiling prim_fun (139675139821392 for args (ShapedArray(float32[2048,2048]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.004391908645629883 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003845691680908203 sec\n",
            "WARNING:absl:Compiling prim_fun (139675144468816 for args (ShapedArray(float32[16384]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0039637088775634766 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00039577484130859375 sec\n",
            "WARNING:absl:Compiling prim_fun (139675144468816 for args (ShapedArray(float32[2048]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003435373306274414 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.000415802001953125 sec\n",
            "WARNING:absl:Compiling prim_fun (139675144468816 for args (ShapedArray(float32[16384]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003865957260131836 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003612041473388672 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003151893615722656 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005867481231689453 sec\n",
            "WARNING:absl:Compiling prim_fun (139675139453968 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.07612800598144531 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.009059429168701172 sec\n",
            "WARNING:absl:Compiling T_manualvec (139675144465488 for args (ShapedArray(float32[16384,2048]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 1.1613154411315918 sec\n",
            "WARNING:absl:Finished tracing + transforming T_autovec for jit in 0.03946495056152344 sec\n",
            "WARNING:absl:Compiling T_autovec (139675142088752 for args (ShapedArray(float32[16384,2048]),).\n",
            "WARNING:absl:Finished XLA compilation of T_autovec in 1.2195868492126465 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006639957427978516 sec\n",
            "WARNING:absl:Compiling prim_fun (139675141201456 for args (ShapedArray(int32[16384]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.004704713821411133 sec\n",
            "WARNING:absl:Finished tracing + transforming T_tpu for pmap in 0.037138938903808594 sec\n",
            "WARNING:absl:Compiling T_tpu (139675140041616) for 1 devices with args [ShapedArray(int32[1,16384]), ShapedArray(float32[16384,2048])]. (num_replicas=1 num_partitions=1)\n",
            "WARNING:absl:Finished XLA compilation of T_tpu in 0.9156644344329834 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0011632442474365234 sec\n",
            "WARNING:absl:Compiling _multi_slice (139675141201456 for args (ShapedArray(int32[1,16384]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.005808115005493164 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0005679130554199219 sec\n",
            "WARNING:absl:Compiling _multi_slice (139675140040272 for args (ShapedArray(float32[16384,2048]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.003172636032104492 sec\n",
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.00782632827758789 sec\n",
            "WARNING:absl:Compiling _linspace (139675139437488 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.07608819007873535 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.0006036758422851562 sec\n",
            "WARNING:absl:Compiling <lambda> (139675142170000 for args (ShapedArray(float32[4096]),).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.057096004486083984 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004086494445800781 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0008463859558105469 sec\n",
            "WARNING:absl:Compiling prim_fun (139675142170000 for args (ShapedArray(float32[4096,4096]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.004689693450927734 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005166530609130859 sec\n",
            "WARNING:absl:Compiling prim_fun (139675124990128 for args (ShapedArray(float32[32768]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003145456314086914 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.000652313232421875 sec\n",
            "WARNING:absl:Compiling prim_fun (139675142170000 for args (ShapedArray(float32[4096]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0034265518188476562 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.000576019287109375 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140040368 for args (ShapedArray(float32[32768]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0031638145446777344 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00030684471130371094 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00034165382385253906 sec\n",
            "WARNING:absl:Compiling prim_fun (139675125043568 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.07559847831726074 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.00965571403503418 sec\n",
            "WARNING:absl:Compiling T_manualvec (139675125044048 for args (ShapedArray(float32[32768,4096]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 5.33627462387085 sec\n",
            "WARNING:absl:Finished tracing + transforming T_autovec for jit in 0.037474632263183594 sec\n",
            "WARNING:absl:Compiling T_autovec (139675140040368 for args (ShapedArray(float32[32768,4096]),).\n",
            "WARNING:absl:Finished XLA compilation of T_autovec in 4.66170859336853 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0008661746978759766 sec\n",
            "WARNING:absl:Compiling prim_fun (139675139875472 for args (ShapedArray(int32[32768]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.004261970520019531 sec\n",
            "WARNING:absl:Finished tracing + transforming T_tpu for pmap in 0.0369112491607666 sec\n",
            "WARNING:absl:Compiling T_tpu (139675137011632) for 1 devices with args [ShapedArray(int32[1,32768]), ShapedArray(float32[32768,4096])]. (num_replicas=1 num_partitions=1)\n",
            "WARNING:absl:Finished XLA compilation of T_tpu in 2.8449368476867676 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0011327266693115234 sec\n",
            "WARNING:absl:Compiling _multi_slice (139675122484912 for args (ShapedArray(int32[1,32768]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.0110931396484375 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0006017684936523438 sec\n",
            "WARNING:absl:Compiling _multi_slice (139675124990128 for args (ShapedArray(float32[32768,4096]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.003576993942260742 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Size of grid': [131072, 2097152, 8388608, 33554432, 134217728], 'Manual Vectorization': [0.004014438000001519, 0.19788137290000804, 1.5700353982000024, 12.1127872346, 97.47866143050001], 'Automatic Vectorization': [0.004789096399997561, 0.29747357039999545, 2.3173598094999988, 20.663012926700002, 224.7583419914], 'TPU Parallelization': [0.007923651699991296, 0.30005611290000617, 2.331630996399997, 20.0357178878, 223.66937429979998]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " For TPU, we get {'Size of grid': [131072, 2097152, 8388608, 33554432, 134217728], 'Manual Vectorization': [0.0032750082000120528, 0.09884197940000376, 0.7798380305999899, 6.246830163599998, 49.94550013509998], 'Automatic Vectorization': [0.0034154570000055175, 0.12627506110000014, 0.9292171296000106, 7.41619310469996, 59.2859591297], 'TPU Parallelization': [0.02720726330001071, 0.0561926443999937, 0.2108149590000039, 1.2764809615000103, 9.70835729370001]}\n",
        "\n",
        " For GPU{'Size of grid': [131072, 2097152, 8388608, 33554432, 134217728], 'Manual Vectorization': [0.004014438000001519, 0.19788137290000804, 1.5700353982000024, 12.1127872346, 97.47866143050001], 'Automatic Vectorization': [0.004789096399997561, 0.29747357039999545, 2.3173598094999988, 20.663012926700002, 224.7583419914], 'TPU Parallelization': [0.007923651699991296, 0.30005611290000617, 2.331630996399997, 20.0357178878, 223.66937429979998]}"
      ],
      "metadata": {
        "id": "d56TPREHHvC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-compiling the entire experiment (not just the update)"
      ],
      "metadata": {
        "id": "Utsxm-o9SUGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_learner_fn(model:dict, iterations):\n",
        "\n",
        "  def v_update(v: jnp.ndarray, error):\n",
        "    params = model[\"params\"]\n",
        "    grids = model[\"grids\"]\n",
        "    P = model[\"Trans_matrix\"]\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:], P[y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = model[\"indices\"][\"a\"]\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_v = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    new_error = jnp.max(jnp.abs(new_v - v))\n",
        "    return new_v, new_error\n",
        "\n",
        "  def learner_fn(v_init):  # repeat many times to avoid going back to Python.\n",
        "    return jax.lax.scan(v_update, v_init, None, length = iterations)\n",
        "\n",
        "  return learner_fn"
      ],
      "metadata": {
        "id": "z1wn2smYuQUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 1435\n",
        "learner_fn = get_learner_fn(model, n_iters)\n",
        "jitted_learner_fn = jax.jit(learner_fn).lower(v_init).compile()"
      ],
      "metadata": {
        "id": "iHqUnEGNuhxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a52788e-a698-4d4b-973a-68be15f1af1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming learner_fn for jit in 0.037129878997802734 sec\n",
            "WARNING:absl:Compiling learner_fn (140487308656176 for args (ShapedArray(float32[4112,256]),).\n",
            "WARNING:absl:Finished XLA compilation of learner_fn in 0.9671173095703125 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time value_final, error_stack = jitted_learner_fn(v_init)\n",
        "plt.plot(range(n_iters),error_stack)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "Z4G1_YNQu1Gd",
        "outputId": "2f97738a-fe0c-4d94-d6b5-6c201e5fa19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006163120269775391 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289207984 for args (ShapedArray(float32[1435]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.021751880645751953 sec\n",
            "WARNING:absl:Finished tracing + transforming divmod for jit in 0.005877256393432617 sec\n",
            "WARNING:absl:Compiling divmod (140487289263984 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of divmod in 0.03612828254699707 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00038170814514160156 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289264272 for args (ShapedArray(int32[]),).\n",
            "WARNING:absl:Finished XLA compilation of convert_element_type in 0.01756453514099121 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004987716674804688 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289263792 for args (ShapedArray(float32[1435]), ShapedArray(int32[])).\n",
            "WARNING:absl:Finished XLA compilation of dynamic_slice in 0.01903390884399414 sec\n",
            "WARNING:absl:Finished tracing + transforming _unstack for jit in 0.020872831344604492 sec\n",
            "WARNING:absl:Compiling _unstack (140487288808656 for args (ShapedArray(float32[100]),).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.7 s, sys: 1min 11s, total: 1min 27s\n",
            "Wall time: 1min 24s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished XLA compilation of _unstack in 0.11274552345275879 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004360675811767578 sec\n",
            "WARNING:absl:Compiling prim_fun (140487288868688 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of lt in 0.02319025993347168 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004856586456298828 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289263984 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of add in 0.018721342086791992 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005552768707275391 sec\n",
            "WARNING:absl:Compiling prim_fun (140487288808656 for args (ShapedArray(bool[], weak_type=True), ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of select_n in 0.02396559715270996 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004684925079345703 sec\n",
            "WARNING:absl:Compiling prim_fun (140489049604176 for args (ShapedArray(int32[], weak_type=True),).\n",
            "WARNING:absl:Finished XLA compilation of convert_element_type in 0.02097344398498535 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006616115570068359 sec\n",
            "WARNING:absl:Compiling prim_fun (140487288808656 for args (ShapedArray(int32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.018973112106323242 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0007677078247070312 sec\n",
            "WARNING:absl:Compiling prim_fun (140489049604176 for args (ShapedArray(float32[1435,1]), ShapedArray(int32[1])).\n",
            "WARNING:absl:Finished XLA compilation of gather in 0.0232388973236084 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0007376670837402344 sec\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc5c0297d50>]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}