{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMZXFIA+yP7FyokVAuaynKo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasCovarrubias/jaxecon/blob/main/Value_Function_Iteration_with_TPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Programming in TPUs Using JAX"
      ],
      "metadata": {
        "id": "9hDVZWighqhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab notebook explores best practices to parallelize code using JAX, with a focus on how to leverage TPUs. Our starting point is Sargent and Stachurski's notebook on how to use JAX and GPUS to perform heaviliy optimized vectorization https://notes.quantecon.org/submission/622ed4daf57192000f918c61. For large state spaces, we get a 10x speed gain when using TPUs. I show that in order to get such speed bump we need to, in addition to vectorize, parallelize the value function update across the 8 cores of a TPU. Also, I show that automatic vectorization using jax.vmap gets close in terms of speed to manual vectorization, while being easier to get right and more generally applicable."
      ],
      "metadata": {
        "id": "CRO5lTwrhyCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem is to maximize the expected discounted sum\n",
        "\n",
        "$$ 𝔼\\sum_{t≥0} β^t u(c_t)$$\n",
        "\n",
        "subject to\n",
        "\n",
        "$$c_t+a_t+1≤Ra_t+y_t, \\quad c_t≥0,\\quad a_t≥0$$\n",
        "\n",
        "for all $t≥0$, with $a_0$ and $y_0$ given. Here $c_t$ is consumption, $a_t$ is assets, $R$ is the gross risk-free rate of return, and $y_t$ is income. The income process follows a Markov chain with transition matrix $P$.\n",
        "\n",
        "The Bellman equation is\n",
        "\n",
        "$$v(a,y)= \\underset{0≤a′≤Ra+y}{\\max} \\{ u(Ra+y−a′)+β\\sum_{y′}v(a′,y′)P(y,y′) \\}$$\n",
        "\n",
        "where $v$ is the value function. The corresponding Bellman operator is\n",
        "\n",
        "$$Tv(a,y)= \\underset{0≤a′≤Ra+y}{\\max} \\{ u(Ra+y−a′)+β\\sum_{y′}v(a′,y′)P(y,y′) \\}$$\n",
        "\n",
        "We solve the dynamic program by value function iteration --- that is, by iterating with T.\n"
      ],
      "metadata": {
        "id": "DuQOCHhqiXl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a preview of the time (in secods) that it take to do un update of the value function under the different parallelization schemes we will review:\n",
        "\n",
        "State Space size $\\to$  |\t131,072\t|\t2,097,152\t|\t8,388,608\t|\t33,554,432\t|\t134,217,728\n",
        "-------------------|-------|-------|-------|-------|-------\n",
        "GPU, manual vectorization | 0.0040\t|\t0.1979\t|\t1.5700\t|\t12.1128\t|\t97.4787\n",
        "GPU, auto. vectorization |\t0.0048\t|\t0.2975\t|\t2.3174\t|\t20.6630\t|\t224.7583\n",
        "TPU, manual vectorization | **0.0033**\t|\t0.0988\t|\t0.7798\t|\t6.2468\t|\t49.9455\n",
        "TPU, auto. vectorization | 0.0034\t|\t0.1263\t|\t0.9292\t|\t7.4162\t|\t59.2860\n",
        "TPU parallelization | 0.0272\t|\t**0.0562**\t|\t**0.2108**\t|\t**1.2765**\t|\t**9.7084**\n",
        "\n",
        "The header of each column represents the size of the state space. It is worth noticing that increase the size of the grid for $a$ and $y$ not only increase the number of points in the state space grid but also increase the cost of the update in each point, because the number of possible $a'$ also increase and calculating the expectation term requires more computation.\n"
      ],
      "metadata": {
        "id": "aq3TbaWeGrn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by installing quantecon and importing the libraries we will use and configuring TPU if needed."
      ],
      "metadata": {
        "id": "0VGwrYCDnece"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installs\n",
        "!pip install -U quantecon # Install quantecon in case it's missing\n",
        "\n",
        "#imports\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from numba import njit\n",
        "import quantecon as qe\n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "from jax import config as jax_config\n",
        "jax_config.update(\"jax_log_compiles\", 1)\n",
        "# to suppress watnings uncomment next two lines\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "#chage to False if using CPU or GPU runtime.\n",
        "use_TPU = False\n",
        "if use_TPU:\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  !nvidia-smi\n"
      ],
      "metadata": {
        "id": "LQqkG9rlkPy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840389ca-106f-47af-8e64-27696248be85"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: quantecon in /usr/local/lib/python3.11/dist-packages (0.8.2)\n",
            "Requirement already satisfied: numba>=0.49.0 in /usr/local/lib/python3.11/dist-packages (from quantecon) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from quantecon) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from quantecon) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from quantecon) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from quantecon) (1.13.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.49.0->quantecon) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->quantecon) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->quantecon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->quantecon) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->quantecon) (2025.7.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->quantecon) (1.3.0)\n",
            "Sat Aug  2 13:01:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0             30W /   70W |   11432MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case you chose a GPU runtime, if you don't use Colab Pro, you should get something lika a Tesla K80. Since I am using Colab Pro, I got a Tesla P100."
      ],
      "metadata": {
        "id": "Lmfv1mhTJF66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting Point: Naive implementation on a small grid"
      ],
      "metadata": {
        "id": "ijfXQhUfIyqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we specify global parameters (we will then move it to local), the grids for the state and actions, and the inital value for the value function."
      ],
      "metadata": {
        "id": "u2Joo0ilkTRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grid for assets\n",
        "#scale = 1/64\n",
        "scale = 32\n",
        "\n",
        "a_min, a_max = 0.01, 2\n",
        "ρ = 0.9\n",
        "σ = 0.1\n",
        "a_size = ap_size = int(1024*scale)\n",
        "a_grid = jnp.linspace(a_min, a_max, a_size)  # grid for a\n",
        "ap_grid = jnp.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "#grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "y_size = int(128*scale)\n",
        "mc = qe.tauchen(y_size, ρ, σ)\n",
        "y_grid = jnp.exp(mc.state_values)\n",
        "P = jnp.array(mc.P)\n",
        "\n",
        "# Organize in dictionaries\n",
        "params = {\n",
        "    \"R\": 1.1,\n",
        "    \"beta\": 0.99,\n",
        "    \"gamma\": 2.5\n",
        "}\n",
        "\n",
        "grids = {\n",
        "  \"a\": a_grid,\n",
        "  \"y\": y_grid,\n",
        "  \"ap\": ap_grid,}\n",
        "\n",
        "model = {\"params\": params,\n",
        "        \"grids\": grids,\n",
        "        \"Trans_matrix\": P,\n",
        "        \"indices\": {\"a\": jnp.array(range(a_size)), \"y\": jnp.array(range(y_size)), \"ap\": jnp.array(range(ap_size))},\n",
        "         \"batched_grids\": {}}\n",
        "\n",
        "# initial value\n",
        "v_init = np.zeros((a_size, y_size))"
      ],
      "metadata": {
        "id": "Fz8O9k67kZxq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to start by using a nainve approach that uses for loops extensively. This structure is going t o be the base of our \"authomatically vectorized\" version."
      ],
      "metadata": {
        "id": "2sEZEyoRGinJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def T_naive(v, model):\n",
        "  params = model[\"params\"]\n",
        "  grids = model[\"grids\"]\n",
        "  P = model[\"Trans_matrix\"]\n",
        "\n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  \"The Bellman operator.\"\n",
        "  # Allocate memory\n",
        "  v_new = np.empty_like(v)\n",
        "  # Step through all states\n",
        "  for i, a in enumerate(a_grid):\n",
        "      for j, y in enumerate(y_grid):\n",
        "          # Choose a' optimally by stepping through all possible values\n",
        "          v_max = - np.inf\n",
        "          for k, ap in enumerate(ap_grid):\n",
        "              c = params[\"R\"] * a + y - ap\n",
        "              # Calculate the right hand side of the Belllman operator\n",
        "              val = u(c) + params[\"beta\"] * np.dot(v[k, :], P[j, :])\n",
        "          v_new[i, j] = val\n",
        "  return v_new\n",
        "\n",
        "%time T_naive(v_init, model)"
      ],
      "metadata": {
        "id": "GyjlPg9DGo7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual vectorization: Reshape grids and transition matrix\n",
        "\n",
        "\n",
        "We add dimensions to arrays so that they will be stretched along the new dimensions when placed in arithmetic operations with other arrays that have more elements along those dimensions. This stretching is done by repeating values, which is what we use to replace loops.\n",
        "\n",
        "The next code cell reshapes all arrays to be three-dimensional."
      ],
      "metadata": {
        "id": "6W6SRD3FmRD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model[\"batched_grids\"] = {\n",
        "    \"P\": jnp.reshape(P, (y_size, y_size, 1)),\n",
        "    \"a\": jnp.reshape(a_grid, (a_size, 1, 1)),\n",
        "    \"y\": jnp.reshape(y_grid, (1, y_size, 1)),\n",
        "    \"ap\": jnp.reshape(ap_grid, (1, 1, ap_size)),\n",
        "}"
      ],
      "metadata": {
        "id": "FkjoN5cfmm0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be5bc46-77e8-43ae-d8f0-814964de7377"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:2025-08-02 13:08:43,697:jax._src.dispatch:184: Finished tracing + transforming reshape for pjit in 0.000348330 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming reshape for pjit in 0.000348330 sec\n",
            "WARNING:2025-08-02 13:08:43,698:jax._src.interpreters.pxla:1911: Compiling reshape with global shapes and types [ShapedArray(float32[4096,4096])]. Argument mapping: (UnspecifiedValue,).\n",
            "WARNING:jax._src.interpreters.pxla:Compiling reshape with global shapes and types [ShapedArray(float32[4096,4096])]. Argument mapping: (UnspecifiedValue,).\n",
            "WARNING:2025-08-02 13:08:43,703:jax._src.dispatch:184: Finished jaxpr to MLIR module conversion jit(reshape) in 0.003479481 sec\n",
            "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(reshape) in 0.003479481 sec\n",
            "WARNING:2025-08-02 13:08:43,717:jax._src.dispatch:184: Finished XLA compilation of jit(reshape) in 0.012159109 sec\n",
            "WARNING:jax._src.dispatch:Finished XLA compilation of jit(reshape) in 0.012159109 sec\n",
            "WARNING:2025-08-02 13:08:43,720:jax._src.dispatch:184: Finished tracing + transforming reshape for pjit in 0.000622749 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming reshape for pjit in 0.000622749 sec\n",
            "WARNING:2025-08-02 13:08:43,721:jax._src.interpreters.pxla:1911: Compiling reshape with global shapes and types [ShapedArray(float32[32768])]. Argument mapping: (UnspecifiedValue,).\n",
            "WARNING:jax._src.interpreters.pxla:Compiling reshape with global shapes and types [ShapedArray(float32[32768])]. Argument mapping: (UnspecifiedValue,).\n",
            "WARNING:2025-08-02 13:08:43,725:jax._src.dispatch:184: Finished jaxpr to MLIR module conversion jit(reshape) in 0.002723217 sec\n",
            "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(reshape) in 0.002723217 sec\n",
            "WARNING:2025-08-02 13:08:43,738:jax._src.dispatch:184: Finished XLA compilation of jit(reshape) in 0.010880947 sec\n",
            "WARNING:jax._src.dispatch:Finished XLA compilation of jit(reshape) in 0.010880947 sec\n",
            "WARNING:2025-08-02 13:08:43,741:jax._src.dispatch:184: Finished tracing + transforming reshape for pjit in 0.000306606 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming reshape for pjit in 0.000306606 sec\n",
            "WARNING:2025-08-02 13:08:43,742:jax._src.interpreters.pxla:1911: Compiling reshape with global shapes and types [ShapedArray(float32[4096])]. Argument mapping: (UnspecifiedValue,).\n",
            "WARNING:jax._src.interpreters.pxla:Compiling reshape with global shapes and types [ShapedArray(float32[4096])]. Argument mapping: (UnspecifiedValue,).\n",
            "WARNING:2025-08-02 13:08:43,746:jax._src.dispatch:184: Finished jaxpr to MLIR module conversion jit(reshape) in 0.003015518 sec\n",
            "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(reshape) in 0.003015518 sec\n",
            "WARNING:2025-08-02 13:08:43,758:jax._src.dispatch:184: Finished XLA compilation of jit(reshape) in 0.009997368 sec\n",
            "WARNING:jax._src.dispatch:Finished XLA compilation of jit(reshape) in 0.009997368 sec\n",
            "WARNING:2025-08-02 13:08:43,761:jax._src.dispatch:184: Finished tracing + transforming reshape for pjit in 0.000268936 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming reshape for pjit in 0.000268936 sec\n",
            "WARNING:2025-08-02 13:08:43,762:jax._src.interpreters.pxla:1911: Compiling reshape with global shapes and types [ShapedArray(float32[32768])]. Argument mapping: (UnspecifiedValue,).\n",
            "WARNING:jax._src.interpreters.pxla:Compiling reshape with global shapes and types [ShapedArray(float32[32768])]. Argument mapping: (UnspecifiedValue,).\n",
            "WARNING:2025-08-02 13:08:43,766:jax._src.dispatch:184: Finished jaxpr to MLIR module conversion jit(reshape) in 0.002693653 sec\n",
            "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(reshape) in 0.002693653 sec\n",
            "WARNING:2025-08-02 13:08:43,777:jax._src.dispatch:184: Finished XLA compilation of jit(reshape) in 0.009921074 sec\n",
            "WARNING:jax._src.dispatch:Finished XLA compilation of jit(reshape) in 0.009921074 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can implement a vectorized version of the Bellman operator, which calculates the same values."
      ],
      "metadata": {
        "id": "BJBjSuCQm4MY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_T_manualvec(model: dict):\n",
        "  params = model[\"params\"]\n",
        "  a = model[\"batched_grids\"][\"a\"]\n",
        "  y = model[\"batched_grids\"][\"y\"]\n",
        "  ap = model[\"batched_grids\"][\"ap\"]\n",
        "  P = model[\"batched_grids\"][\"P\"]\n",
        "\n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_manualvec(v):\n",
        "      vp = jnp.dot(v, P) # vp has shape (a_size, y_size, 1)\n",
        "      c = params[\"R\"] * a + y - ap # c has shape (a_size, y_size, ap_size)\n",
        "      # m = jnp.where(c > 0, u(c) + β * vp, -np.inf) # m has shape (a_size, y_size, ap_size)\n",
        "      m = u(c) + params[\"beta\"] * vp # m has shape (a_size, y_size, ap_size)\n",
        "      return jnp.max(m, axis=2) # we to average over the last axis, that is, for each a and y, get the the max over ap.\n",
        "\n",
        "  return T_manualvec\n"
      ],
      "metadata": {
        "id": "gMVrgeNim435"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now wer are gonig to precompile the T operation so JAX optimize it use of the hardware."
      ],
      "metadata": {
        "id": "N4spbHVTm___"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T_manualvec = get_T_manualvec(model)\n",
        "T_manualvec_jit = jax.jit(T_manualvec).lower(v_init).compile()\n",
        "%time T_manualvec_jit(v_init).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "LeDrpKwbtsgQ",
        "outputId": "c6ff5114-b87f-43a7-faad-7ff1ecb8023d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:2025-08-02 13:08:43,793:jax._src.dispatch:184: Finished tracing + transforming dot for pjit in 0.002693176 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming dot for pjit in 0.002693176 sec\n",
            "WARNING:2025-08-02 13:08:43,795:jax._src.dispatch:184: Finished tracing + transforming multiply for pjit in 0.000522137 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming multiply for pjit in 0.000522137 sec\n",
            "WARNING:2025-08-02 13:08:43,797:jax._src.dispatch:184: Finished tracing + transforming add for pjit in 0.000332355 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming add for pjit in 0.000332355 sec\n",
            "WARNING:2025-08-02 13:08:43,798:jax._src.dispatch:184: Finished tracing + transforming subtract for pjit in 0.000534534 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming subtract for pjit in 0.000534534 sec\n",
            "WARNING:2025-08-02 13:08:43,800:jax._src.dispatch:184: Finished tracing + transforming _power for pjit in 0.000575542 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming _power for pjit in 0.000575542 sec\n",
            "WARNING:2025-08-02 13:08:43,802:jax._src.dispatch:184: Finished tracing + transforming true_divide for pjit in 0.000383377 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming true_divide for pjit in 0.000383377 sec\n",
            "WARNING:2025-08-02 13:08:43,803:jax._src.dispatch:184: Finished tracing + transforming multiply for pjit in 0.000366449 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming multiply for pjit in 0.000366449 sec\n",
            "WARNING:2025-08-02 13:08:43,805:jax._src.dispatch:184: Finished tracing + transforming add for pjit in 0.000334024 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming add for pjit in 0.000334024 sec\n",
            "WARNING:2025-08-02 13:08:43,807:jax._src.dispatch:184: Finished tracing + transforming _reduce_max for pjit in 0.000574827 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming _reduce_max for pjit in 0.000574827 sec\n",
            "WARNING:2025-08-02 13:08:43,809:jax._src.dispatch:184: Finished tracing + transforming T_manualvec for pjit in 0.019658089 sec\n",
            "WARNING:jax._src.dispatch:Finished tracing + transforming T_manualvec for pjit in 0.019658089 sec\n",
            "WARNING:2025-08-02 13:08:43,811:jax._src.interpreters.pxla:1911: Compiling T_manualvec with global shapes and types [ShapedArray(float32[32768,4096])]. Argument mapping: (UnspecifiedValue,).\n",
            "WARNING:jax._src.interpreters.pxla:Compiling T_manualvec with global shapes and types [ShapedArray(float32[32768,4096])]. Argument mapping: (UnspecifiedValue,).\n",
            "WARNING:2025-08-02 13:08:43,904:jax._src.dispatch:184: Finished jaxpr to MLIR module conversion jit(T_manualvec) in 0.092183828 sec\n",
            "WARNING:jax._src.dispatch:Finished jaxpr to MLIR module conversion jit(T_manualvec) in 0.092183828 sec\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "XlaRuntimeError",
          "evalue": "INTERNAL: RET_CHECK failure (external/xla/xla/backends/gpu/codegen/fusion_emitter.cc:98) (device_info.block_dim_limit().x == 0 || launch_dims.block_counts().x < device_info.block_dim_limit().x) && (device_info.block_dim_limit().y == 0 || launch_dims.block_counts().y < device_info.block_dim_limit().y) Kernel 'input_reduce_fusion' launch needs more blocks (2147483648, 1) than allowed by hardware (2147483647, 65535).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2312999995.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mT_manualvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_T_manualvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mT_manualvec_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_manualvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'T_manualvec_jit(v_init).block_until_ready()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/stages.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"compiler_options\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompiler_options\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     return Compiled(\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lowering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pytype: disable=wrong-keyword-args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2444\u001b[0m     \u001b[0mcompiler_options_kvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiler_options_kvs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt_compiler_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompiler_options_kvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       executable = UnloadedMeshExecutable.from_hlo(\n\u001b[0m\u001b[1;32m   2447\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m           compiler_options_kvs=compiler_options_kvs)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mfrom_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pxla_cached_compilation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2959\u001b[0;31m     xla_executable = _cached_compilation(\n\u001b[0m\u001b[1;32m   2960\u001b[0m         \u001b[0mhlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspmd_lowering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m         \u001b[0mtuple_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_spmd_lowering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_prop_to_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[0m\n\u001b[1;32m   2755\u001b[0m       \u001b[0;34m\"Finished XLA compilation of {fun_name} in {elapsed_time:.9f} sec\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m       fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2757\u001b[0;31m     xla_executable = compiler.compile_or_get_cached(\n\u001b[0m\u001b[1;32m   2758\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m         pgle_profiler)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[0m\n\u001b[1;32m    471\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mlog_persistent_cache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     return _compile_and_write_cache(\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    688\u001b[0m ) -> xc.LoadedExecutable:\n\u001b[1;32m    689\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m   executable = backend_compile(\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    328\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhandler_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mhandler_result\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;31m# to take in `host_callbacks`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilt_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mxc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXlaRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0merror_handler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_XLA_RUNTIME_ERROR_HANDLERS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: RET_CHECK failure (external/xla/xla/backends/gpu/codegen/fusion_emitter.cc:98) (device_info.block_dim_limit().x == 0 || launch_dims.block_counts().x < device_info.block_dim_limit().x) && (device_info.block_dim_limit().y == 0 || launch_dims.block_counts().y < device_info.block_dim_limit().y) Kernel 'input_reduce_fusion' launch needs more blocks (2147483648, 1) than allowed by hardware (2147483647, 65535)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a Tesla P100 GPU, we get 30ms. With TPU, we get 16.9"
      ],
      "metadata": {
        "id": "5iNU6avAozFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authomatic vecotrization using vmap\n",
        "\n",
        "When we calculate the value function at each point of the grid, we want to pass all the possible actions as a vector, and then calculate the maximum. In order to get the actio_value for all the possible actions, we are going to use vmap."
      ],
      "metadata": {
        "id": "KTHLO-DHMv1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we calculate the value function at each point of the grid, we want to pass all the possible actions as a vector, and then calculate the maximum. In order to get the actio_value for all the possible actions, we are going to use vmap."
      ],
      "metadata": {
        "id": "sn5sngld7KX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_T_autovec(model:dict):\n",
        "  params = model[\"params\"]\n",
        "  grids = model[\"grids\"]\n",
        "  P = model[\"Trans_matrix\"]\n",
        "\n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_autovec(v: jnp.ndarray):\n",
        "\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      #\n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:],P[y_ind, :]) ,- jnp.inf)\n",
        "\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = model[\"indices\"][\"a\"]\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_state_value = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    return new_state_value\n",
        "\n",
        "  return T_autovec"
      ],
      "metadata": {
        "id": "v3zJaJk89npP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T_autovec = get_T_autovec(model)\n",
        "T_autovec_jit = jax.jit(T_autovec).lower(v_init).compile()"
      ],
      "metadata": {
        "id": "27x1EKcp95xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time T_autovec_jit(v_init).block_until_ready()"
      ],
      "metadata": {
        "id": "ks6iS0OVONFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To finish the exercise off, let's iterate until convergence."
      ],
      "metadata": {
        "id": "oyMHJQ4OpAfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vfi_iterator(v_init=v_init, tol=1e-6, max_iter=1435):\n",
        "    error = tol + 1\n",
        "    i = 0\n",
        "    v = v_init\n",
        "    # while error > tol and i < max_iter:\n",
        "    while i < max_iter:\n",
        "        new_v = T_autovec_jit(v)\n",
        "        error = jnp.max(jnp.abs(new_v - v))\n",
        "        v = new_v\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "        i += 1\n",
        "\n",
        "    if i == max_iter:\n",
        "        print(f\"Warning: iteration hit upper bound {max_iter}.\")\n",
        "    else:\n",
        "        print(f\"\\nConverged at iteration {i}.\")\n",
        "    return v\n",
        "\n",
        "%time v = vfi_iterator()"
      ],
      "metadata": {
        "id": "TqgYaYNFpDNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelizing across TPU cores using pmaps\n",
        "\n",
        "We are going to make a very small change to our update funciton so we can give a partition of the grid of a as an input and we get can update the value function only on those states. Then, with pmap, we will vectorize that function so we give a n array with n_device partitions of a and it will update those partitions of the state in parallel across TPU_cores."
      ],
      "metadata": {
        "id": "y2_IS1SzOinP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_T_tpu(model:dict):\n",
        "\n",
        "  params = model[\"params\"]\n",
        "  grids = model[\"grids\"]\n",
        "  P = model[\"Trans_matrix\"]\n",
        "\n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_tpu(a_partition:jnp.array, v: jnp.ndarray):\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      #\n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:], model[\"Trans_matrix\"][y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = a_partition\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_state_value = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    return new_state_value\n",
        "\n",
        "  return T_tpu"
      ],
      "metadata": {
        "id": "px-H46ldOnOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, we create an array with the partitions. The dimension of that array should be (n_cores,a_size/n_cores), so the leading axis organize the different partitions."
      ],
      "metadata": {
        "id": "HVlCCZV0PaYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = jax.local_device_count()\n",
        "a_partitions = jnp.reshape(model[\"indices\"][\"a\"], (n_devices, a_size//n_devices))"
      ],
      "metadata": {
        "id": "rky2k2rIPZh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are goingh to pmap the function so instead of accepting a single partition of a as an input, it takes n array with partitions."
      ],
      "metadata": {
        "id": "MBmdVcbMRHEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfetto profiling\n",
        "T_tpu = get_T_tpu(model)\n",
        "T_tpu_jit = jax.pmap(T_tpu, in_axes = (0,None)).lower(a_partitions, v_init).compile()\n",
        "with jax.profiler.trace(\"/tmp/jax-trace\"):\n",
        "  T_tpu_jit(a_partitions,v_init).block_until_ready()"
      ],
      "metadata": {
        "id": "wET5GrxCRPKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! tensorboard --logdir /tmp/jax-trace\n"
      ],
      "metadata": {
        "id": "YJqj33hn30rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('jax-trace', 'zip', '/tmp/jax-trace')"
      ],
      "metadata": {
        "id": "Lez1TtMO46Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a X2 increase in throughput vs the manually vectorized solution."
      ],
      "metadata": {
        "id": "m2bt6XWsRiRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vfi_iterator(v_init=v_init, tol=1e-6, max_iter=1435):\n",
        "    error = tol + 1\n",
        "    i = 0\n",
        "    v = v_init\n",
        "    while error > tol and i < max_iter:\n",
        "    # while i < max_iter:\n",
        "        new_v = T_tpu_jit(a_partitions,v).reshape(a_size,y_size)\n",
        "        error = jnp.max(jnp.abs(new_v - v))\n",
        "        v = new_v\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "        i += 1\n",
        "\n",
        "    if i == max_iter:\n",
        "        print(f\"Warning: iteration hit upper bound {max_iter}.\")\n",
        "    else:\n",
        "        print(f\"\\nConverged at iteration {i}.\")\n",
        "    return v\n",
        "\n",
        "%time v = vfi_iterator()"
      ],
      "metadata": {
        "id": "T_Kd-64jzk2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarks\n",
        "\n",
        "Now we are going to store our results for different levels of scales"
      ],
      "metadata": {
        "id": "FFAPDQ5I2m09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_dict = {\"Size of grid\": [], \"Manual Vectorization\": [], \"Automatic Vectorization\": [], \"TPU Parallelization\": []}\n",
        "for scale in [1, 4, 8, 16, 32]:\n",
        "  # grid for assets\n",
        "  a_size = ap_size = 1024*scale\n",
        "  a_grid = jnp.linspace(a_min, a_max, a_size)  # grid for a\n",
        "  ap_grid = jnp.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "  #grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "  ρ = 0.9\n",
        "  σ = 0.1\n",
        "  y_size = 128*scale\n",
        "  mc = qe.tauchen(ρ, σ, n=y_size)\n",
        "  y_grid = jnp.exp(mc.state_values)\n",
        "  P = jnp.array(mc.P)\n",
        "  results_dict[\"Size of grid\"].append(a_size*y_size)\n",
        "\n",
        "  params = {\n",
        "      \"R\": 1.1,\n",
        "      \"beta\": 0.99,\n",
        "      \"gamma\": 2.5\n",
        "  }\n",
        "  grids = {\n",
        "    \"a\": a_grid,\n",
        "    \"y\": y_grid,\n",
        "    \"ap\": ap_grid,}\n",
        "\n",
        "  batched_grids = {\n",
        "    \"P\": jnp.reshape(P, (y_size, y_size, 1)),\n",
        "    \"a\": jnp.reshape(a_grid, (a_size, 1, 1)),\n",
        "    \"y\": jnp.reshape(y_grid, (1, y_size, 1)),\n",
        "    \"ap\": jnp.reshape(ap_grid, (1, 1, ap_size)),\n",
        "    }\n",
        "\n",
        "  model = {\"params\": params,\n",
        "          \"grids\": grids,\n",
        "           \"batched_grids\": batched_grids,\n",
        "          \"Trans_matrix\": P,\n",
        "          \"indices\": {\"a\": jnp.array(range(a_size)), \"y\": jnp.array(range(y_size)), \"ap\": jnp.array(range(ap_size))}\n",
        "           }\n",
        "\n",
        "  # initial value\n",
        "  v_init = jnp.zeros((a_size, y_size))\n",
        "\n",
        "  # Get and compile T functions\n",
        "  T_manualvec = get_T_manualvec(model)\n",
        "  T_manualvec_jit = jax.jit(T_manualvec).lower(v_init).compile()\n",
        "  T_autovec = get_T_autovec(model)\n",
        "  T_autovec_jit = jax.jit(T_autovec).lower(v_init).compile()\n",
        "  T_tpu = get_T_tpu(model)\n",
        "  a_partitions = jnp.reshape(model[\"indices\"][\"a\"], (n_devices, a_size//n_devices))\n",
        "  T_tpu_jit = jax.pmap(T_tpu, in_axes = (0,None)).lower(a_partitions, v_init).compile()\n",
        "  # run for 10 times and time it using timeit\n",
        "\n",
        "\n",
        "\n",
        "  results_dict[\"Manual Vectorization\"].append(timeit.timeit('T_manualvec_jit(v_init).block_until_ready()', globals=globals(), number=10)/10)\n",
        "  results_dict[\"Automatic Vectorization\"].append(timeit.timeit('T_autovec_jit(v_init).block_until_ready()', globals=globals(), number=10)/10)\n",
        "  results_dict[\"TPU Parallelization\"].append(timeit.timeit('T_tpu_jit(a_partitions, v_init).block_until_ready()', globals=globals(), number=10)/10)\n",
        "\n",
        "print(results_dict)"
      ],
      "metadata": {
        "id": "A6mw4oTZ2Kvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " For TPU, we get {'Size of grid': [131072, 2097152, 8388608, 33554432, 134217728], 'Manual Vectorization': [0.0032750082000120528, 0.09884197940000376, 0.7798380305999899, 6.246830163599998, 49.94550013509998], 'Automatic Vectorization': [0.0034154570000055175, 0.12627506110000014, 0.9292171296000106, 7.41619310469996, 59.2859591297], 'TPU Parallelization': [0.02720726330001071, 0.0561926443999937, 0.2108149590000039, 1.2764809615000103, 9.70835729370001]}\n",
        "\n",
        " For GPU{'Size of grid': [131072, 2097152, 8388608, 33554432, 134217728], 'Manual Vectorization': [0.004014438000001519, 0.19788137290000804, 1.5700353982000024, 12.1127872346, 97.47866143050001], 'Automatic Vectorization': [0.004789096399997561, 0.29747357039999545, 2.3173598094999988, 20.663012926700002, 224.7583419914], 'TPU Parallelization': [0.007923651699991296, 0.30005611290000617, 2.331630996399997, 20.0357178878, 223.66937429979998]}"
      ],
      "metadata": {
        "id": "d56TPREHHvC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-compiling the entire experiment (not just the update)"
      ],
      "metadata": {
        "id": "Utsxm-o9SUGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_learner_fn(model:dict, iterations):\n",
        "\n",
        "  def v_update(v: jnp.ndarray, error):\n",
        "    params = model[\"params\"]\n",
        "    grids = model[\"grids\"]\n",
        "    P = model[\"Trans_matrix\"]\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      #\n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:], P[y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = model[\"indices\"][\"a\"]\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_v = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    new_error = jnp.max(jnp.abs(new_v - v))\n",
        "    return new_v, new_error\n",
        "\n",
        "  def learner_fn(v_init):  # repeat many times to avoid going back to Python.\n",
        "    return jax.lax.scan(v_update, v_init, None, length = iterations)\n",
        "\n",
        "  return learner_fn"
      ],
      "metadata": {
        "id": "z1wn2smYuQUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 1435\n",
        "learner_fn = get_learner_fn(model, n_iters)\n",
        "jitted_learner_fn = jax.jit(learner_fn).lower(v_init).compile()"
      ],
      "metadata": {
        "id": "iHqUnEGNuhxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time value_final, error_stack = jitted_learner_fn(v_init)\n",
        "plt.plot(range(n_iters),error_stack)"
      ],
      "metadata": {
        "id": "Z4G1_YNQu1Gd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}