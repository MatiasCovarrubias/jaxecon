{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasCovarrubias/jaxecon/blob/main/jaxDEQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmNe6o_4tTEy"
      },
      "source": [
        "# DEQN Solver in Jax.\n",
        "\n",
        "This notebook trains a neural net to output the optimal policy of a nonlinear Rbc model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue0dX6Q14A1Q",
        "outputId": "916749de-4b7d-4aeb-afe3-9671c98b8a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CpuDevice(id=0)]\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "! pip install optax -q\n",
        "! pip install flax -q\n",
        "\n",
        "\n",
        "# the default is CPU\n",
        "TPU = False # set True if using TPU runtime\n",
        "GPU = False # if both TPU and GPU are false, we are using CPU\n",
        "if TPU:\n",
        "  !pip install --upgrade jax jaxlib \n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu('tpu_driver_20221011')\n",
        "elif GPU:\n",
        "  !nvidia-smi\n",
        "\n",
        "# Imports\n",
        "import jax\n",
        "from jax import lax\n",
        "from jax import random\n",
        "from jax import numpy as jnp\n",
        "from jax.config import config \n",
        "config.update(\"jax_debug_nans\", True)\n",
        "import flax.linen as nn\n",
        "from flax.training import checkpoints\n",
        "from flax.core import freeze, unfreeze\n",
        "import optax\n",
        "import time\n",
        "from time import time\n",
        "import timeit\n",
        "from typing import Sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(jax.devices())\n",
        "\n",
        "# Mount Google Drive to store results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhImo5eJu6RS"
      },
      "source": [
        "## 0. Import model structure and create policies\n",
        "\n",
        "To start, we will import the parameters of the model and the loglinear policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIQLSPJLvaPE"
      },
      "source": [
        "### Create Neural Net policy\n",
        "\n",
        "First, we use Flax to create the Neural Net, Notice that we activate the last layer using Softplus to guarantee that we get possitive outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVGY1ZCnvtXh"
      },
      "outputs": [],
      "source": [
        "class MLP_softplus(nn.Module):\n",
        "  features: Sequence[int]\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    for feat in self.features[:-1]:\n",
        "      x = nn.relu(nn.Dense(feat)(x))\n",
        "    x = nn.softplus(nn.Dense(self.features[-1])(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LXEca1NwAiC"
      },
      "source": [
        "## 1. Economic model\n",
        "\n",
        "We will represent our model as clas with four main methods (or functions): initial_obs to get first observation; step to advance a period, expectation to get the expectation term given a state, policy, and shock; and a loss funciton that gets as the loss given a state, policy and expectation term. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roYc6Stsjrfp"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "  \"\"\"A JAX implementation of an RBC model.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.beta = 0.985\n",
        "    self.alpha = 0.3\n",
        "    self.delta = 0.03\n",
        "    self.rho = .9\n",
        "    self.shock_sd = 0.01\n",
        "    \n",
        "    self.k_ss = jnp.log((self.alpha/(1/self.beta-1+self.delta))**(1/(1-self.alpha)))\n",
        "    self.a_ss = 0\n",
        "    self.obs_ss = jnp.array([self.k_ss,0,0])\n",
        "    self.state_ss = jnp.array([self.k_ss,0])\n",
        "    self.policy_ss = jnp.log(self.delta*jnp.exp(self.k_ss))\n",
        "    self.states_sd = jnp.array([1,1]) # use 1 if you don't have an estimate\n",
        "    self.obs_sd = jnp.array([1,1,1]) # use 1 if you don't have an estimate\n",
        "    self.n_actions = 1\n",
        "\n",
        "  def initial_obs(self, rng):\n",
        "    \"\"\" Get initial obs given first shock \"\"\"\n",
        "    e = self.sample_shock(rng)\n",
        "    obs_init_notnorm = jnp.array([self.k_ss,self.a_ss,e])\n",
        "    obs_init = (obs_init_notnorm-self.obs_ss)/self.obs_sd # normalize\n",
        "    return obs_init\n",
        "\n",
        "  def step(self, obs, policy, shock): \n",
        "    \"\"\" A period step of the model, given current obs, the shock and policy_params \"\"\"\n",
        "    \n",
        "    obs_notnorm = obs*self.obs_sd + self.obs_ss# denormalize \n",
        "    K = jnp.exp(obs_notnorm[0]) # Kt in levels\n",
        "    a_tmin1 = obs_notnorm[1] # a_{t-1}\n",
        "    shock_tmin1 = obs_notnorm[2] # \\epsilon_t\n",
        "    a = self.rho * a_tmin1 + self.shock_sd*shock_tmin1 # recover a_t\n",
        "    policy_notnorm = policy*jnp.exp(self.policy_ss) # multiply by steady state pol in levels\n",
        "    K_tplus1 = (1-self.delta)*K + policy_notnorm[0] #get K_{t+1}\n",
        "    obs_next_notnorm = jnp.array([jnp.log(K_tplus1),a, shock]) #concatenate observation\n",
        "    obs_next = (obs_next_notnorm-self.obs_ss)/self.obs_sd # normalize\n",
        "  \n",
        "    return obs_next\n",
        "\n",
        "\n",
        "  def expect_realization(self, obs_next, policy_next):\n",
        "    \"\"\" A realization (given a shock) of the expectation terms in system of equation \"\"\"\n",
        "\n",
        "    policy_notnorm = policy_next*jnp.exp(self.policy_ss) # multiply by ss policy in levels\n",
        "    I = policy_notnorm[0] # define investment\n",
        "    \n",
        "    # Process observation\n",
        "    obs_notnorm = obs_next*self.obs_sd + self.obs_ss # denormalize obs\n",
        "    K = jnp.exp(obs_notnorm[0]) # K_{t+1} in levels\n",
        "    a_tmin1 = obs_notnorm[1] # a_{t}\n",
        "    shock_tmin1 = obs_notnorm[2] #\\epsilon_{t+1}\n",
        "    a = self.rho * a_tmin1 + self.shock_sd*shock_tmin1 # recover a_{t+1}\n",
        "\n",
        "    # Rest of variables\n",
        "    A = jnp.exp(a)\n",
        "    Y = A * K**self.alpha\n",
        "    C = Y-I  \n",
        "\n",
        "    # Calculate the FOC for Pk\n",
        "    expect_realization = (1/C) * (1+ A * self.alpha * K**(self.alpha-1)-self.delta)\n",
        "    \n",
        "    return expect_realization\n",
        "\n",
        "  def loss(self, obs, expect, policy):\n",
        "    \"\"\" Calculate loss associated with observing obs, having policy_params, and expectation exp \"\"\"\n",
        "\n",
        "    policy_notnorm = policy*jnp.exp(self.policy_ss)\n",
        "    I = policy_notnorm[0]\n",
        "    \n",
        "    # Process observation\n",
        "    obs_notnorm = obs*self.obs_sd + self.obs_ss # denormalize \n",
        "    K = jnp.exp(obs_notnorm[0]) # put in levels\n",
        "    a_tmin1 = obs_notnorm[1]\n",
        "    shock_tmin1 = obs_notnorm[2]\n",
        "    a = self.rho * a_tmin1 + self.shock_sd*shock_tmin1 # recover a_t\n",
        "\n",
        "    # Rest of variables\n",
        "    A = jnp.exp(a)\n",
        "    Y = A * K**self.alpha\n",
        "    C = Y-I    \n",
        "\n",
        "    # Calculate the FOC for Pk\n",
        "    loss = (1/C)/(self.beta*expect) - 1\n",
        "    quad_loss = loss**2\n",
        "    accuracy = 1-jnp.abs(loss)\n",
        "    return quad_loss, accuracy\n",
        "\n",
        "  def sample_shock(self, rng):\n",
        "    \"\"\" sample one realization of the shock. \n",
        "    Uncomment second line for continuous shocks instead of grid \"\"\"\n",
        "    # return random.choice(rng, jnp.array([-1.2816,-0.6745,0,0.6745, 1.2816]))\n",
        "    return random.normal(rng) \n",
        "  \n",
        "  def mc_shocks(self, rng=random.PRNGKey(0), mc_draws=8):\n",
        "    \"\"\" sample omc_draws realizations of the shock (for monte-carlo)\n",
        "    Uncomment second line for continuous shocks instead of grid \"\"\"\n",
        "    # return  jnp.array([-1.2816,-0.6745,0,0.6745, 1.2816])\n",
        "    return jax.random.normal(rng, shape=(mc_draws,))\n",
        "      \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the environment\n",
        "We are going to make sure that the functions in our model are correct"
      ],
      "metadata": {
        "id": "Fz-RH8Wt23Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Model()\n",
        "rng_test = random.PRNGKey(1)\n",
        "\n",
        "# test steady state policies with random params\n",
        "obs_ss = jnp.zeros_like(env.obs_ss)\n",
        "nn_policy_class = MLP_softplus([8,8] + [env.n_actions])\n",
        "nn_policy = nn_policy_class.apply\n",
        "params_test = nn_policy_class.init(rng_test, obs_ss) # we initialize random params\n",
        "policy_ss = nn_policy(params_test,obs_ss)\n",
        "print(\"policy in steady state\", policy_ss)\n",
        "\n",
        "# intialize env\n",
        "obs_init = env.initial_obs(rng_test)\n",
        "print(\"initial obs\", obs_init)\n",
        "\n",
        "# apply a step\n",
        "policy_firststep = nn_policy(params_test,obs_init)\n",
        "print(\"policy in first step\", policy_firststep)\n",
        "shock_firststep = env.sample_shock(rng_test)\n",
        "next_obs_firststep = env.step(obs_init, policy_firststep, shock_firststep)\n",
        "print(\"next obs first step\", next_obs_firststep)\n",
        "\n",
        "# calculate loss in first step.\n",
        "\n",
        "#First, we calculate expectations\n",
        "mc_shocks = jnp.array([-1.2816,-0.6745,0,0.6745, 1.2816]) # grid (or sample) of shocks\n",
        "mc_nextobs = jax.vmap(env.step, in_axes = (None,None,0))(obs_init, policy_firststep, mc_shocks) # next obs given policy and for each shock in mc_shocks\n",
        "\n",
        "mc_nextpols = nn_policy(params_test, mc_nextobs)\n",
        "expect_firststep = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols))\n",
        "\n",
        "# Second, we calculate loss given expectations and policy\n",
        "quad_loss, accuracy = env.loss(obs_init, expect_firststep, policy_firststep)\n",
        "print(\"loss and accuracy in first step\", quad_loss, accuracy)\n",
        "\n",
        "# calculate loss with policy = 1\n",
        "policy_ones = jnp.ones_like(policy_firststep)\n",
        "mc_nextobs = jax.vmap(env.step, in_axes = (None,None,0))(obs_ss, policy_ones, jnp.zeros_like(mc_shocks)) # next obs given policy and for each shock in mc_shocks\n",
        "print(\"next obs with policies =1\", mc_nextobs)\n",
        "mc_nextpols = jnp.ones_like(mc_nextpols)\n",
        "expect = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols))\n",
        "quad_loss, accuracy = env.loss(obs_ss, expect, policy_ones)\n",
        "print(\"loss and accuracy with policies =1\", quad_loss, accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g60VDRJ-Zx_",
        "outputId": "0acc53cd-3fc4-4001-f837-da8bbd2cc27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy in steady state [0.6931472]\n",
            "initial obs [ 0.         0.        -1.1842843]\n",
            "policy in first step [0.7405023]\n",
            "next obs first step [-0.00781536 -0.01184284 -1.1842843 ]\n",
            "loss and accuracy in first step 2.3560372e-06 0.99846506\n",
            "next obs with policies =1 [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "loss and accuracy with policies =1 1.4210855e-14 0.9999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FGm6vTqXqzz"
      },
      "source": [
        "## 2. Create training function\n",
        "No we define a function specifying an entire epoque of learning. This is the minimal unti lfo computation that we wil compile and pass to all the devices (devices are sunchronized to average gradients)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_epoque_learner_fn(\n",
        "    env, nn_policy, opt_update, periods_per_epis, epis_per_epoque, mc_draws, n_cores):\n",
        "  \"\"\"It runs and epoque with learing. This is what the compiler reads and parallelize (the minimal unit of computation).\"\"\"\n",
        "\n",
        "  def period_step_fn(env_obs, period_rng, params):\n",
        "    \"\"\" Given observation env_obs, rng period_rng, and policy parameters pararms, \n",
        "    give next observation obs_next \"\"\"\n",
        "\n",
        "    # get rngs\n",
        "    period_rng, mc_rng = random.split(period_rng,2)\n",
        "    \n",
        "    # calculate policy\n",
        "    policy = nn_policy(params, env_obs)\n",
        "\n",
        "    # Sample next obs\n",
        "    period_shock = env.sample_shock(period_rng)\n",
        "    obs_next = env.step(env_obs, policy, period_shock)  # apply period steps for each row shock in shocks.\n",
        "\n",
        "    return obs_next, obs_next # we pass it two times because of the syntax of the lax.scan loop\n",
        "\n",
        "  def epis_loss_fn(params, epis_rng, obs_batch):\n",
        "    \"\"\" loss function of an episode, which has periods_per_epis time periods \"\"\"\n",
        "\n",
        "    epis_rng, *period_mc_rngs = random.split(epis_rng, periods_per_epis+1)\n",
        "    policies_batch = nn_policy(params, obs_batch) # get the policies for the entire obs batch.\n",
        "\n",
        "    def period_loss(obs, policy, period_mc_rng):\n",
        "      \"\"\" loss function for an individual period \"\"\"\n",
        "      mc_shocks = env.mc_shocks(period_mc_rng)\n",
        "      mc_nextobs = jax.vmap(env.step, in_axes = (None,None,0))(obs, policy, mc_shocks)\n",
        "      mc_nextpols = nn_policy(params, mc_nextobs)\n",
        "      expect = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols))\n",
        "      # calculate loss\n",
        "      quad_loss, accuracy = env.loss(obs, expect, policy)\n",
        "      return quad_loss, accuracy\n",
        "    \n",
        "    # parallelize callculation of period_loss for the entire batch\n",
        "    quad_losses, accuracies = jax.vmap(period_loss)(obs_batch, policies_batch, jnp.stack(period_mc_rngs))\n",
        "    mean_quad_losses = jnp.mean(quad_losses) \n",
        "    mean_accuracy = jnp.mean(accuracies) \n",
        "    obs_final =obs_batch[-1,:] # get the final observation as the last obs from obs_batch\n",
        "    aux_info = (obs_final, jnp.array([mean_quad_losses]), jnp.array([mean_accuracy])) # pass as auxiliary info\n",
        "    return mean_quad_losses, aux_info\n",
        "\n",
        "  def update_fn(params, opt_state, epoque_rng, env_obs, quad_loss, accuracy):\n",
        "    \"\"\"Compute a gradient update from a single episode.\"\"\"\n",
        "\n",
        "    new_epoque_rng, epis_rng = random.split(epoque_rng)\n",
        "    epis_rng, *period_rngs = random.split(epis_rng, periods_per_epis+1)\n",
        "    period_step_partialfn = jax.tree_util.Partial(period_step_fn,params=params)\n",
        "    _, obs_batch = lax.scan(period_step_partialfn, env_obs, jnp.stack(period_rngs))\n",
        "    grads, aux_info  = jax.grad(  # compute gradient on a single trajectory.\n",
        "        epis_loss_fn, has_aux=True)(params, epis_rng, obs_batch)\n",
        "    new_env_obs, quad_loss, accuracy= aux_info\n",
        "    updates, new_opt_state = opt_update(grads, opt_state, params)  # transform grads.\n",
        "    new_params = optax.apply_updates(params, updates)  # update parameters.\n",
        "    \n",
        "    return new_params, new_opt_state, new_epoque_rng, new_env_obs, quad_loss, accuracy\n",
        "\n",
        "\n",
        "  def epoque_learner_fn(params, opt_state, epoque_rng, env_obs, quad_loss, accuracy):\n",
        "    \"\"\"Vectorise and repeat the update to complete an epoque, made aout of epis_per_epoque episodes.\"\"\"\n",
        "    # batched_update_fn = jax.vmap(update_fn, axis_name='j')  # vectorize across batch.\n",
        "    def iterate_fn(_, val):  # repeat many times to avoid going back to Python.\n",
        "      params, opt_state, epoque_rng, env_obs, quad_loss, accuracy = val\n",
        "      return update_fn(params, opt_state, epoque_rng, env_obs, quad_loss, accuracy)\n",
        "    return lax.fori_loop(0, epis_per_epoque, iterate_fn, (\n",
        "        params, opt_state, epoque_rng, env_obs, quad_loss, accuracy))\n",
        "\n",
        "  return epoque_learner_fn"
      ],
      "metadata": {
        "id": "zZoQdJ31e7EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the update function\n",
        "\n",
        "We can apply one update and inspect what is going on at each point by printing elements."
      ],
      "metadata": {
        "id": "JYEia3OD-Lij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test period loss function\n",
        "\n",
        "def period_step_fn(env_obs, period_rng, params):\n",
        "  \n",
        "  # get rngs\n",
        "  period_rng, mc_rng = random.split(period_rng,2)\n",
        "  \n",
        "  # calculate policy\n",
        "  policy = nn_policy(params, env_obs)\n",
        "\n",
        "  # Sample next obs\n",
        "  period_shock = random.choice(period_rng, jnp.array([-1.2816,-0.6745,0,0.6745, 1.2816]))\n",
        "  # period_shock = random.normal(period_rng)\n",
        "  obs_next = env.step(env_obs, policy, period_shock)  # apply period steps for each row shock in shocks.\n",
        "\n",
        "  return obs_next, obs_next\n",
        "\n",
        "def epis_loss_fn(params, epis_rng, obs_batch):\n",
        "  epis_rng, *period_mc_rngs = random.split(epis_rng, periods_per_epis+1)\n",
        "  policies_batch = nn_policy(params, obs_batch)\n",
        "  def period_loss(obs, policy, period_mc_rng):\n",
        "    # mc_shocks = jax.random.normal(period_mc_rng, shape=(mc_draws,))\n",
        "    mc_shocks = env.mc_shocks(period_mc_rng)\n",
        "    mc_nextobs = jax.vmap(env.step, in_axes = (None,None,0))(obs, policy, mc_shocks)\n",
        "    mc_nextpols = nn_policy(params, mc_nextobs)\n",
        "    expect = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols))\n",
        "    # calculate loss\n",
        "    quad_loss, accuracy = env.loss(obs, expect, policy)\n",
        "    return quad_loss, accuracy\n",
        "    \n",
        "  quad_losses, accuracies = jax.vmap(period_loss)(obs_batch, policies_batch, jnp.stack(period_mc_rngs))\n",
        "  mean_quad_losses = jnp.mean(quad_losses)\n",
        "  mean_accuracy = jnp.mean(accuracies)\n",
        "  obs_final =obs_batch[-1,:]\n",
        "  aux_info = (obs_final, jnp.array([mean_quad_losses]), jnp.array([mean_accuracy]))\n",
        "  return mean_quad_losses, aux_info\n",
        "\n",
        "def update_fn(params, opt_state, epoque_rng, env_obs, quad_loss, accuracy):\n",
        "  \"\"\"Compute a gradient update from a single trajectory.\"\"\"\n",
        "\n",
        "  new_epoque_rng, epis_rng = random.split(epoque_rng)\n",
        "  epis_rng, *period_rngs = random.split(epis_rng, periods_per_epis+1)\n",
        "  period_step_partialfn = jax.tree_util.Partial(period_step_fn,params=params)\n",
        "  _, obs_batch = lax.scan(period_step_partialfn, env_obs, jnp.stack(period_rngs))\n",
        "  print(\"shape of obs_batch\", obs_batch.shape)\n",
        "  grads, aux_info  = jax.grad(  # compute gradient on a single trajectory.\n",
        "      epis_loss_fn, has_aux=True)(params, epis_rng, obs_batch)\n",
        "  print(\"Gradients:\", grads)\n",
        "  new_env_obs, quad_loss, accuracy= aux_info\n",
        "  updates, new_opt_state = opt_update(grads, opt_state, params)  # transform grads.\n",
        "  new_params = optax.apply_updates(params, updates)  # update parameters.\n",
        "  \n",
        "  return new_params, new_opt_state, new_epoque_rng, new_env_obs, quad_loss, accuracy\n",
        "\n",
        "env = Model()\n",
        "rng_test = random.PRNGKey(1)\n",
        "\n",
        "# test steady state policies\n",
        "obs_init = env.initial_obs(rng_test)\n",
        "mc_draws = 10\n",
        "periods_per_epis = 16\n",
        "nn_policy_class = MLP_softplus([8,8] + [env.n_actions])\n",
        "rng, rng_p = random.split(random.PRNGKey(1), num=2)  # prng keys.\n",
        "dummy_obs = env.initial_obs(rng) # dummy for net init.\n",
        "params_test = nn_policy_class.init(rng_p, dummy_obs)\n",
        "# print(\"variables init:\", variables)\n",
        "obs_ss = jnp.zeros_like(obs_init)\n",
        "nn_policy = nn_policy_class.apply\n",
        "\n",
        "print(\"test nn update fn\")\n",
        "quad_loss = jnp.array([0.0])\n",
        "accuracy = jnp.array([0.0])\n",
        "optim = optax.adam(0.0000005)  # define optimiser.\n",
        "opt_state = optim.init(params_test)  # initialise optimiser stats.\n",
        "opt_update = optim.update\n",
        "new_params, new_opt_state, new_epoque_rng, new_env_obs, quad_loss, accuracy = update_fn(params_test, opt_state, rng_test, obs_init, quad_loss, accuracy)\n",
        "print(\"New variables \\n\", new_params)\n",
        "print(\"Quad loss \\n\", quad_loss)\n",
        "print(\"Accuracy \\n\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hicUBLteIjCP",
        "outputId": "289c4646-51c3-458f-85fe-832f0b28c6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test nn update fn\n",
            "shape of obs_batch (16, 3)\n",
            "Gradients: FrozenDict({\n",
            "    params: {\n",
            "        Dense_0: {\n",
            "            bias: DeviceArray([-1.2591408e-06,  4.8194430e-05, -2.0673961e-05,\n",
            "                         -3.8637249e-05,  1.5909585e-05,  2.8352588e-05,\n",
            "                          7.0902766e-05, -6.0636621e-06], dtype=float32),\n",
            "            kernel: DeviceArray([[-5.9791682e-07, -4.2094844e-06,  1.0405238e-06,\n",
            "                           2.9731423e-06, -1.1712336e-06, -1.3268345e-06,\n",
            "                          -4.7844933e-06,  1.9994206e-07],\n",
            "                         [-1.6830410e-07, -2.2443865e-07,  1.9969254e-07,\n",
            "                           3.8647369e-07, -9.1991893e-08, -5.1363133e-07,\n",
            "                          -8.1130730e-07,  4.8062454e-08],\n",
            "                         [-4.8371712e-06,  4.7403249e-05, -2.6977268e-05,\n",
            "                          -5.6288722e-05, -6.9067016e-07,  7.0641647e-05,\n",
            "                           1.1025336e-04, -1.7845291e-06]], dtype=float32),\n",
            "        },\n",
            "        Dense_1: {\n",
            "            bias: DeviceArray([-2.8728295e-05,  2.2681576e-04, -1.7979713e-05,\n",
            "                          4.5933903e-06, -9.4612114e-06, -1.7247259e-05,\n",
            "                          1.9688552e-05, -2.6586698e-05], dtype=float32),\n",
            "            kernel: DeviceArray([[-1.5215406e-06,  1.9575768e-05, -1.5266647e-07,\n",
            "                           7.2188001e-08, -7.6557052e-07,  0.0000000e+00,\n",
            "                          -1.2574535e-06, -3.1193913e-06],\n",
            "                         [-1.5151027e-06,  4.8966453e-05, -8.2859633e-06,\n",
            "                           1.6529463e-06,  2.9367748e-06, -7.1275704e-06,\n",
            "                           2.5354122e-05, -1.4208457e-05],\n",
            "                         [-1.8437250e-08,  4.1006944e-05, -6.9297639e-06,\n",
            "                           1.3824008e-06,  2.4560993e-06, -6.3494317e-06,\n",
            "                           2.2904249e-05, -1.2313818e-05],\n",
            "                         [-3.5509329e-06,  4.4824777e-05, -7.6054721e-06,\n",
            "                           1.5513713e-06,  2.3723958e-06, -5.9476210e-06,\n",
            "                           2.0513588e-05, -1.2285677e-05],\n",
            "                         [ 0.0000000e+00,  1.9699580e-04,  0.0000000e+00,\n",
            "                           0.0000000e+00, -5.6101362e-07,  0.0000000e+00,\n",
            "                           0.0000000e+00, -4.7794616e-05],\n",
            "                         [-1.5739777e-06,  2.4698564e-05, -4.1907160e-06,\n",
            "                           8.4624003e-07,  1.4534086e-06, -3.4050936e-06,\n",
            "                           1.1814109e-05, -6.8979830e-06],\n",
            "                         [-8.0567895e-07,  1.0116584e-05, -1.7155071e-06,\n",
            "                           3.4974772e-07,  5.4301989e-07, -1.3293633e-06,\n",
            "                           4.6100977e-06, -2.7720077e-06],\n",
            "                         [ 0.0000000e+00,  1.1928574e-05, -2.0158091e-06,\n",
            "                           4.0212876e-07,  7.1445868e-07, -1.8693725e-06,\n",
            "                           6.8974186e-06, -3.5837659e-06]], dtype=float32),\n",
            "        },\n",
            "        Dense_2: {\n",
            "            bias: DeviceArray([-4.5828638e-05], dtype=float32),\n",
            "            kernel: DeviceArray([[3.1224779e-08],\n",
            "                         [2.9944989e-05],\n",
            "                         [3.7027581e-05],\n",
            "                         [5.4409560e-05],\n",
            "                         [6.1374321e-06],\n",
            "                         [2.7667025e-07],\n",
            "                         [3.4027635e-06],\n",
            "                         [3.8761184e-05]], dtype=float32),\n",
            "        },\n",
            "    },\n",
            "})\n",
            "New variables \n",
            " FrozenDict({\n",
            "    params: {\n",
            "        Dense_0: {\n",
            "            bias: DeviceArray([ 4.9605700e-07, -4.9989291e-07,  4.9975489e-07,\n",
            "                          4.9986733e-07, -4.9968259e-07, -4.9982032e-07,\n",
            "                         -4.9992616e-07,  4.9917344e-07], dtype=float32),\n",
            "            kernel: DeviceArray([[-0.608663  ,  0.55748296,  1.0598944 , -0.29405704,\n",
            "                          -0.34176168, -0.02079846, -0.0673866 ,  0.53183526],\n",
            "                         [-1.2118798 , -0.0081175 , -0.01858417, -0.1197021 ,\n",
            "                          -1.217833  , -0.24635087,  0.06880467,  0.08844869],\n",
            "                         [-0.05416802,  0.37439814,  0.3460669 ,  0.29397902,\n",
            "                          -1.2735078 ,  0.16950008,  0.06681283,  0.10795815]],            dtype=float32),\n",
            "        },\n",
            "        Dense_1: {\n",
            "            bias: DeviceArray([ 4.9982265e-07, -4.9997459e-07,  4.9971868e-07,\n",
            "                         -4.9891054e-07,  4.9946874e-07,  4.9970691e-07,\n",
            "                         -4.9974284e-07,  4.9980866e-07], dtype=float32),\n",
            "            kernel: DeviceArray([[-0.02202892, -0.01496574, -0.28740138,  0.11548422,\n",
            "                           0.7191744 , -0.40732506, -0.08753818, -0.47369558],\n",
            "                         [ 0.04907801,  0.6920921 ,  0.37546885,  0.6239084 ,\n",
            "                          -0.11542673, -0.30303907, -0.33828536,  0.6739903 ],\n",
            "                         [-0.69323194, -0.30081263, -0.08314998,  0.07146894,\n",
            "                          -0.15952681,  0.6234207 ,  0.39155945,  0.09805348],\n",
            "                         [-0.0130881 , -0.2782948 ,  0.2855624 , -0.07371911,\n",
            "                           0.28557095,  0.0102289 , -0.26044488, -0.16051038],\n",
            "                         [-0.02178821,  0.02069918, -0.41783738, -0.61767757,\n",
            "                          -0.7764953 , -0.612989  , -0.29856446,  0.06531089],\n",
            "                         [ 0.06021715,  0.04425256, -0.03135597,  0.5002699 ,\n",
            "                           0.28181592, -0.46229824,  0.53640884, -0.32676262],\n",
            "                         [-0.06938238,  0.4697581 , -0.02116949, -0.17038098,\n",
            "                          -0.29348788, -0.7012    ,  0.60227484,  0.25300217],\n",
            "                         [-0.17332332,  0.24850409,  0.01678292, -0.14976582,\n",
            "                           0.25767568,  0.40767977, -0.412986  , -0.1323249 ]],            dtype=float32),\n",
            "        },\n",
            "        Dense_2: {\n",
            "            bias: DeviceArray([4.9988756e-07], dtype=float32),\n",
            "            kernel: DeviceArray([[-0.6552541 ],\n",
            "                         [ 0.72379   ],\n",
            "                         [-0.12231383],\n",
            "                         [ 0.02439949],\n",
            "                         [ 0.04335074],\n",
            "                         [-0.4158266 ],\n",
            "                         [ 0.41851443],\n",
            "                         [-0.2174528 ]], dtype=float32),\n",
            "        },\n",
            "    },\n",
            "})\n",
            "Quad loss \n",
            " [3.61582e-05]\n",
            "Accuracy \n",
            " [0.99486923]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEAiCNRxC1AJ"
      },
      "source": [
        "## 3. Run experiment\n",
        "Now we the entire experiment workflow as a function to call later."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(env, config):\n",
        "  \"\"\"Runs experiment.\"\"\"\n",
        "\n",
        "  n_cores = len(jax.devices())  # get available TPU cores.\n",
        "  \n",
        "  # Nueral Net\n",
        "  nn_policy_class = MLP_softplus(config[\"layers\"] + [env.n_actions])\n",
        "  rng, rng_p = random.split(random.PRNGKey(config[\"seed\"]), num=2)  # prng keys.\n",
        "  dummy_obs = env.initial_obs(rng) # dummy for net init.\n",
        "  params = nn_policy_class.init(rng_p, dummy_obs)\n",
        "  nn_policy = nn_policy_class.apply\n",
        "  \n",
        "  #Optimizer\n",
        "  optim = optax.adam(config[\"learning_rate\"])  # define optimiser.\n",
        "  opt_state = optim.init(params)  # initialise optimiser stats.\n",
        "\n",
        "  # Create epoque learner fn\n",
        "  epoque_learner = jax.jit(get_epoque_learner_fn(env, nn_policy, optim.update, config[\"periods_per_epis\"], config[\"epis_per_epoque\"],config[\"mc_draws\"], n_cores))\n",
        "  \n",
        "\n",
        "  # rng, env_rngs = jax.random.split(rng, n_cores * config[\"n_batches\"]+ 1)\n",
        "  rng, env_rngs = jax.random.split(rng, 2)\n",
        "  env_obs = env.initial_obs(env_rngs)  # init envs.\n",
        "  # rng, epoque_rngs = jax.random.split(rng, n_cores * config[\"n_batches\"] + 1)\n",
        "  rng, epoque_rngs = jax.random.split(rng, 2)\n",
        "  # rng, eval_rngs = jax.random.split(rng, n_cores * config[\"n_batches\"] + 1)\n",
        "  quad_loss = jnp.array([0.0])\n",
        "  accuracy = jnp.array([0.0])\n",
        "  mean_losses = []\n",
        "  mean_accuracy = []\n",
        "  num_steps = n_cores * config[\"epis_per_epoque\"] * config[\"periods_per_epis\"]\n",
        "\n",
        "  time_start = time()\n",
        "\n",
        "  epoque_learner(params, opt_state, epoque_rngs, env_obs, quad_loss, accuracy)  # compiles\n",
        "  time_compilation = time() - time_start\n",
        "  print(\"Time Elapsed for Compilation:\", time_compilation, \"seconds\")\n",
        "\n",
        "  #First run, we calculate periods per second\n",
        "  time_start = time()\n",
        "\n",
        "  params, opt_state, epoque_rngs, env_obs, quad_loss, accuracy = epoque_learner(\n",
        "      params, opt_state, epoque_rngs, env_obs, quad_loss, accuracy)\n",
        "  \n",
        "  time_epoque = time() - time_start\n",
        "  print(\"Time Elapsed for Epoque:\", time_epoque, \"seconds\")\n",
        "  steps_persec = num_steps/time_epoque\n",
        "  print(\"Steps per second:\", steps_persec, \"st/s\")\n",
        "\n",
        "  mean_losses.append(float(jnp.mean(quad_loss)))\n",
        "  mean_accuracy.append(float(jnp.mean(accuracy)))\n",
        "  print()  \n",
        "  print('Iteration:', 1*config[\"epis_per_epoque\"], \n",
        "        \", Mean_loss:\", jnp.mean(quad_loss),\n",
        "        \", Mean_accuracy:\", jnp.mean(accuracy),\n",
        "        \", Learning rate:\", config[\"learning_rate\"](1*config[\"epis_per_epoque\"]), \n",
        "        )\n",
        "  \n",
        "  #Rest of the runs\n",
        "  time_start = time()\n",
        "  env_obs = jnp.zeros_like(env_obs)\n",
        "  for i in range(2,config[\"n_epoques\"]+1):\n",
        "    params, opt_state, epoque_rngs, env_obs, quad_loss, accuracy = epoque_learner( \n",
        "        params, opt_state, epoque_rngs, env_obs, quad_loss, accuracy) \n",
        "    \n",
        "    mean_losses.append(float(jnp.mean(quad_loss))) \n",
        "    mean_accuracy.append(float(jnp.mean(accuracy)))\n",
        "      \n",
        "    print('Iteration:', i*config[\"epis_per_epoque\"], \n",
        "          \", Mean_loss:\", jnp.mean(quad_loss),\n",
        "          \", Mean_accuracy:\", jnp.mean(accuracy),\n",
        "          \", Learning rate:\", config[\"learning_rate\"](i*config[\"epis_per_epoque\"]), \n",
        "          )\n",
        "    \n",
        "    if i%config[\"reset_env_nepoques\"]==0:\n",
        "      env_obs = jnp.zeros_like(env_obs)\n",
        "      print(\"ENV RESET\")\n",
        "\n",
        "  # Results\n",
        "  min_loss = min(mean_losses)\n",
        "  max_acc = max(mean_accuracy)\n",
        "  print(\"Minimum loss attained in training:\", min_loss)\n",
        "  print(\"Maximum accuracy attained in training:\", max_acc)\n",
        "  # time elasped\n",
        "  time_fullexp = (time() - time_start)/60\n",
        "  print(\"Time Elapsed for Full Experiment:\", time_fullexp, \"minutes\")\n",
        "\n",
        "  results = {\n",
        "      \"min_loss\": min_loss,\n",
        "      \"max_acc\": max_acc,\n",
        "      \"Losses_list\": mean_losses,\n",
        "      \"accuracy_list\": mean_accuracy,\n",
        "      \"Time for Full Experiment (m)\": time_fullexp,\n",
        "      \"Time for Epoque (s)\": time_epoque,\n",
        "      \"Time for Compilation (s)\": time_compilation,\n",
        "      \"Steps per second\": steps_persec,\n",
        "      \"n_cores\": n_cores,\n",
        "      \"periods_per_epis\": config[\"periods_per_epis\"],\n",
        "      \"epis_per_epoque\": config[\"epis_per_epoque\"],\n",
        "      \"n_epoques\": config[\"n_epoques\"],\n",
        "      \"date\": config[\"date\"],\n",
        "      \"seed\": config[\"seed\"]\n",
        "  }\n",
        "\n",
        "  if not os.path.exists(config['working_dir']+config['run_name']):\n",
        "    os.mkdir(config['working_dir']+config['run_name']) \n",
        "  with open(config['working_dir']+config['run_name']+\"/results.json\", \"w\") as write_file:\n",
        "    json.dump(results, write_file)\n",
        "  \n",
        "  #Checkpoint\n",
        "  checkpoints.save_checkpoint(ckpt_dir=config['working_dir']+config['run_name'], target=params, step=config[\"n_epoques\"]*config[\"epis_per_epoque\"])\n",
        "\n",
        "  # Plots\n",
        "  # Mean Losses\n",
        "  plt.plot([(i)*config[\"epis_per_epoque\"] for i in range(len(mean_losses))], mean_losses)\n",
        "  plt.xlabel('Episodes (NN updates)')\n",
        "  plt.ylabel('Mean Losses')\n",
        "  plt.savefig(config['working_dir']+config['run_name']+'/mean_losses.jpg')\n",
        "  plt.close()\n",
        "\n",
        "  # Accuracy\n",
        "  plt.plot([(i)*config[\"epis_per_epoque\"] for i in range(len(mean_accuracy))], mean_accuracy)\n",
        "  plt.xlabel('Episodes (NN updates)')\n",
        "  plt.ylabel('Mean Accuracy (%)')\n",
        "  plt.savefig(config['working_dir']+config['run_name']+'/mean_accuracy.jpg')\n",
        "  plt.close()\n",
        "\n",
        "  # Learning rate schedule   \n",
        "  plt.plot([i for i in range(len(mean_losses))], [config[\"learning_rate\"](i*config[\"epis_per_epoque\"]) for i in range(len(mean_losses))])\n",
        "  plt.xlabel('Episodes (NN updates')\n",
        "  plt.ylabel('Learning Rate')\n",
        "  plt.savefig(config['working_dir']+config['run_name']+'/learning_rate.jpg')\n",
        "  plt.close()\n",
        " \n",
        "  return params, optim, nn_policy, mean_losses, mean_accuracy"
      ],
      "metadata": {
        "id": "tw69_Ic-dcVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlXarixBjTC2"
      },
      "source": [
        "Now we are to configure our experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zWgbr0HjQua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cba58cb-50ae-42a0-b87e-3fa43b48edea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters of NN: 20\n",
            "periods per episode: 256\n",
            "periods per epoque: 256000\n"
          ]
        }
      ],
      "source": [
        "'''Confg dictionary'''\n",
        "\n",
        "lr_schedule = optax.join_schedules(\n",
        "      schedules= [optax.constant_schedule(0.001),\n",
        "                  optax.constant_schedule(0.0005),\n",
        "                  optax.constant_schedule(0.0001),\n",
        "                  optax.constant_schedule(0.00005),\n",
        "                  optax.constant_schedule(0.00001),\n",
        "                  optax.constant_schedule(0.000005)],   \n",
        "      boundaries=[10000,20000,30000,40000,50000]\n",
        "      )\n",
        "\n",
        "# Now we create a config dict\n",
        "config = {\n",
        "    \"periods_per_epis\": 256, # periods per episode\n",
        "    \"epis_per_epoque\": 1000, #  episodes per eqpoque\n",
        "    \"n_epoques\": 100, # number of epoques\n",
        "    \"mc_draws\": 16, # only applies if shock is continuous \n",
        "    \"layers\": [4,4], # layers of the NN\n",
        "    \"learning_rate\": lr_schedule,\n",
        "    \"seed\": 48, # random seed, set to whatever int.\n",
        "    \"reset_env_nepoques\": 1,\n",
        "    \"run_name\": \"rbc_Dec20_ly2x4mc16ppe256\",\n",
        "    \"date\": \"December_20\",\n",
        "    \"working_dir\": \"/content/drive/MyDrive/Jaxecon/Rbc/Training/\" # replace\n",
        "}\n",
        "\n",
        "# Print some key statistics\n",
        "print(\n",
        "    \"Number of parameters of NN:\",\n",
        "    jnp.sum(jnp.array([(config[\"layers\"][i]+1)*config[\"layers\"][i+1] for i in range(len(config[\"layers\"])-1)])))\n",
        "n_cores = len(jax.devices())\n",
        "\n",
        "num_periods_perepisode =  config[\"periods_per_epis\"] \n",
        "print(\"periods per episode:\", num_periods_perepisode)\n",
        "\n",
        "num_periods_pereqpoque =  config[\"epis_per_epoque\"] * config[\"periods_per_epis\"] \n",
        "print(\"periods per epoque:\", num_periods_pereqpoque)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awmY1xXgDOcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3680271-23e4-4681-9319-e271f7e73d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time Elapsed for Compilation: 3.8919308185577393 seconds\n",
            "Time Elapsed for Epoque: 0.5405137538909912 seconds\n",
            "Steps per second: 473623.47055395955 st/s\n",
            "\n",
            "Iteration: 1000 , Mean_loss: 1.8294233e-05 , Mean_accuracy: 0.99646866 , Learning rate: 0.001\n",
            "Iteration: 2000 , Mean_loss: 1.9587576e-05 , Mean_accuracy: 0.9964795 , Learning rate: 0.001\n",
            "ENV RESET\n",
            "Iteration: 3000 , Mean_loss: 6.2493115e-07 , Mean_accuracy: 0.9994198 , Learning rate: 0.001\n",
            "ENV RESET\n",
            "Iteration: 4000 , Mean_loss: 3.7367644e-07 , Mean_accuracy: 0.9995164 , Learning rate: 0.001\n",
            "ENV RESET\n",
            "Iteration: 5000 , Mean_loss: 5.5908345e-07 , Mean_accuracy: 0.99941915 , Learning rate: 0.001\n",
            "ENV RESET\n",
            "Iteration: 6000 , Mean_loss: 4.5684916e-07 , Mean_accuracy: 0.9994634 , Learning rate: 0.001\n",
            "ENV RESET\n",
            "Iteration: 7000 , Mean_loss: 3.5999858e-07 , Mean_accuracy: 0.9995234 , Learning rate: 0.001\n",
            "ENV RESET\n",
            "Iteration: 8000 , Mean_loss: 4.6283262e-07 , Mean_accuracy: 0.9994352 , Learning rate: 0.001\n",
            "ENV RESET\n",
            "Iteration: 9000 , Mean_loss: 2.886756e-07 , Mean_accuracy: 0.9995696 , Learning rate: 0.001\n",
            "ENV RESET\n",
            "Iteration: 10000 , Mean_loss: 5.485599e-07 , Mean_accuracy: 0.9994054 , Learning rate: 0.0005\n",
            "ENV RESET\n",
            "Iteration: 11000 , Mean_loss: 4.691973e-07 , Mean_accuracy: 0.9994664 , Learning rate: 0.0005\n",
            "ENV RESET\n",
            "Iteration: 12000 , Mean_loss: 4.950225e-07 , Mean_accuracy: 0.999419 , Learning rate: 0.0005\n",
            "ENV RESET\n",
            "Iteration: 13000 , Mean_loss: 3.2675922e-07 , Mean_accuracy: 0.9995462 , Learning rate: 0.0005\n",
            "ENV RESET\n",
            "Iteration: 14000 , Mean_loss: 4.505903e-07 , Mean_accuracy: 0.9994693 , Learning rate: 0.0005\n",
            "ENV RESET\n",
            "Iteration: 15000 , Mean_loss: 3.553412e-07 , Mean_accuracy: 0.99953216 , Learning rate: 0.0005\n",
            "ENV RESET\n",
            "Iteration: 16000 , Mean_loss: 3.6407081e-07 , Mean_accuracy: 0.9995229 , Learning rate: 0.0005\n",
            "ENV RESET\n",
            "Iteration: 17000 , Mean_loss: 4.58737e-07 , Mean_accuracy: 0.9994528 , Learning rate: 0.0005\n",
            "ENV RESET\n",
            "Iteration: 18000 , Mean_loss: 3.496624e-07 , Mean_accuracy: 0.9995334 , Learning rate: 0.0005\n",
            "ENV RESET\n",
            "Iteration: 19000 , Mean_loss: 4.043071e-07 , Mean_accuracy: 0.9994987 , Learning rate: 0.0005\n",
            "ENV RESET\n",
            "Iteration: 20000 , Mean_loss: 4.3450294e-07 , Mean_accuracy: 0.99946845 , Learning rate: 1e-04\n",
            "ENV RESET\n",
            "Iteration: 21000 , Mean_loss: 3.4109206e-07 , Mean_accuracy: 0.9995298 , Learning rate: 1e-04\n",
            "ENV RESET\n",
            "Iteration: 22000 , Mean_loss: 4.4242321e-07 , Mean_accuracy: 0.99945724 , Learning rate: 1e-04\n",
            "ENV RESET\n",
            "Iteration: 23000 , Mean_loss: 3.1458615e-07 , Mean_accuracy: 0.9995413 , Learning rate: 1e-04\n",
            "ENV RESET\n",
            "Iteration: 24000 , Mean_loss: 3.5156478e-07 , Mean_accuracy: 0.99953747 , Learning rate: 1e-04\n",
            "ENV RESET\n",
            "Iteration: 25000 , Mean_loss: 4.8690856e-07 , Mean_accuracy: 0.99945307 , Learning rate: 1e-04\n",
            "ENV RESET\n",
            "Iteration: 26000 , Mean_loss: 3.375016e-07 , Mean_accuracy: 0.9995453 , Learning rate: 1e-04\n",
            "ENV RESET\n",
            "Iteration: 27000 , Mean_loss: 5.02257e-07 , Mean_accuracy: 0.99943256 , Learning rate: 1e-04\n",
            "ENV RESET\n",
            "Iteration: 28000 , Mean_loss: 6.8405996e-07 , Mean_accuracy: 0.9993388 , Learning rate: 1e-04\n",
            "ENV RESET\n",
            "Iteration: 29000 , Mean_loss: 4.0926437e-07 , Mean_accuracy: 0.99948585 , Learning rate: 1e-04\n",
            "ENV RESET\n",
            "Iteration: 30000 , Mean_loss: 3.865557e-07 , Mean_accuracy: 0.999502 , Learning rate: 5e-05\n",
            "ENV RESET\n",
            "Iteration: 31000 , Mean_loss: 4.3408636e-07 , Mean_accuracy: 0.9994733 , Learning rate: 5e-05\n",
            "ENV RESET\n",
            "Iteration: 32000 , Mean_loss: 4.620156e-07 , Mean_accuracy: 0.99944925 , Learning rate: 5e-05\n",
            "ENV RESET\n",
            "Iteration: 33000 , Mean_loss: 4.804633e-07 , Mean_accuracy: 0.99944013 , Learning rate: 5e-05\n",
            "ENV RESET\n",
            "Iteration: 34000 , Mean_loss: 4.5403166e-07 , Mean_accuracy: 0.9994703 , Learning rate: 5e-05\n",
            "ENV RESET\n",
            "Iteration: 35000 , Mean_loss: 4.1469966e-07 , Mean_accuracy: 0.9994897 , Learning rate: 5e-05\n",
            "ENV RESET\n",
            "Iteration: 36000 , Mean_loss: 4.4602464e-07 , Mean_accuracy: 0.99944353 , Learning rate: 5e-05\n",
            "ENV RESET\n",
            "Iteration: 37000 , Mean_loss: 3.304475e-07 , Mean_accuracy: 0.9995423 , Learning rate: 5e-05\n",
            "ENV RESET\n",
            "Iteration: 38000 , Mean_loss: 4.0778713e-07 , Mean_accuracy: 0.99949217 , Learning rate: 5e-05\n",
            "ENV RESET\n",
            "Iteration: 39000 , Mean_loss: 2.9236594e-07 , Mean_accuracy: 0.9995694 , Learning rate: 5e-05\n",
            "ENV RESET\n",
            "Iteration: 40000 , Mean_loss: 4.793652e-07 , Mean_accuracy: 0.9994396 , Learning rate: 1e-05\n",
            "ENV RESET\n",
            "Iteration: 41000 , Mean_loss: 4.606039e-07 , Mean_accuracy: 0.99945974 , Learning rate: 1e-05\n",
            "ENV RESET\n",
            "Iteration: 42000 , Mean_loss: 5.492602e-07 , Mean_accuracy: 0.99940675 , Learning rate: 1e-05\n",
            "ENV RESET\n",
            "Iteration: 43000 , Mean_loss: 3.5321716e-07 , Mean_accuracy: 0.9995179 , Learning rate: 1e-05\n",
            "ENV RESET\n",
            "Iteration: 44000 , Mean_loss: 4.996678e-07 , Mean_accuracy: 0.99944437 , Learning rate: 1e-05\n",
            "ENV RESET\n",
            "Iteration: 45000 , Mean_loss: 4.5616082e-07 , Mean_accuracy: 0.9994813 , Learning rate: 1e-05\n",
            "ENV RESET\n",
            "Iteration: 46000 , Mean_loss: 3.528678e-07 , Mean_accuracy: 0.9995346 , Learning rate: 1e-05\n",
            "ENV RESET\n",
            "Iteration: 47000 , Mean_loss: 3.4809486e-07 , Mean_accuracy: 0.9995284 , Learning rate: 1e-05\n",
            "ENV RESET\n",
            "Iteration: 48000 , Mean_loss: 4.543415e-07 , Mean_accuracy: 0.99946296 , Learning rate: 1e-05\n",
            "ENV RESET\n",
            "Iteration: 49000 , Mean_loss: 4.3722304e-07 , Mean_accuracy: 0.9994737 , Learning rate: 1e-05\n",
            "ENV RESET\n",
            "Iteration: 50000 , Mean_loss: 4.4318716e-07 , Mean_accuracy: 0.9994724 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 51000 , Mean_loss: 5.0959807e-07 , Mean_accuracy: 0.99946076 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 52000 , Mean_loss: 4.0725766e-07 , Mean_accuracy: 0.999485 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 53000 , Mean_loss: 3.5616665e-07 , Mean_accuracy: 0.9995184 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 54000 , Mean_loss: 4.5164927e-07 , Mean_accuracy: 0.9994712 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 55000 , Mean_loss: 4.150123e-07 , Mean_accuracy: 0.9994783 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 56000 , Mean_loss: 3.5634585e-07 , Mean_accuracy: 0.99951595 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 57000 , Mean_loss: 3.6124112e-07 , Mean_accuracy: 0.9995321 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 58000 , Mean_loss: 4.41241e-07 , Mean_accuracy: 0.9994746 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 59000 , Mean_loss: 4.6152817e-07 , Mean_accuracy: 0.99945146 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 60000 , Mean_loss: 6.1049246e-07 , Mean_accuracy: 0.9994521 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 61000 , Mean_loss: 4.131921e-07 , Mean_accuracy: 0.99947155 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 62000 , Mean_loss: 4.3703517e-07 , Mean_accuracy: 0.9994858 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 63000 , Mean_loss: 3.269772e-07 , Mean_accuracy: 0.99954355 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 64000 , Mean_loss: 3.974757e-07 , Mean_accuracy: 0.9994924 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 65000 , Mean_loss: 3.8474948e-07 , Mean_accuracy: 0.9995001 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 66000 , Mean_loss: 4.0209252e-07 , Mean_accuracy: 0.99950373 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 67000 , Mean_loss: 3.1552133e-07 , Mean_accuracy: 0.99955666 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 68000 , Mean_loss: 6.5103586e-07 , Mean_accuracy: 0.99934065 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 69000 , Mean_loss: 4.4219092e-07 , Mean_accuracy: 0.9994618 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 70000 , Mean_loss: 4.5312032e-07 , Mean_accuracy: 0.9994464 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 71000 , Mean_loss: 5.835537e-07 , Mean_accuracy: 0.9993892 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 72000 , Mean_loss: 2.9662812e-07 , Mean_accuracy: 0.99955845 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 73000 , Mean_loss: 3.97307e-07 , Mean_accuracy: 0.9994958 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 74000 , Mean_loss: 4.7028718e-07 , Mean_accuracy: 0.999501 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 75000 , Mean_loss: 3.0608425e-07 , Mean_accuracy: 0.9995594 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 76000 , Mean_loss: 5.876674e-07 , Mean_accuracy: 0.99938077 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 77000 , Mean_loss: 2.5813398e-07 , Mean_accuracy: 0.9995785 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 78000 , Mean_loss: 6.290002e-07 , Mean_accuracy: 0.99936926 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 79000 , Mean_loss: 3.1555948e-07 , Mean_accuracy: 0.99955857 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 80000 , Mean_loss: 3.6858307e-07 , Mean_accuracy: 0.9995284 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 81000 , Mean_loss: 3.567498e-07 , Mean_accuracy: 0.99952877 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 82000 , Mean_loss: 4.978084e-07 , Mean_accuracy: 0.9994397 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 83000 , Mean_loss: 4.937897e-07 , Mean_accuracy: 0.9994429 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 84000 , Mean_loss: 4.0535883e-07 , Mean_accuracy: 0.99949485 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 85000 , Mean_loss: 5.934969e-07 , Mean_accuracy: 0.9993806 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 86000 , Mean_loss: 3.476781e-07 , Mean_accuracy: 0.9995222 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 87000 , Mean_loss: 3.8956114e-07 , Mean_accuracy: 0.99951416 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 88000 , Mean_loss: 4.5203365e-07 , Mean_accuracy: 0.99947107 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 89000 , Mean_loss: 4.1731147e-07 , Mean_accuracy: 0.99947625 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 90000 , Mean_loss: 4.4986496e-07 , Mean_accuracy: 0.9994816 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 91000 , Mean_loss: 4.886456e-07 , Mean_accuracy: 0.9994368 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 92000 , Mean_loss: 4.01268e-07 , Mean_accuracy: 0.99948555 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 93000 , Mean_loss: 3.142929e-07 , Mean_accuracy: 0.9995583 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 94000 , Mean_loss: 3.7453916e-07 , Mean_accuracy: 0.99951077 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 95000 , Mean_loss: 3.7029722e-07 , Mean_accuracy: 0.99950457 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 96000 , Mean_loss: 3.9582858e-07 , Mean_accuracy: 0.99948883 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 97000 , Mean_loss: 3.3454967e-07 , Mean_accuracy: 0.9995401 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 98000 , Mean_loss: 4.825639e-07 , Mean_accuracy: 0.99943614 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 99000 , Mean_loss: 4.981815e-07 , Mean_accuracy: 0.9994267 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Iteration: 100000 , Mean_loss: 4.1093182e-07 , Mean_accuracy: 0.9994749 , Learning rate: 5e-06\n",
            "ENV RESET\n",
            "Minimum loss attained in training: 2.5813397996898857e-07\n",
            "Maximum accuracy attained in training: 0.9995784759521484\n",
            "Time Elapsed for Full Experiment: 0.9101432085037231 minutes\n"
          ]
        }
      ],
      "source": [
        "# Run Experiment\n",
        "params_trained, optim, nn_policy, mean_losses, mean_accuracy = run_experiment(Model(), config)\n",
        "# Close the session\n",
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Test Results\n",
        "\n",
        "We will first test that the environment works as its supposed to. First, we check if the error in the steady state is 0 under the loglinear policy. for that, we will firsr create a function that calculates the loss in the steady state"
      ],
      "metadata": {
        "id": "v0Vb1mt4G-Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 1: steady state"
      ],
      "metadata": {
        "id": "hIKEQKv2HtXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test policies in steady state\n",
        "env = Model()\n",
        "params_test = params_trained # Choose parameteres to train\n",
        "rng_test = random.PRNGKey(1)\n",
        "\n",
        "# test steady state policies\n",
        "obs_init = env.initial_obs(rng_test)\n",
        "obs_ss = jnp.zeros_like(obs_init)\n",
        "policy_ss = nn_policy(params_test, obs_ss) # nn_policy is the nn policy that comes from the training experiment\n",
        "print(\"Pretrain Policy in ss:\", policy_ss)\n",
        "\n",
        "print(\"Neural Net Gives SS polices in obs_ss?\",jnp.allclose(policy_ss,jnp.exp(env.policy_ss), rtol=1e-02))\n",
        "\n",
        "# check that steady state policies takes a zero step\n",
        "shock_ss = 0\n",
        "obs_next_ss = env.step(obs_ss, policy_ss, shock_ss)\n",
        "print(\"Nex obs after ss policies (should be ~ 0\")\n",
        "print(obs_next_ss)\n",
        "\n",
        "# take a step\n",
        "start = time()\n",
        "obs_next_ss_current = env.step(obs_ss, policy_ss, shock_ss)\n",
        "finish = time()\n",
        "print(f\"executing a step took {(finish-start)*1000} miliseconds\")\n",
        "print(obs_next_ss_current)\n",
        "print(\"Difference between neural net output and calculates steady state policy\", jnp.max(obs_next_ss-obs_ss))\n",
        "\n",
        "# calculate expectation\n",
        "start = time()\n",
        "exp_ss = env.expect_realization(obs_ss, policy_ss)\n",
        "print(\"Expectation in SS:\", exp_ss)\n",
        "finish = time()\n",
        "print(f\"executing an expectation realization took {(finish-start)*1000} miliseconds\")\n",
        "\n",
        "# calculate loss\n",
        "start = time()\n",
        "loss, _ = env.loss(obs_ss,exp_ss, policy_ss)\n",
        "print(\"Loss in SS:\", loss)\n",
        "finish = time()\n",
        "print(f\"executing the loss function {(finish-start)*1000} miliseconds\")\n",
        "# print(\"Loss in steady state (should be ~ zero):\", loss)\n",
        "\n",
        "# print(\"Log linear state\")\n",
        "# next_state = env.step(obs_ss,shock_ss)\n",
        "# print(next_state)"
      ],
      "metadata": {
        "id": "tMDHWyamqM8g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bde750a-580e-41ae-faec-1d50fc309029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrain Policy in ss: [0.9992661]\n",
            "Neural Net Gives SS polices in obs_ss? False\n",
            "Nex obs after ss policies (should be ~ 0\n",
            "[-2.193451e-05  0.000000e+00  0.000000e+00]\n",
            "executing a step took 7.3413848876953125 miliseconds\n",
            "[-2.193451e-05  0.000000e+00  0.000000e+00]\n",
            "Difference between neural net output and calculates steady state policy 0.0\n",
            "Expectation in SS: 0.5632294\n",
            "executing an expectation realization took 8.684158325195312 miliseconds\n",
            "Loss in SS: 1.4210855e-14\n",
            "executing the loss function 7.289409637451172 miliseconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 2: step and loss function\n",
        "\n",
        "Now we test that the step function and loss function works correctly."
      ],
      "metadata": {
        "id": "duUIS9EaCIZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Model()\n",
        "rng_test = random.PRNGKey(0)\n",
        "params_test = params_trained\n",
        "nn_policy = MLP_softplus(config[\"layers\"] + [env.n_actions])\n",
        "nn_forward = nn_policy.apply\n",
        "\n",
        "# test envreset\n",
        "obs_init = env.initial_obs(rng_test)\n",
        "print(\"shape of initial obs:\", obs_init.shape)\n",
        "\n",
        "# test step\n",
        "shock = env.sample_shock(rng_test)\n",
        "policy = nn_forward(params_test, obs_init)\n",
        "obs_next = env.step(obs_init,policy,shock)\n",
        "print(\"shape of next obs\", obs_next.shape)\n",
        "\n",
        "# test expectation\n",
        "mc_shocks = env.mc_shocks()\n",
        "mc_nextobs = jax.vmap(env.step, in_axes=(None, None,0))(obs_init,  policy, mc_shocks)\n",
        "mc_nextpols = nn_forward(params_test,jnp.stack(mc_nextobs))\n",
        "exp = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols))\n",
        "print(\"shape of exp\", exp.shape)\n",
        "print(\"exp\", exp)\n",
        "\n",
        "#test loss\n",
        "loss = env.loss(obs_init, exp, policy)\n",
        "print(\"shape of loss\", loss[0].shape)\n",
        "print(\"loss\", loss)\n"
      ],
      "metadata": {
        "id": "IRycSm2iCKKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a150f3f-0ab1-4758-e0eb-6eebdd1b3959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of initial obs: (3,)\n",
            "shape of next obs (3,)\n",
            "shape of exp ()\n",
            "exp 0.5632846\n",
            "shape of loss ()\n",
            "loss (DeviceArray(6.130462e-08, dtype=float32), DeviceArray(0.9997524, dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 3: Simulating the environment (only states and policy, no loss)"
      ],
      "metadata": {
        "id": "3x-jPOefIMfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate env \n",
        "env = Model()\n",
        "params_test = params_trained\n",
        "rng_test = random.PRNGKey(0)\n",
        "nn_policy = MLP_softplus(config[\"layers\"] + [env.n_actions])\n",
        "nn_forward = nn_policy.apply\n",
        "obs_init = env.initial_obs(rng_test)\n",
        "\n",
        "def period_step_fn(obs, shock, nn_params):\n",
        "    \n",
        "  # Step the environemnt\n",
        "  policy = nn_forward(nn_params, obs)\n",
        "  # obs_next = env.step_loglinear(obs, shock)  # apply period steps for each row shock in shocks.\n",
        "  obs_next = env.step(obs, policy, shock)  # apply period steps for each row shock in shocks.\n",
        "\n",
        "  return obs_next, (obs_next, policy)\n",
        "\n",
        "# test envreset\n",
        "\n",
        "n_periods = 1000000\n",
        "shocks = random.normal(rng_test, shape=(n_periods,))\n",
        "period_step = jax.tree_util.Partial(period_step_fn, nn_params = params_test)\n",
        "start = time()\n",
        "obs_final, obs_policy_pair = lax.scan(period_step,obs_init,shocks)\n",
        "finish = time()\n",
        "print(f\"Simulating {n_periods} with only state evolution given policy took {(finish-start)} seconds\")\n",
        "\n",
        "obs, policy = obs_policy_pair\n",
        "print('mean for each state (should be ~ 0)')\n",
        "print(jnp.mean(obs,axis=0))\n",
        "print('std for each state (should be ~ 1)')\n",
        "print(jnp.std(obs, axis=0))\n",
        "print('average policy', jnp.mean(policy,axis=0))"
      ],
      "metadata": {
        "id": "pPZz_th8ITA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f12b50c-9c16-4e42-cab0-85aa5835f5a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulating 1000000 with only state evolution given policy took 0.41986608505249023 seconds\n",
            "mean for each state (should be ~ 0)\n",
            "[5.5361754e-04 3.3594817e-05 3.3591059e-04]\n",
            "std for each state (should be ~ 1)\n",
            "[0.03446417 0.02294647 1.0005815 ]\n",
            "average policy [1.0011472]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 4: Evaluate loss in fixed simulation"
      ],
      "metadata": {
        "id": "uCl6x_gesykv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate env \n",
        "env = Model()\n",
        "rng_test = random.PRNGKey(0)\n",
        "def get_shock_stats(rng,n_periods):\n",
        "  rng = random.PRNGKey(1000)\n",
        "  shocks = jax.random.multivariate_normal(rng, jnp.zeros((env.n_sectors,)), env.Sigma_A, shape=(n_periods,))\n",
        "  return jnp.mean(shocks,axis=0), jnp.std(shocks,axis=0), jnp.transpose(jnp.quantile(shocks, jnp.array([0.13,0.25,0.38,0.5,0.62,0.75,0.87]), axis=0))\n",
        "\n",
        "mean_shocks, sd_shocks, qtl_shocks = get_shock_stats(rng_test,1000000)\n",
        "\n",
        "reg_test, rng_simul = random.split(rng_test)\n",
        "nn_policy = MLP_softplus(config[\"layers\"] + [env.n_actions])\n",
        "nn_forward = nn_policy.apply\n",
        "obs_init = env.initial_obs(rng_test)\n",
        "mc_draws = 2000\n",
        "n_simul = 5000 \n",
        "\n",
        "def get_simul_fn(qtl_shocks, env, nn_policy, nn_params, env_obs, n_simul, mc_draws, rng_simul):\n",
        "  def get_closest(shock,grid):\n",
        "      # return jnp.min(jnp.abs(grid-jnp.float32(shock)))+shock\n",
        "      return grid[jnp.argmin(jnp.abs(grid-shock))]\n",
        "  get_closest_vector = jax.vmap(get_closest)\n",
        "  get_closest_array = jax.vmap(get_closest_vector, in_axes = (0,None))\n",
        "\n",
        "  def period_loss_fn(env_obs, period_rng):\n",
        "      \n",
        "    period_rng, mc_rng = random.split(period_rng)   # get rngs\n",
        "    # calculate exp\n",
        "    policy = nn_policy(nn_params, env_obs)\n",
        "    mc_shocks = jax.random.multivariate_normal(mc_rng, jnp.zeros((env.n_sectors,)), env.Sigma_A, shape=(mc_draws,))  \n",
        "    mc_nextobs = jax.vmap(env.step, in_axes = (None,None,0))(env_obs, policy, mc_shocks)\n",
        "    mc_nextpols = nn_policy(nn_params, mc_nextobs)\n",
        "    expect = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols))\n",
        "    quad_loss, abs_loss = env.loss(env_obs, expect, policy) # calculate loss\n",
        "    # Sample next obs\n",
        "    period_shock = jax.random.multivariate_normal(period_rng, jnp.zeros((env.n_sectors,)), env.Sigma_A)\n",
        "    period_shock_disc = get_closest_vector(period_shock, qtl_shocks) # discretize on a grid\n",
        "    # obs_next = env.step(env_obs, policy, period_shock_disc)  # apply period steps for each row shock in shocks.\n",
        "    obs_next = env.step_loglinear(env_obs, period_shock_disc)  # apply period steps for each row shock in shocks.\n",
        "    return obs_next, (quad_loss, abs_loss)\n",
        "      \n",
        "  def simul_loss_fn():\n",
        "    period_rngs = random.split(rng_simul, n_simul)\n",
        "    obs_final, metrics = lax.scan(period_loss_fn, env_obs, jnp.stack(period_rngs))\n",
        "    quad_losses, accuracy = metrics\n",
        "    mean_quad_losses = jnp.mean(quad_losses)\n",
        "    mean_accuracy = jnp.mean(accuracy)\n",
        "    aux_info = (obs_final, jnp.array([mean_quad_losses]), jnp.array([mean_accuracy]))\n",
        "    return mean_quad_losses, aux_info, quad_losses, accuracy\n",
        "\n",
        "  return simul_loss_fn\n",
        "\n",
        "simul_fn_jitted = jax.jit(get_simul_fn(qtl_shocks, env, nn_forward, params, obs_init, n_simul,mc_draws, rng_simul))\n",
        "\n",
        "mean_quad_losses, aux_info, quad_losses, accuracy = simul_fn_jitted()\n",
        "\n"
      ],
      "metadata": {
        "id": "zdt21KpZs8B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(aux_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oFh-JZVfVsy",
        "outputId": "11e99082-8359-4804-bf0e-7e6e62ed2f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(DeviceArray([ 1.0403451e+00, -1.6905632e-02, -1.2537338e-02,\n",
            "              8.0404788e-01, -5.6612349e-01, -7.7403051e-01,\n",
            "             -2.4841851e-01, -5.2038854e-01,  6.1690104e-01,\n",
            "             -3.5971797e-01, -3.8373685e-01,  5.4313976e-01,\n",
            "             -8.0975868e-02, -1.3529728e-01,  1.1451080e+00,\n",
            "             -3.0299145e-01,  2.8066963e-01,  6.1999637e-01,\n",
            "             -9.8308891e-02, -9.5078933e-01, -4.1164610e-01,\n",
            "             -7.9166234e-02,  1.4708129e-01,  4.8137006e-01,\n",
            "              3.6902660e-01, -4.3758699e-01,  8.1045485e-01,\n",
            "              6.8829703e-01, -4.0977463e-02,  1.2977964e-01,\n",
            "              1.2651724e+00,  1.0551288e-01,  7.7827448e-01,\n",
            "             -3.4922424e-01, -7.3109262e-02,  5.7056034e-01,\n",
            "             -2.9369785e-02, -8.1830117e-04,  4.0362591e-01,\n",
            "             -1.2483090e-01, -1.6497889e+00,  1.0977222e+00,\n",
            "              8.9612323e-01,  3.2554069e-01,  8.1307924e-01,\n",
            "             -6.3473421e-01,  6.9432724e-01,  7.4148023e-01,\n",
            "             -1.3198723e+00,  7.8183562e-01, -3.0833891e-01,\n",
            "             -6.5623671e-02,  7.1856540e-01, -3.1533644e-01,\n",
            "              9.9324407e-03,  9.9839669e-01,  1.0844616e+00,\n",
            "              9.1143543e-01, -4.1825756e-01,  8.6495280e-01,\n",
            "             -8.1065893e-02,  2.2366826e-01,  4.7965291e-01,\n",
            "             -1.3779367e+00,  1.5673090e+00,  7.2829688e-01,\n",
            "              9.5400608e-01, -7.6635247e-01,  4.5406225e-01,\n",
            "              5.9585470e-01,  2.6022342e-01,  4.1893449e-01,\n",
            "              2.6371428e-01,  2.5844455e-01, -1.1258849e+00,\n",
            "              8.3288971e-05, -6.7500037e-01, -1.1239820e+00,\n",
            "              3.0433631e-01, -1.7211322e-03, -3.0456921e-01,\n",
            "              1.1252252e+00,  4.6910951e-04,  6.7277563e-01,\n",
            "              3.0523235e-01, -1.1219611e+00,  2.3598082e-03,\n",
            "             -1.1269696e+00,  3.0741885e-01,  3.0431271e-01,\n",
            "             -1.1257377e+00, -9.9704461e-04,  1.1263325e+00,\n",
            "              6.7684227e-01,  1.1227989e+00, -6.7808479e-01,\n",
            "              3.0422887e-01, -6.7100745e-01, -3.0422676e-01,\n",
            "              1.1254449e+00, -1.1267393e+00,  1.1282909e+00,\n",
            "              1.1257057e+00,  6.7678291e-01,  1.1223012e+00,\n",
            "              1.1247811e+00,  1.1254721e+00,  1.1320536e+00,\n",
            "             -6.7544782e-01, -3.0656344e-01, -1.1295037e+00],            dtype=float32), DeviceArray([2.6908285e-06], dtype=float32), DeviceArray([0.9988601], dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 5: Accuracy of Montecarlo Estimation"
      ],
      "metadata": {
        "id": "aJDp4kuLHqyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate montecarlo estimation of expectation\n",
        "env = Model()\n",
        "rng_mc = random.PRNGKey(0)\n",
        "obs_init = env.initial_obs(rng_mc)\n",
        "nn_policy = MLP_softplus(config[\"layers\"] + [env.n_actions])\n",
        "nn_forward = nn_policy.apply\n",
        "\n",
        "def get_expectation_fn(nn_forward, params, env):\n",
        "  def expectation(n_draws, rng,  obs):\n",
        "    policy = nn_forward(params, obs)\n",
        "    mc_obs = env.mc_shocks(rng, mc_draws=n_draws)\n",
        "    mc_nextobs = jax.vmap(env.step, in_axes=(None, None,0))(obs, policy, mc_obs)\n",
        "    mc_nextpols = nn_forward(params,jnp.stack(mc_nextobs))\n",
        "    exp = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols))\n",
        "    return exp\n",
        "  return expectation\n",
        "\n",
        "rng_mc, *mc_rngs = jax.random.split(rng_mc, 100 + 1)\n",
        "\n",
        "obs_list = [obs_init]\n",
        "n_draws_list = [16, 32, 64, 128, 256]\n",
        "# n_draws_list = [500, 1000]\n",
        "rng_list =list(mc_rngs)\n",
        "\n",
        "mc_explist = {n_draws_list[i]: [] for i in range(len(n_draws_list))}\n",
        "mc_error = {f\"error_{n_draws_list[i]}_draws\": [] for i in range(len(n_draws_list))}\n",
        "for i in range(len(n_draws_list)):\n",
        "  for j in range(len(rng_list)):\n",
        "    for k in range(len(obs_list)):\n",
        "      # exp_jitted = jax.jit(get_expectation_fn(nn_forward, params, env)).lower(n_draws_list[i], rng_list[j], obs_list[k]).compile\n",
        "      mc_explist[n_draws_list[i]].append(get_expectation_fn(nn_forward, params_trained, env)(n_draws_list[i], rng_list[j],  obs_list[k]))\n",
        "      # mc_eval[(i,j,k)] = exp_jitted(n_draws_list[i], rng_list[j],  obs_list[k])\n",
        "  mc_error[f\"error_{n_draws_list[i]}_draws\"] = jnp.std(jnp.array(mc_explist[n_draws_list[i]]))\n",
        "print(mc_error)\n",
        "\n",
        "error_list = [float(mc_error[f\"error_{n_draws_list[i]}_draws\"]) for i in range(len(n_draws_list))]\n",
        "print(error_list)\n",
        "n_drawsstr_list = [str(n_draws_list[i]) for i in range(len(n_draws_list))]\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(n_drawsstr_list, error_list)\n",
        "plt.xlabel('Number of Montecarlo draws')\n",
        "plt.ylabel('Standard Deviation of expectations')\n",
        "# plt.show\n",
        "fig.savefig(config['working_dir']+config['run_name']+'/mc_eval.jpg', bbox_inches=\"tight\", pad_inches=1)\n",
        "  "
      ],
      "metadata": {
        "id": "dq6XyondH93j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "abf00868-30e1-43c8-de06-c575f1e6d6b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'error_16_draws': DeviceArray(0.00019636, dtype=float32), 'error_32_draws': DeviceArray(0.00013121, dtype=float32), 'error_64_draws': DeviceArray(8.696818e-05, dtype=float32), 'error_128_draws': DeviceArray(6.727537e-05, dtype=float32), 'error_256_draws': DeviceArray(4.5715475e-05, dtype=float32)}\n",
            "[0.00019636347133200616, 0.0001312146196141839, 8.696818258613348e-05, 6.727536674588919e-05, 4.57154746982269e-05]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAFNCAYAAAByowfoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7wdZXn28d9lwkGRM6kgBxMgSoNy6gYPtVhAORRL1IKGl1ZsobEWxFaLJO37AqK8Ba3yFgtqlFREa0BASSVyUA5SFUKQczS6G4IkpSUm4eCBYML1/jHP1sVi7b0nMbN29l7X9/NZnz3rmZl77lka1r1mnnke2SYiIiJ60wtGOoGIiIgYOSkEIiIielgKgYiIiB6WQiAiIqKHpRCIiIjoYSkEIiIietj4kU5gtNlhhx08ceLEkU4jIiJindx1110/tT2hvT2FwDqaOHEiCxYsGOk0IiIi1omkhzu159ZARERED0shEBER0cNSCERERPSwRgsBSUdKWiSpX9KMDus3k3R5WX+HpIkt62aW9kWSjhgupqQvlfYHJM2WtElpl6QLy/b3STqgZZ8TJf24vE5s6nOIiIjYWDVWCEgaB1wEHAVMAY6XNKVts5OAVbb3BC4Azi/7TgGmAXsDRwIXSxo3TMwvAXsBrwJeCJxc2o8CJpfXdOBT5RjbAWcBrwYOAs6StO2G/AwiIiI2dk1eETgI6Le92PYzwBxgats2U4FLy/KVwGGSVNrn2F5t+yGgv8QbNKbteS6A+cAuLcf4Qll1O7CNpJ2AI4Abba+0vQq4karoiIiI6BlNFgI7A4+0vF9a2jpuY3sN8ASw/RD7Dhuz3BL4M+C6YfKok99AzOmSFkhasHz58k6bREREjEpjsbPgxcC3bd+2oQLanmW7z3bfhAnPG4shIiJi1GqyEFgG7NryfpfS1nEbSeOBrYEVQ+w7ZExJZwETgPfXyKNOfhEREWNak4XAncBkSZMkbUrV+W9u2zZzgYHe+scCN5V7/HOBaeWpgklUHf3mDxVT0slU9/2Pt/1s2zHeWZ4eeA3whO1HgeuBwyVtWzoJHl7aIiIiekZjQwzbXiPpVKov13HAbNsPSjoHWGB7LnAJcJmkfmAl1Rc7ZbsrgIXAGuAU22sBOsUsh/w08DDwvaq/IVfbPgeYB/wRVYfDXwB/Xo6xUtKHqYoLgHNsr2zq84iIiNgYqfoBHnX19fU5cw1ERMRoI+ku233t7Zl0aARNnHHtSKewUVhy3tEjnUJERM8ai08NRERERE0pBCIiInpYCoGIiIgelkIgIiKih6UQiIiI6GEpBCIiInpYCoGIiIgelkIgIiKih6UQiIiI6GEpBCIiInpYCoGIiIgelkIgIiKih6UQiIiI6GEpBCIiInpYCoGIiIgelkIgIiKih6UQiIiI6GEpBCIiInpYCoGIiIgelkIgIiKih6UQiIiI6GEpBCIiInpYCoGIiIgelkIgIiKih6UQiIiI6GGNFgKSjpS0SFK/pBkd1m8m6fKy/g5JE1vWzSztiyQdMVxMSaeWNkvaoaX9dEn3lNcDktZK2q6sWyLp/rJuQVOfQ0RExMaqsUJA0jjgIuAoYApwvKQpbZudBKyyvSdwAXB+2XcKMA3YGzgSuFjSuGFifgd4I/Bw6wFsf8z2frb3A2YCt9pe2bLJIWV934Y694iIiNGiySsCBwH9thfbfgaYA0xt22YqcGlZvhI4TJJK+xzbq20/BPSXeIPGtH237SXD5HQ88OXf/tQiIiLGhiYLgZ2BR1reLy1tHbexvQZ4Ath+iH3rxOxI0ouori5c1dJs4AZJd0maPsS+0yUtkLRg+fLldQ4XERExKvRSZ8E/Br7Tdlvg9bYPoLrVcIqkgzvtaHuW7T7bfRMmTOhGrhEREV3RZCGwDNi15f0upa3jNpLGA1sDK4bYt07MwUyj7baA7WXl72PAV6luPURERPSMJguBO4HJkiZJ2pTqi3hu2zZzgRPL8rHATbZd2qeVpwomAZOB+TVjPo+krYE3ANe0tG0hacuBZeBw4IH1PtuIiIhRaHxTgW2vkXQqcD0wDpht+0FJ5wALbM8FLgEuk9QPrKT6YqdsdwWwEFgDnGJ7LVSPCbbHLO2nAR8EdgTukzTP9sklnbcCN9j+eUuKLwG+WvVNZDzwb7ava+rziIiI2Bip+gEedfX19XnBgg0z5MDEGddukDij3ZLzjh7pFCIixjxJd3V6VL6XOgtGREREmxQCERERPSyFQERERA9LIRAREdHDUghERET0sBQCERERPSyFQERERA9LIRAREdHDhi0EJH1U0laSNpH0LUnLJf1pN5KLiIiIZtW5InC47SeBNwNLgD2B05tMKiIiIrqjTiEwMB/B0cBXbD/RYD4RERHRRXUmHfq6pB8CvwTeI2kC8HSzaUVEREQ3DHtFwPYM4HVAn+1fAT8HpjadWERERDSv7jTEewETJbVu/4UG8omIiIguGrYQkHQZsAdwD7C2NJsUAhEREaNenSsCfcAU2246mYiIiOiuOk8NPADs2HQiERER0X11rgjsACyUNB9YPdBo+5jGsoqIiIiuqFMInN10EhERETEyhi0EbN8q6SXAgaVpvu3Hmk0rIiIiuqHOXANvB+YDxwFvB+6QdGzTiUVERETz6twa+AfgwIGrAGVkwW8CVzaZWERERDSvzlMDL2i7FbCi5n4RERGxkatzReA6SdcDXy7v3wHMay6liIiI6JY6nQVPl/QnwO+Xplm2v9psWhEREdENteYasH0VcFXDuURERESXDXqvX9J/lL9PSXqy5fWUpCfrBJd0pKRFkvolzeiwfjNJl5f1d0ia2LJuZmlfJOmI4WJKOrW0WdIOLe1/KOkJSfeU15l184uIiBjrBr0iYPv15e+W6xNY0jjgIuBNwFLgTklzbS9s2ewkYJXtPSVNA84H3iFpCjAN2Bt4KfBNSS8v+wwW8zvA14FbOqRzm+03r0d+ERERY1qdcQQuq9PWwUFAv+3Ftp8B5gBT27aZClxalq8EDpOk0j7H9mrbDwH9Jd6gMW3fbXtJjbzWJb+IiIgxrc5jgHu3vpE0Hvi9GvvtDDzS8n5paeu4je01wBPA9kPsWydmJ6+VdK+kb0gaOJ/asSRNl7RA0oLly5fXOFxERMToMFQfgZmSngL2ae0fAPwPcE3XMvztfR94me19gU8CX1vXALZn2e6z3TdhwoQNnmBERMRIGbQQsP2PpX/Ax2xvVV5b2t7e9swasZcBu7a836W0ddymXGnYmmrAosH2rROz/TyetP2zsjwP2KR0JlznWBEREWPNsLcGbM+UtK2kgyQdPPCqEftOYLKkSZI2per8N7dtm7nAiWX5WOAm2y7t08pTBZOAyVTzHdSJ+RySdiz9DpB0UDnnFesTKyIiYqwZdhwBSScD76P6xXwP8Brge8ChQ+1ne42kU4HrgXHAbNsPSjoHWGB7LnAJcJmkfmAl1ZcxZbsrgIXAGuAU22tLPs+LWdpPAz4I7AjcJ2me7ZOpCoz3SFoD/BKYVoqNjvnV+tQiIiLGCFXfiUNsIN1PNQXx7bb3k7QX8H9tv60bCW5s+vr6vGDBgg0Sa+KMazdInNFuyXlHj3QKERFjnqS7bPe1t9d5auBp20+XIJvZ/iHwig2dYERERHRfnSGGl0rahqq3/Y2SVgEPN5tWREREdEOdSYfeWhbPlnQzVc/+bzSaVURERHTFOo0saPvW0slvdqNZRURERFesz8iC46g3smBERERs5NZnZMHHGF0jC0ZERMQgmhxZMCIiIjZydW4NzJe09cAbSdtIekuDOUVERESX1CkEzrL9xMAb248DZzWXUkRERHRLnUKg0zZ1xh+IiIiIjVydQmCBpE9I2qO8PgHc1XRiERER0bw6hcB7gWeAy4E5wNPAKU0mFREREd1RZ2TBnwMzJG1RliMiImKMqDOy4OskLQR+UN7vK+nixjOLiIiIxtW5NXABcASwAsD2vcDBTSYVERER3VGnEMD2I21NaxvIJSIiIrqszmOAj0h6HWBJmwDvo9wmiIiIiNGtzhWBv6J6SmBn4L+A/chTAxEREWNCnacGfgqc0IVcIiIiosvqPDWwu6R/l7Rc0mOSrpG0ezeSi4iIiGbVuTXwb8AVwE7AS4GvAF9uMqmIiIjojjqdBV9k+7KW91+UdHpTCUWsq4kzrh3pFEbckvOOHukUImKUqlMIfEPSDKrhhQ28A5gnaTsA2ysbzC8iIiIaVKcQeHv5++629mlUhUH6C0RERIxSdZ4amNSNRCIiIqL76jw18GFJ41rebyXpX5tNKyIiIrqhzlMD44H5kvaR9CbgTuCuOsElHSlpkaT+0s+gff1mki4v6++QNLFl3czSvkjSEcPFlHRqabOkHVraT5B0n6T7JX1X0r4t65aU9nskLahzThEREWNJnVsDMyV9E7gDWAUcbLt/uP3KVYSLgDcBS4E7Jc21vbBls5OAVbb3lDQNOB94h6QpVH0Q9qZ6ZPGbkl5e9hks5neArwO3tKXyEPAG26skHQXMAl7dsv6QMmhSREREz6lza+Bg4ELgHKov2U9KemmN2AcB/bYX236G6qmDqW3bTAUuLctXAodJUmmfY3u17YeA/hJv0Ji277a9pD0J29+1vaq8vR3YpUbuERERPaHOUwP/BBw38Ete0tuAm4C9htlvZ6B11sKlPPeX+HO2sb1G0hPA9qX99rZ9dy7Lw8UcyknAN1reG7hBkoHP2J7VaSdJ04HpALvttts6HC4iImLjVqcQeK3tX087bPtqSbc2mFMjJB1CVQi8vqX59baXSfod4EZJP7T97fZ9S4EwC6Cvr89dSTgiIqIL6nQW3EPStyQ9ACBpH+A9NfZbBuza8n6X0tZxG0njga2BFUPsWyfm85ScPwdMtb1ioN32svL3MeCrVLceIiIiekadQuCzwEzgVwC276PqyDecO4HJkiZJ2rTsM7dtm7nAiWX5WOAm2y7t08pTBZOAycD8mjGfQ9JuwNXAn9n+UUv7FpK2HFgGDgceqHFeERERY0bduQbmV334fm3NcDuVe/6nAtcD44DZth+UdA6wwPZc4BLgMkn9wEpKgVG2uwJYWI51ysDtiU4xS/tpwAeBHYH7JM2zfTJwJlW/g4vLOayx3Qe8BPhqaRsP/Jvt62p8HhEREWNGnULgp5L2oOpYh6RjgUfrBLc9D5jX1nZmy/LTwHGD7HsucG6dmKX9QqqnG9rbTwZO7tC+GNi3vT0iIqKX1CkETqHqKLeXpGVUz+Wf0GhWERER0RV1BhRaDLyx3Ed/ge2nmk8rIiIiuqHOFQEAbP+8yUQiIiKi++o8NRARERFj1KCFgKTjyt9MQxwRETFGDXVFYGb5e1U3EomIiIjuG6qPwApJNwCTJD1v0B7bxzSXVkRERHTDUIXA0cABwGXAx7uTTkRERHTToIVAmeb3dkmvs71c0otL+8+6ll1EREQ0qs5TAy+RdDfwILBQ0l2SXtlwXhEREdEFdQqBWcD7bb/M9m7AB0pbREREjHJ1CoEtbN888Mb2LcAWjWUUERERXVNnZMHFkv4PVadBgD8FFjeXUkRERHRLnSsCfwFMAK6mGlNgh9IWERERo1ydSYdWAad1IZeIiIjossw1EBER0cNSCERERPSwFAIRERE9bNg+ApImAH8JTGzd3nY6DEZERIxydR4fvAa4DfgmsLbZdCIiIqKb6hQCL7J9RuOZRERERNfV6SPwdUl/1HgmERER0XV1CoH3URUDT0t6qryebDqxiIiIaF6dAYW27EYiERER0X11+ggg6Rjg4PL2Fttfby6liIiI6JZhbw1IOo/q9sDC8nqfpH9sOrGIiIhoXp0+An8EvMn2bNuzgSOBo+sEl3SkpEWS+iXN6LB+M0mXl/V3SJrYsm5maV8k6YjhYko6tbRZ0g4t7ZJ0YVl3n6QDWtadKOnH5XVinXOKiIgYS+qOLLhNy/LWdXaQNA64CDgKmAIcL2lK22YnAats7wlcAJxf9p0CTAP2pio8LpY0bpiY3wHeCDzcdoyjgMnlNR34VDnGdsBZwKuBg4CzJG1b59wiIiLGijqFwD8Cd0v6vKRLgbuAc2vsdxDQb3ux7WeAOcDUtm2mApeW5SuBwySptM+xvdr2Q0B/iTdoTNt3217SIY+pwBdcuR3YRtJOwBHAjbZXlhkWb6QqOiIiInpGnacGvizpFuDA0nSG7f+uEXtn4JGW90upfn133Mb2GklPANuX9tvb9t25LA8Xs04eOw/R/jySplNdTWC33XYb5nARERGjx6BXBCTtVf4eAOxE9UW5FHhp6332XmB7lu0+230TJkwY6XQiIiI2mKGuCLyf6lfwxzusM3DoMLGXAbu2vN+ltHXaZqmk8VT9D1YMs+9wMevmsQz4w7b2W4aJFRERMaYMekXA9vSyeJTtQ1pfVE8SDOdOYLKkSZI2per8N7dtm7nAQG/9Y4GbbLu0TytPFUyi6ug3v2bMdnOBd5anB14DPGH7UeB64HBJ25ZOgoeXtoiIiJ5Rp7Pgd2u2PYftNcCpVF+uPwCusP2gpHPKAEUAlwDbS+qnugIxo+z7IHAF1bgF1wGn2F47WEwASadJWkr1y/4+SZ8rx5gHLKbqcPhZ4K/LMVYCH6YqLu4EziltERERPWPQWwOSdqTqPPdCSfsDKqu2Al5UJ7jteVRfxK1tZ7YsPw0cN8i+59Lh6YROMUv7hcCFHdoNnDLIMWYDs4c8iYiIiDFsqD4CRwDvovqF/YmW9qeAv28wp4iIiOiSQQsB25cCl0r6E9tXdTGniIiI6JI64whcJeloqlH+Nm9pP6fJxCIiIqJ5dSYd+jTwDuC9VP0EjgNe1nBeERER0QV1nhp4ne13Us0J8CHgtcDLm00rIiIiuqFOIfDL8vcXkl4K/IpqpMGIiIgY5YbtIwB8XdI2wMeA71ONKvi5oXeJiIiI0aBOZ8EPl8WrJH0d2Nz2E82mFREREd0w1IBCh9q+SdLbOqzD9tXNphYRERFNG+qKwBuAm4A/7rDOQAqBiIiIUW6oAYXOKosn217bpXwiIiKii+o8NfCQpFmSDpOk4TePiIiI0aLOUwN7AW+mmrjnktJhcI7t/2g0s4joqokzrh3pFEbckvOOHukUIrpu2CsCtn9h+wrbbwP2p5p98NbGM4uIiIjG1bk1gKQ3SLoYuItqvoG3N5pVREREdMWwtwYkLQHuBq4ATrf986aTioiIiO6o00dgH9tPNp5JREREdF2dWwM7SvqWpAcAJO0j6X83nFdERER0QZ1C4LPATKrJhrB9HzCtyaQiIiKiO+oUAi+yPb+tbU0TyURERER31SkEfippD6phhZF0LPBoo1lFREREV9TpLHgKMAvYS9Iy4CHghEazioiIiK6oMw3xYuCNkrYAXmD7qebTioiIiG4YshCQ9ApgOtUwwwA/kDTL9o8azywiIiIaN2gfAUmvBW4BnqK6NfBZ4OfALZJe05XsIiIiolFDdRY8Ezje9tm2r7H9tTI18fHAWUPs92uSjpS0SFK/pBkd1m8m6fKy/g5JE1vWzSztiyQdMVxMSZNKjP4Sc9PSfoGke8rrR5Ieb9lnbcu6uXXOKSIiYiwZqhDYw/Yt7Y22bwV2Hy6wpHHARcBRwBTgeElT2jY7CVhle0/gAuD8su8UqrEK9gaOBC6WNG6YmOcDF5RYq0psbP+t7f1s7wd8Eri65fi/HFhn+5jhzikiImKsGaoQGKpTYJ35Bg4C+m0vtv0MMAeY2rbNVODSsnwlcJgklfY5tlfbfgjoL/E6xiz7HFpiUGK+pUNOxwNfrpF7RERETxiqs+Cuki7s0C5g5xqxdwYeaXm/FHj1YNvYXiPpCWD70n57274Dx+wUc3vgcdtrOmxfJS29DJgE3NTSvLmkBVQDJJ1n+2s1zisiImLMGKoQOH2IdQs2dCJdMA240vbalraX2V4maXfgJkn32/7P9h0lTad6eoLddtutO9lGRER0waCFgO1LB1tX0zJg15b3u5S2TtsslTQe2BpYMcy+ndpXANtIGl+uCnQ61jSqwZF+zfay8nexpFuA/YHnFQK2Z1E9OUFfX58HPeOIiIhRps4Qw+vrTmBy6c2/KdUXcXvP/LnAiWX5WOAm2y7t08pTBZOAycD8wWKWfW4uMSgxrxk4iKS9gG2B77W0bStps7K8A/D7wMINdvYRERGjQJ0hhtdLued/KnA9MA6YbftBSecAC2zPBS4BLpPUD6ykzGpYtruC6ot5DXDKwCX9TjHLIc8A5kj6CHB3iT1gGlXnw9Zf878LfEbSs1QF0Xm2UwhERERPaawQALA9D5jX1nZmy/LTwHGD7HsucG6dmKV9MdVTBZ1ind2h7bvAq4Y8gYiIiDFu0EJA0icpMw52Yvu0RjKKiIiIrhmqj8AC4C5gc+AA4MfltR+wafOpRURERNOGfWpA0nuA1w88oy/p08Bt3UkvIiIimlTnqYFtga1a3r+4tEVERMQoV6ez4HnA3ZJuphpV8GDg7CaTioiIiO4YshCQ9AJgEdUwvgPDA59h+7+bTiwiIiKaN2QhYPtZSRfZ3p+WAXoiIqKziTOuHekURtyS844e6RRiHdTpI/AtSX9SZviLiIiIMaROIfBu4CvAaklPSnpK0pMN5xURERFdMGxnQdtbdiORiIiI6L5aQwxL2pZq4p/NB9psf7uppCIiIqI7hi0EJJ0MvI9qat97gNdQzeJ3aLOpRURERNPq9BF4H3Ag8LDtQ4D9gccbzSoiIiK6ok4h8HSZJRBJm9n+IfCKZtOKiIiIbqjTR2CppG2ArwE3SloFPNxsWhEREdENdZ4aeGtZPLsMM7w1cF2jWUVERERXDFoISNquQ/P95e+LgZWNZBQRERFdM9QVgbsAU000tBuwqixvA/wEmNR4dhEREdGoQTsL2p5ke3fgm8Af297B9vbAm4EbupVgRERENKfOUwOvsT1v4I3tbwCvay6liIiI6JY6Tw38l6T/DXyxvD8B+K/mUoqIiIhuqXNF4HhgAvDV8vqd0hYRERGjXJ3HB1dSjS4YERERY0yduQZeDvwdMLF1e9uZayAiImKUq9NH4CvAp4HPAWubTSciIiK6qU4hsMb2pxrPJCIiIrquTmfBf5f015J2krTdwKtOcElHSlokqV/SjA7rN5N0eVl/h6SJLetmlvZFko4YLqakSSVGf4m5aWl/l6Tlku4pr5Nb9jlR0o/L68Q65xQRETGW1LkiMPAFeXpLm4Hdh9pJ0jjgIuBNwFLgTklzbS9s2ewkYJXtPSVNA84H3iFpCjAN2Bt4KfDN0leBIWKeD1xge46kT5fYA1cyLrd9alt+2wFnAX3lfO4qsVbV+EwiIiLGhGGvCJQRBttfQxYBxUFAv+3Ftp8B5gBT27aZClxalq8EDpOk0j7H9mrbDwH9JV7HmGWfQ0sMSsy3DJPfEcCNtleWL/8bgSNrnFdERMSYUeeKAJJeCUwBNh9os/2FYXbbGXik5f1S4NWDbWN7jaQngO1L++1t++5cljvF3B543PaaDtsD/Imkg4EfAX9r+5FB8mvdJyIiYswb9oqApLOAT5bXIcBHgWMazmtD+ndgou19qH71XzrM9s8jabqkBZIWLF++fIMnGBERMVLqdBY8FjgM+G/bfw7sC2xdY79lwK4t73cpbR23kTS+xF0xxL6Dta8AtikxnnMs2ytsry7tnwN+bx3yo8SYZbvPdt+ECROGOOWIiIjRpU4h8EvbzwJrJG0FPMZzv0AHcycwufTm35Sq89/ctm3m8pvOiMcCN9l2aZ9WniqYBEwG5g8Ws+xzc4lBiXkNgKSdWo53DPCDsnw9cLikbSVtCxxe2iIiInpGnT4CCyRtA3wWuAv4GfC94XYq9/xPpfpyHQfMtv2gpHOABbbnApcAl0nqB1ZSfbFTtrsCWAisAU6xvRagU8xyyDOAOZI+AtxdYgOcJumYEmcl8K5yjJWSPkxVXACcU4ZTjoiI6Bl15hr467L4aUnXAVvZvq9O8DJ98by2tjNblp8Gjhtk33OBc+vELO2LqZ4qaG+fCcwc5BizgdlDnkRERMQYVqez4LcGlm0vsX1fa1tERESMXoNeEZC0OfAiYIdyD11l1VbkMbuIiGjIxBnXjnQKG4Ul5x3dleMMdWvg3cDfUI3sdxe/KQSeBP6l4bwiIiKiCwYtBGz/M/DPkt5r+5NdzCkiIiK6ZNA+ApIOlLTjQBEg6Z2SrpF0Yd1JhyIiImLjNlRnwc8AzwCU4XnPA74APAHMaj61iIiIaNpQfQTGtTxX/w5glu2rgKsk3dN8ahEREdG0oa4IjGsZsvcw4KaWdbUmK4qIiIiN21Bf6F8GbpX0U+CXwG0Akvakuj0QERERo9xQTw2cWwYO2gm4oYznD9VVhPd2I7mIiIho1pCX+G3f3qHtR82lExEREd1UZ/bBiIiIGKNSCERERPSwFAIRERE9LIVARERED0shEBER0cNSCERERPSwFAIRERE9LIVARERED0shEBER0cNSCERERPSwFAIRERE9LIVARERED0shEBER0cNSCERERPSwFAIRERE9rNFCQNKRkhZJ6pc0o8P6zSRdXtbfIWliy7qZpX2RpCOGiylpUonRX2JuWtrfL2mhpPskfUvSy1r2WSvpnvKa29TnEBERsbFqrBCQNA64CDgKmAIcL2lK22YnAats7wlcAJxf9p0CTAP2Bo4ELpY0bpiY5wMXlFirSmyAu4E+2/sAVwIfbTn+L23vV17HbMDTj4iIGBWavCJwENBve7HtZ4A5wNS2baYCl5blK4HDJKm0z7G92vZDQH+J1zFm2efQEoMS8y0Atm+2/YvSfjuwSwPnGhERMSo1WQjsDDzS8n5paeu4je01wBPA9kPsO1j79sDjJcZgx4LqKsE3Wt5vLmmBpNslvaX+qUVERIwN40c6gW6R9KdAH/CGluaX2V4maXfgJkn32/7PDvtOB6YD7Lbbbl3JNyIiohuavCKwDNi15f0upa3jNpLGA1sDK4bYd7D2FcA2JcbzjiXpjcA/AMfYXj3QbntZ+bsYuAXYv9OJ2J5lu89234QJE4Y774iIiFGjyULgTmBy6c2/KVXnv/ae+XOBE8vyscBNtvL6z+UAAAzZSURBVF3ap5WnCiYBk4H5g8Us+9xcYlBiXgMgaX/gM1RFwGMDB5a0raTNyvIOwO8DCzfoJxAREbGRa+zWgO01kk4FrgfGAbNtPyjpHGCB7bnAJcBlkvqBlVRf7JTtrqD6Yl4DnGJ7LUCnmOWQZwBzJH2E6kmBS0r7x4AXA1+p+hTyk/KEwO8Cn5H0LFVBdJ7tFAIREdFTGu0jYHseMK+t7cyW5aeB4wbZ91zg3DoxS/tiqqcK2tvfOEj87wKvGvoMIiIixraMLBgREdHDUghERET0sBQCERERPSyFQERERA9LIRAREdHDUghERET0sBQCERERPSyFQERERA9LIRAREdHDUghERET0sBQCERERPSyFQERERA9LIRAREdHDUghERET0sBQCERERPSyFQERERA9LIRAREdHDUghERET0sBQCERERPSyFQERERA9LIRAREdHDUghERET0sBQCERERPSyFQERERA9LIRAREdHDUghERET0sEYLAUlHSlokqV/SjA7rN5N0eVl/h6SJLetmlvZFko4YLqakSSVGf4m56foeIyIiolc0VghIGgdcBBwFTAGOlzSlbbOTgFW29wQuAM4v+04BpgF7A0cCF0saN0zM84ELSqxVJfY6H2PDfgoREREbtyavCBwE9NtebPsZYA4wtW2bqcClZflK4DBJKu1zbK+2/RDQX+J1jFn2ObTEoMR8y3oeIyIiomc0WQjsDDzS8n5paeu4je01wBPA9kPsO1j79sDjJUb7sdb1GBERET1j/EgnMBpImg5ML29/JmnRSOazge0A/HQkE9D5I3n0DWZEP8cx8hlCPscNIf+mN4yx+Dm+rFNjk4XAMmDXlve7lLZO2yyVNB7YGlgxzL6d2lcA20gaX371t26/Psd4DtuzgFnDnO+oJGmB7b6RzmO0y+e4YeRz/O3lM9wweulzbPLWwJ3A5NKbf1Oqjnlz27aZC5xYlo8FbrLt0j6t9PifBEwG5g8Ws+xzc4lBiXnNeh4jIiKiZzR2RcD2GkmnAtcD44DZth+UdA6wwPZc4BLgMkn9wEqqL3bKdlcAC4E1wCm21wJ0ilkOeQYwR9JHgLtLbNbnGBEREb1C1Y/j6FWSppdbH/FbyOe4YeRz/O3lM9wweulzTCEQERHRwzLEcERERA9LIdBDJM2W9JikB9ra3yvph5IelPTRkcpvNJC0uaT5ku4tn9eHSvuXylDVD5TPeZORznVjJ2kbSVeW/+/9QNJrW9Z9QJIl7TCSOW6MOv07lvSx8jneJ+mrkrYp7ZtIulTS/eUznjlymW88JO0q6WZJC8u/4/eV9rMlLZN0T3n9Ucs++0j6Xtn+fkmbj9wZbFgpBHrL56mGU/41SYdQjbK4r+29gX8agbxGk9XAobb3BfYDjpT0GuBLwF7Aq4AXAiePXIqjxj8D19neC9gX+AFU/5EGDgd+MoK5bcw+T9u/Y+BG4JW29wF+BAx84R8HbGb7VcDvAe9unW+lh60BPmB7CvAa4JSW4eovsL1fec0DKI+efxH4q/LfyT8EfjUCeTcihUAPsf1tqicnWr0HOM/26rLNY11PbBRx5Wfl7SblZdvzyjpTPYa6y4glOQpI2ho4mPJ0j+1nbD9eVl8AfBBIB6YOOv07tn1Dy8iqt/Ob//8Z2KJ8kb0QeAZ4slu5bqxsP2r7+2X5KaoidKiRZQ8H7rN9b9lnxVh6yiyFQLwc+IMyM+Otkg4c6YQ2dmUCrHuAx4Abbd/Rsm4T4M+A60Yqv1FiErAc+FdJd0v6nKQtJE0Flg38BzfWy18A3yjLVwI/Bx6lusLyT7bbfwz0tHKFZH9g4N/xqeUWy2xJ25a2lwOWdL2k70v64Aik2pgUAjEe2I7q8tjpwBVlUqYYhO21tvej+tV1kKRXtqy+GPi27dtGJrtRYzxwAPAp2/tTfVmdDfw9cOYI5jWqSfoHqsveXypNBwFrgZdSFV8fkLT7CKW30ZH0YuAq4G9sPwl8CtiD6rbfo8DHy6bjgdcDJ5S/b5V0WPczbkYKgVgKXF2uas8HnqUaYzuGUS5l30y5XyvpLGAC8P6RzGuUWAosbbmaciVVYTAJuFfSEqpC6/uSdhyZFEcXSe8C3gyc4N88F/6/qPph/Krc9vsO0BPD5g6nXL27CviS7asBbP9PKfSfBT7Lb2akXUpV4P/U9i+AeVT/fx0TUgjE14BDACS9HNiUEZ5oY2MmaUJLj+wXAm8CfijpZOAI4PjyH5EYgu3/Bh6R9IrSdBjwfdu/Y3ui7YlU//E9oGwbQ5B0JFW/imPKF9WAn1BN0Y6kLaiu/P2w+xluXMpVz0uAH9j+REv7Ti2bvRUYeDLjeuBVkl5U+lu8gWpU2jEhsw/2EElfpurtuoOkpcBZwGxgdnkU6RngxJZfE/F8OwGXShpHVUhfYfvrktYADwPfK3dWrrZ9zgjmORq8F/hSmTdkMfDnI5zPqDDIv+OZwGbAjeX/f7fb/ivgIqp+GA8CAv7V9n0jkvjG5fep+vLcX/r7QHVb6nhJ+1F1slwCvBvA9ipJn6Ca78bAPNvXdj3rhmRkwYiIiB6WWwMRERE9LIVARERED0shEBER0cNSCERERPSwFAIRERE9LIVARBeVGfU+3vL+7ySdvYFif17SsRsi1jDHOa7MZHdzW/vEcn4faWnbQdKvJP3Leh5roqT/9dvmXOM471rfHMv+SzJTYoxWKQQiums18LaN7UujDJJS10nAX9o+pMO6h4CjW94fBzz4W6Q2kWp0vMas47mvS1xJyn9jY6OX/5NGdNcaYBbwt+0r2n/RS/pZ+fuHZUKoayQtlnSepBMkzS/zou/REuaNkhZI+pGkN5f9x6mar/7OMpnKu1vi3iZpLh1GSZN0fIn/gKTzS9uZVGOtXyLpYx3O7xfADyQNDGP7DuCKlpgTJd1U8viWpN1azv1CSd8t5zjwOZxHNSnWPZL+drBzKTHOKPneK+m80vaXZdt7JV0l6UUtx/u0pDuAj7add8cc27bZXtINquam/xzVYD0D+y6S9AWqUel2lfSp8r/Jg5I+VLY7UNLVZXmqpF9K2lTS5pIWl/bTJC0seczp8FlHbBAZWTCi+y4C7pP00WG3/I19gd+lmn52MfA52wdJeh/VCH1/U7abSDU++h7AzZL2BN4JPGH7QEmbAd+RdEPZ/gCqeewfaj2YpJcC51PNYb8KuEHSW2yfI+lQ4O9sLxgk1znANEn/QzXhzX9RTXoD8EngUtuXSvoL4ELgLWXdTlRFxl7AXKr5B2aUYw0UNdMHOZe9gKnAq23/QtJ2JebVtj9b9v0I1dWMT5Z1uwCvs71W1Tj9A4bKccBZwH+Uz+PoEnfAZKoROm8vx/0H2ytVjUb5LUn7AHdTTWwD8AdURcOBVP9NHph/YQYwyfZqlWGtI5qQKwIRXVZmOfsCcNo67HZnmUN9NfCfwMAX+f1UX/4DrrD9rO0fUxUMe1HNpf5OVUOp3gFsT/VlBTC/vQgoDgRusb28zHP/JeDgmrleRzUHwzTg8rZ1rwX+rSxfRvXFP+BrJfeFwEsGiT3YubyRavjcXwC0TLX7ynLV436qmeP2bon1lUHmlB8qxwEHA18sx7qWqlga8PBAEVC8XdL3qb789wamlM/0PyX9LlXh9okS8w+AgZkr76MagvlPqa4kRTQihUDEyPh/VL8it2hpW0P5N1nuLW/asm51y/KzLe+f5blX9trHDDfVZev32t6vvCbZHigkfv5bnUUHtp8B7gI+QPWrvq7WcxxsKuyhzqWTzwOn2n4V8CFg85Z1G/zc2+NKmgT8HXCY7X2Aa1ty+DZwFPAr4JtUBcfr+U0hcDTV1aMDgDub6ssQkUIgYgSUX6xX8NxLykuoLsUDHANssh6hj5P0gtJvYHdgEdXMae9RNe0qkl6uaia6ocwH3qCq1/844Hjg1nXI4+PAGS2/zAd8l+pKAVS/0G9jaE8BW7a8H+xcbgT+vKUPwMCtgS2BR8v2J9TMvU6O36Z0YpR0FLDtILG2oioMnpD0Eqov/gG3Ud3S+Z7t5VRXN14BPFAKwV1t3wycAWwNvLhm/hHrJBVmxMj5OHBqy/vPAtdIupfq8vr6/GL9CdWX+FbAX9l+unRmmwh8X5KA5Tz/nvdz2H5U0gzgZqpf4dfavqZuErYfpPPTAu+lmg3v9JLHcDMO3gesLZ/J54F/7nQutq9TNWvcAknPUM0X//fA/6G6hbC8/N3yeUdYvxw/BHxZ1ax+36X63J/H9r2S7qaa+vcR4Dstq++gugXy7ZZz3dG2y6//L0ramurzv9D24zVyj1hnmX0wIiKih+XWQERERA9LIRAREdHDUghERET0sBQCERERPSyFQERERA9LIRAREdHDUghERET0sBQCERERPez/A92Wyr8muZuxAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "WNY20VuEIGep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now perform three analysis. First, we analyse business cycle frequencies and amplifications."
      ],
      "metadata": {
        "id": "X5eMjoQjINtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Calculate fourier transform \"\"\"\n",
        "\n",
        "# simulate 100000 periods\n",
        "rng_test = random.PRNGKey(0)\n",
        "env = Model()\n",
        "n_periods = 1000000\n",
        "state_init = jnp.array(jnp.log(jax.random.uniform(rng_test,(2*env.n_sectors,),minval=0.9,maxval=1.1)))\n",
        "shocks = jax.random.multivariate_normal(rng_test, jnp.zeros((env.n_sectors,)), env.Sigma_A, shape=(n_periods,))\n",
        "_, state_policy_pairs = lax.scan(env.step,state_init,shocks)\n",
        "states, _ = state_policy_pairs\n",
        "print(states.shape) #check that last dimension is the time dimension\n",
        "fourier = jax.scipy.fft.dct(states)\n",
        "del states\n",
        "del shocks\n"
      ],
      "metadata": {
        "id": "rVIE1CZfIXOi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "b6b4cff7-3f6b-4cdf-de6b-c2a215a0e702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-604d68dcc43d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mshocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_sectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigma_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_periods\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_policy_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_policy_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow/loops.py\u001b[0m in \u001b[0;36mscan\u001b[0;34m(f, init, xs, length, reverse, unroll)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;31m# necessary, a second time with modified init values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m   \u001b[0minit_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarry_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarry_avals_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m   \u001b[0mnew_init_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchanged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_promote_weak_typed_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarry_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarry_avals_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow/loops.py\u001b[0m in \u001b[0;36m_create_jaxpr\u001b[0;34m(init)\u001b[0m\n\u001b[1;32m    245\u001b[0m     jaxpr, consts, out_tree = _initial_style_jaxpr(\n\u001b[0;32m--> 246\u001b[0;31m         f, in_tree, (*carry_avals, *x_avals), \"scan\")\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mout_tree_children\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow/common.py\u001b[0m in \u001b[0;36m_initial_style_jaxpr\u001b[0;34m(fun, in_tree, in_avals, primitive_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m   jaxpr, consts, out_tree = _initial_style_open_jaxpr(\n\u001b[0;32m---> 61\u001b[0;31m       fun, in_tree, in_avals, primitive_name)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0mclosed_jaxpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClosedJaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_constvars_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow/common.py\u001b[0m in \u001b[0;36m_initial_style_open_jaxpr\u001b[0;34m(fun, in_tree, in_avals, primitive_name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mdebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"<unknown>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_dynamic\u001b[0;34m(fun, in_avals, debug_info, keep_inputs)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n\u001b[0;32m-> 1982\u001b[0;31m       fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)\n\u001b[0m\u001b[1;32m   1983\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals, keep_inputs, debug_info)\u001b[0m\n\u001b[1;32m   1997\u001b[0m     \u001b[0min_tracers_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: TypeError: step() missing 1 required positional argument: 'shock'\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-604d68dcc43d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mstate_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_sectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mshocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_sectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigma_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_periods\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_policy_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_policy_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#check that last dimension is the time dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'shock'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis 2: Impulse of responses of larges sectors"
      ],
      "metadata": {
        "id": "ZmnpWXEr-1sT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impulse responses of dynare policy vs learned policy\n",
        "env = Model()\n",
        "obs_ss = jnp.zeros_like(env.states_ss)\n",
        "\n",
        "shokcs_dir = {\n",
        "    f\"e_ir_{i}\": jnp.concatenate([jnp.array([env.states_sd[2*env.n_sectors+i]]),jnp.zeros((env.n_sectors-1,))]) \n",
        "    for i in range(env.n_sectors)}\n",
        "state_init = jnp.zeros(shape = 3*env.n_sectors)\n",
        "\n",
        "shocks = jnp.zeros((40, env.n_sectors))\n",
        "state_final, train_pairs = lax.scan(env.step, state_init, shocks)\n",
        "obs, policy = train_pairs\n",
        "alt_policy = MLP_softplus.apply(params,obs)\n",
        "plt.plot(list(range(40)) ,policy[:,0], label =\"dynare policy\")\n",
        "plt.plot(list(range(40)) ,alt_policy[:,0], label =\"neural net policy\")\n",
        "plt.plot(list(range(40)), [jnp.exp(env.ss_policy[0]) for i in range(config[\"length_IR\"])])\n",
        "plt.legend()\n",
        "plt.savefig(config['working_dir']+config['run_name']+'/IR.jpg')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "mmBJ9LxS-54A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "dac9780e-d978-481d-cc2d-bff5d601c04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6db0eef14441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Impulse responses of dynare policy vs learned policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRbcProdNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mobs_ss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates_ss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m shokcs_dir = {\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RbcProdNet' object has no attribute 'states_ss'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obsolete"
      ],
      "metadata": {
        "id": "AxcdZawR-2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def loss(self, rng, state, policy_params):\n",
        "    \n",
        "  #   # process state\n",
        "  #   state = state*self.states_sd\n",
        "  #   K = jnp.exp(state[:self.n_sectors]+self.states_ss[:self.n_sectors])\n",
        "  #   a = state[self.n_sectors:]+self.states_ss[self.n_sectors:]\n",
        "\n",
        "  #   # calculate the policy variables\n",
        "  #   policy = self.policy_fn(policy_params,state) \n",
        "  #   I = policy[:self.n_sectors]\n",
        "  #   Q = policy[self.n_sectors:2*self.n_sectors]\n",
        "  #   P = policy[2*self.n_sectors:3*self.n_sectors]\n",
        "  #   L = policy[3*self.n_sectors:]\n",
        "\n",
        "  #   #Rest of variables\n",
        "  #   A = jnp.exp(a)\n",
        "  #   Lagg = jnp.sum(L)  # Aggregate labor\n",
        "  #   Pagg = jnp.dot(jnp.transpose(self.xi),P**(1-self.sigma_c))**(1/(1-self.sigma_c)) # Agg price\n",
        "  #   Cagg = 1/Pagg  # Agg consumption\n",
        "  #   C = self.xi * (P/Pagg)**(-self.sigma_c) * Cagg  # Sectoral Consumption\n",
        "  #   Pm = (jnp.dot(jnp.transpose(self.Gamma_M),P**(1-self.sigma_m)))**(1/(1-self.sigma_m)) #  Interm. Price Index\n",
        "  #   M = self.mu * (Pm/P)**(-self.sigma_q) * Q  # Sectoral Intermediates\n",
        "  #   Mout = P**(-self.sigma_m) * jnp.dot(self.Gamma_M,Pm**(self.sigma_m)*M) # interm sold by sector\n",
        "  #   Pk = (jnp.dot(jnp.transpose(self.Gamma_I),P**(1-self.sigma_I)))**(1/(1-self.sigma_I)) #  Capital Price Index\n",
        "  #   Iout = P**(-self.sigma_I) * jnp.dot(self.Gamma_I,Pk**(self.sigma_I)*I) # New Capital sold by sector\n",
        "  #   Y = (A * (\n",
        "  #       self.alpha**(1/self.sigma_y)*K**((self.sigma_y-1)/self.sigma_y) + \n",
        "  #       (1-self.alpha)**(1/self.sigma_y)*L**((self.sigma_y-1)/self.sigma_y) \n",
        "  #       )**(self.sigma_y/(self.sigma_y-1)))\n",
        "\n",
        "  #   # Now we calculate the expectation term through montecarlo. We know K_next    \n",
        "  #   K_next = (1-self.delta)*K + I  # get next K\n",
        "    \n",
        "  #   # First, we get what is inside expectations for the realization of one shock\n",
        "  #   def exp_realization(shock):\n",
        "  #     # get next state\n",
        "  #     a_next = self.rho*a + shock\n",
        "  #     state_next = jnp.concatenate([jnp.log(K_next),a_next])-self.states_ss\n",
        "      \n",
        "  #     #calculate policies\n",
        "  #     policy_next = self.policy_fn(policy_params,state_next) \n",
        "  #     I_next = policy_next[:self.n_sectors]\n",
        "  #     Q_next = policy_next[self.n_sectors:2*self.n_sectors]\n",
        "  #     P_next = policy_next[2*self.n_sectors:3*self.n_sectors]\n",
        "  #     L_next = policy_next[3*self.n_sectors:]\n",
        "\n",
        "  #     # Solve for the rest of the endogenous variables\n",
        "  #     A_next = jnp.exp(a_next)\n",
        "  #     Lagg = jnp.sum(L)\n",
        "  #     Pk_next = (jnp.dot(jnp.transpose(self.Gamma_I),P_next**(1-self.sigma_I)))**(1/(1-self.sigma_I))\n",
        "  #     Y_next = (A_next * (\n",
        "  #       self.alpha**(1/self.sigma_y)*K_next**((self.sigma_y-1)/self.sigma_y) + \n",
        "  #       (1-self.alpha)**(1/self.sigma_y)*L_next**((self.sigma_y-1)/self.sigma_y) \n",
        "  #       )**(self.sigma_y/(self.sigma_y-1)))\n",
        "\n",
        "  #     # Solve for the expectation term\n",
        "  #     exp_realization = (P_next*A_next**((self.sigma_y-1)/self.sigma_y) *\n",
        "  #       ((1-self.mu)*Q_next/Y_next)**(1/self.sigma_q) *\n",
        "  #       (self.alpha*Y_next/K_next)**(1/self.sigma_y) +\n",
        "  #       (1-self.delta)*Pk_next)\n",
        "  #     return exp_realization\n",
        "    \n",
        "  #   # Now we calculate the expectation through montecarlo \n",
        "  #   def monte_carlo_exp(rng, mc_draws):\n",
        "  #     shocks = jax.random.multivariate_normal(rng, jnp.zeros((self.n_sectors,)), self.Sigma_A, shape=(mc_draws,))\n",
        "  #     exp = jnp.mean(jax.vmap(exp_realization)(shocks),axis=0)\n",
        "  #     return exp\n",
        "  #   # exp = monte_carlo_exp(rng, self.mc_draws)\n",
        "  #   # rng, *mc_rngs  = random.split(rng,self.n_mcs+1)\n",
        "  #   exp = monte_carlo_exp(rng,self.mc_draws)\n",
        "    \n",
        "  #   # Calculate model implied Q\n",
        "  #   Qmod = ((\n",
        "  #       (1-self.mu)**(1/self.sigma_q)*Y**((self.sigma_q-1)/self.sigma_q) + \n",
        "  #       (self.mu)**(1/self.sigma_q)*M**((self.sigma_q-1)/self.sigma_q) \n",
        "  #       )**(self.sigma_q/(self.sigma_q-1)))\n",
        "    \n",
        "  #   # Calculate right hand side of error eq for L\n",
        "  #   Lmod = (P*A**((self.sigma_y-1)/self.sigma_y) * \n",
        "  #     ((1-self.mu)*Q/Y)**(1/self.sigma_q) * \n",
        "  #     ((1-self.alpha)*Y/L)**(1/self.sigma_y))\n",
        "\n",
        "  #   Q_loss = Q / Qmod - 1\n",
        "  #   Pk_loss = Pk/(self.beta*exp) - 1\n",
        "  #   Market_loss = Q / (C+Mout+Iout) - 1\n",
        "  #   L_loss = Lagg**(1/self.eps_l)/Lmod - 1\n",
        "  #   loss = (jnp.sum(jnp.array([jnp.square(Q_loss),jnp.square(Pk_loss),jnp.square(Market_loss), jnp.square(L_loss)])))/self.n_actions\n",
        "\n",
        "  #   return lax.stop_gradient(loss)\n",
        "\n",
        "  # def expectation(self, rng, state, K_next, policy_params):\n",
        "  #   K = jnp.exp(state[:self.n_sectors]+self.states_ss[:self.n_sectors])\n",
        "  #   a = state[self.n_sectors:]\n",
        "\n",
        "  #   def exp_realization(shock):\n",
        "  #     # get next state\n",
        "  #     a_next = self.rho*a + shock\n",
        "  #     state_next = jnp.concatenate([jnp.log(K_next),a_next])-self.states_ss\n",
        "      \n",
        "  #     #calculate policies\n",
        "  #     policy_next = self.policy_fn(policy_params,state_next) \n",
        "  #     I_next = policy_next[:self.n_sectors]\n",
        "  #     Q_next = policy_next[self.n_sectors:2*self.n_sectors]\n",
        "  #     P_next = policy_next[2*self.n_sectors:3*self.n_sectors]\n",
        "  #     L_next = policy_next[3*self.n_sectors:]\n",
        "\n",
        "  #     # Solve for the rest of the endogenous variables\n",
        "  #     A_next = jnp.exp(a_next)\n",
        "  #     Lagg = jnp.sum(L)\n",
        "  #     Pk_next = (jnp.dot(jnp.transpose(self.Gamma_I),P_next**(1-self.sigma_I)))**(1/(1-self.sigma_I))\n",
        "  #     Y_next = (A_next * (\n",
        "  #       self.alpha**(1/self.sigma_y)*K_next**((self.sigma_y-1)/self.sigma_y) + \n",
        "  #       (1-self.alpha)**(1/self.sigma_y)*L_next**((self.sigma_y-1)/self.sigma_y) \n",
        "  #       )**(self.sigma_y/(self.sigma_y-1)))\n",
        "\n",
        "  #     # Solve for the expectation term\n",
        "  #     exp_realization = (P_next*A_next**((self.sigma_y-1)/self.sigma_y) *\n",
        "  #       ((1-self.mu)*Q_next/Y_next)**(1/self.sigma_q) *\n",
        "  #       (self.alpha*Y_next/K_next)**(1/self.sigma_y) +\n",
        "  #       (1-self.delta)*Pk_next)\n",
        "  #     return exp_realization\n",
        "\n",
        "  #   shocks = jax.random.multivariate_normal(rng, jnp.zeros((self.n_sectors,)), self.Sigma_A, shape=(self.mc_draws,))\n",
        "  #   exp = jnp.mean(jax.vmap(exp_realization)(shocks), axis=0)\n",
        "  #   return exp"
      ],
      "metadata": {
        "id": "lXkrrPTm-4UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment\n",
        "# class ProdNetRbc_SS():\n",
        "#   \"\"\"A JAX implementation of an RBC model with Production Networks.\"\"\"\n",
        "\n",
        "#   def __init__(self, params = params, states_ss=states_ss, policies_ss=policies_ss, states_sd = states_sd, policy_layers=[1024,1024]):\n",
        "\n",
        "#     self.alpha = params[\"alpha\"]\n",
        "#     self.beta = params[\"beta\"]\n",
        "#     self.delta = params[\"delta\"]\n",
        "#     self.rho = params[\"rho\"]\n",
        "#     self.eps_l = params[\"eps_l\"]\n",
        "#     self.sigma_c = params[\"sigma_c\"]\n",
        "#     self.sigma_m = params[\"sigma_m\"]\n",
        "#     self.sigma_q = params[\"sigma_q\"]\n",
        "#     self.sigma_y = params[\"sigma_y\"]\n",
        "#     self.sigma_I = params[\"sigma_I\"]\n",
        "#     self.xi = jnp.array(params[\"xi\"])\n",
        "#     self.mu = jnp.array(params[\"mu\"])\n",
        "#     self.Gamma_M = jnp.array(params[\"Gamma_M\"])\n",
        "#     self.Gamma_I = jnp.array(params[\"Gamma_I\"])\n",
        "#     self.n_sectors = params[\"n_sectors\"]\n",
        "#     self.states_ss = jnp.concatenate([states_ss,jnp.zeros(shape=(self.n_sectors,))])\n",
        "#     self.policies_ss = jnp.array(policies_ss)\n",
        "#     self.n_actions = 4*self.n_sectors\n",
        "#     self.states_sd = states_sd\n",
        "#   def initial_state(self):\n",
        "#     state_init = jnp.zeros(shape=(2*self.n_sectors,))\n",
        "#     return lax.stop_gradient(state_init)\n",
        "\n",
        "#   def step(self, state): \n",
        "#     state = state*self.states_sd\n",
        "#     K = jnp.exp(state[:self.n_sectors]+self.states_ss[:self.n_sectors])\n",
        "#     a = state[self.n_sectors:]\n",
        "#     policy = jnp.exp(self.policies_ss)\n",
        "#     K_next = (1-self.delta)*K + policy[:self.n_sectors]\n",
        "#     a_next = self.rho*a\n",
        "#     state_next = (jnp.concatenate([jnp.log(K_next),a_next])-self.states_ss)/self.states_sd\n",
        "#     # return lax.stop_gradient(state_next), lax.stop_gradient(state_next)\n",
        "#     return lax.stop_gradient(state_next)\n",
        "\n",
        "\n",
        "#   def loss(self, state):\n",
        "#     state = state*self.states_sd\n",
        "#     # process state\n",
        "#     K = jnp.exp(state[:self.n_sectors]+self.states_ss[:self.n_sectors])\n",
        "\n",
        "#     # calculate the policy variables\n",
        "#     policy = jnp.exp(self.policies_ss)\n",
        "#     I = policy[:self.n_sectors]\n",
        "#     Q = policy[self.n_sectors:2*self.n_sectors]\n",
        "#     P = policy[2*self.n_sectors:3*self.n_sectors]\n",
        "#     L = policy[3*self.n_sectors:]\n",
        "  \n",
        "#     #Rest of variables\n",
        "#     Lagg = jnp.sum(L)  # Aggregate labor\n",
        "#     Pagg = jnp.dot(jnp.transpose(self.xi),P**(1-self.sigma_c))**(1/(1-self.sigma_c)) # Agg price\n",
        "#     Cagg = 1/Pagg  # Agg consumption\n",
        "#     C = self.xi * (P/Pagg)**(-self.sigma_c) * Cagg  # Sectoral Consumption\n",
        "#     Pm = (jnp.dot(jnp.transpose(self.Gamma_M),P**(1-self.sigma_m)))**(1/(1-self.sigma_m)) #  Interm. Price Index\n",
        "#     M = self.mu * (Pm/P)**(-self.sigma_q) * Q  # Sectoral Intermediates\n",
        "#     Mout = P**(-self.sigma_m) * jnp.dot(self.Gamma_M,Pm**(self.sigma_m)*M) # interm sold by sector\n",
        "#     Pk = (jnp.dot(jnp.transpose(self.Gamma_I),P**(1-self.sigma_I)))**(1/(1-self.sigma_I)) #  Capital Price Index\n",
        "#     Iout = P**(-self.sigma_I) * jnp.dot(self.Gamma_I,Pk**(self.sigma_I)*I) # New Capital sold by sector\n",
        "#     Y = (\n",
        "#         (\n",
        "#         self.alpha**(1/self.sigma_y)*K**((self.sigma_y-1)/self.sigma_y) + \n",
        "#         (1-self.alpha)**(1/self.sigma_y)*L**((self.sigma_y-1)/self.sigma_y) \n",
        "#         )**(self.sigma_y/(self.sigma_y-1)))\n",
        "    \n",
        "#     print(Y)\n",
        "#     # Now we calculate the expectation term through montecarlo. We know K_next\n",
        "#     K_next = (1-self.delta)*K + I  # get next K\n",
        "#     exp = (\n",
        "#         P *\n",
        "#         ((1-self.mu)*Q/Y)**(1/self.sigma_q) *\n",
        "#         (self.alpha*Y/K_next)**(1/self.sigma_y) +\n",
        "#         (1-self.delta)*Pk\n",
        "#         )\n",
        "#     # Now we get what is inside expectations for each realization of the shock\n",
        "#     # def exp_realization():\n",
        "#     #   # a_next = self.rho*a       \n",
        "#     #   # state_next = jnp.concatenate([jnp.log(K_next),a_next])-self.states_ss\n",
        "      \n",
        "#     #   #calculate policies\n",
        "#     #   # policy_next = policy\n",
        "#     #   # I_next = policy_next[:self.n_sectors]\n",
        "#     #   # Q_next = policy_next[self.n_sectors:2*self.n_sectors]\n",
        "#     #   # P_next = policy_next[2*self.n_sectors:3*self.n_sectors]\n",
        "#     #   # L_next = policy_next[3*self.n_sectors:]\n",
        "\n",
        "#     #   # Solve for the rest of the endogenous variables\n",
        "#     #   # A_next = jnp.exp(a_next)\n",
        "#     #   # Lagg = jnp.sum(L)\n",
        "#     #   # Pk_next = (jnp.dot(jnp.transpose(Gamma_I),P_next**(1-self.sigma_I)))**(1/(1-self.sigma_I))\n",
        "#     #   # Y_next = (A_next * (\n",
        "#     #   #   self.alpha**(1/self.sigma_y)*K_next**((self.sigma_y-1)/self.sigma_y) + \n",
        "#     #   #   (1-self.alpha)**(1/self.sigma_y)*L_next**((self.sigma_y-1)/self.sigma_y) \n",
        "#     #   #   )**(self.sigma_y/(self.sigma_y-1)))\n",
        "\n",
        "#     #   # Solve for the expectation term\n",
        "#     #   # exp_realization = (P_next*A_next**((self.sigma_y-1)/self.sigma_y) *\n",
        "#     #   #   ((1-self.mu)*Q_next/Y_next)**(1/self.sigma_q) *\n",
        "#     #   #   (self.alpha*Y_next/K_next)**(1/self.sigma_y) +\n",
        "#     #   #   (1-self.delta)*Pk_next)\n",
        "#     #   exp_realization = (P*A**((self.sigma_y-1)/self.sigma_y) *\n",
        "#     #     ((1-self.mu)*Q/Y)**(1/self.sigma_q) *\n",
        "#     #     (self.alpha*Y/K_next)**(1/self.sigma_y) +\n",
        "#     #     (1-self.delta)*Pk)\n",
        "#     #   return exp_realization\n",
        "\n",
        "#     # Now we calculate the expectation\n",
        "\n",
        "#     # Calculate model implied Q\n",
        "#     Qmod = ((\n",
        "#         (1-self.mu)**(1/self.sigma_q)*Y**((self.sigma_q-1)/self.sigma_q) + \n",
        "#         (self.mu)**(1/self.sigma_q)*M**((self.sigma_q-1)/self.sigma_q) \n",
        "#         )**(self.sigma_q/(self.sigma_q-1)))\n",
        "\n",
        "#     # Calculate right hand side of error eq for L\n",
        "#     Lmod = (P * \n",
        "#       ((1-self.mu)*Q/Y)**(1/self.sigma_q) * \n",
        "#       ((1-self.alpha)*Y/L)**(1/self.sigma_y))\n",
        "\n",
        "#     Q_loss = Q / Qmod - 1\n",
        "#     Pk_loss = Pk/(self.beta*exp) - 1\n",
        "#     Market_loss = Q / (C+Mout+Iout) - 1\n",
        "#     L_loss = Lagg**(1/self.eps_l)/Lmod - 1\n",
        "#     loss = (jnp.sum(jnp.array([jnp.square(Q_loss),jnp.square(Pk_loss),jnp.square(Market_loss), jnp.square(L_loss)])))/self.n_actions\n",
        "\n",
        "#     return lax.stop_gradient(loss)"
      ],
      "metadata": {
        "id": "jrdME6CMJ0kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(env, config):\n",
        "  \"\"\"Runs experiment.\"\"\"\n",
        "  cores_count = len(jax.devices())  # get available TPU cores.\n",
        "  nn_policy = MLP_softplus(config[\"layers\"] + [env.n_actions])\n",
        "  optim = optax.adam(config[\"learning_rate\"])  # define optimiser.\n",
        "\n",
        "  rng, rng_e = random.split(random.PRNGKey(config[\"seed\"]), 2)  # prng keys.\n",
        "  dummy_obs = env.initial_obs(rng_e)  # dummy for net init.\n",
        "  params = params # initialise params.\n",
        "  nn_forward = nn_policy.apply\n",
        "  mean_loss = jnp.array([0.0]) # initialize loss\n",
        "  # mean_abs_loss = jnp.array([0.0]) # initialize loss\n",
        "  opt_state = optim.init(params)  # initialise optimiser stats.\n",
        "  learn = jax.jit(get_epoque_learner_fn(env, nn_forward, optim.update, config[\"batch_size\"], config[\"epoque_iters\"]))\n",
        "  # learn = jax.pmap(learn, axis_name='i')  # replicate over multiple cores.\n",
        "\n",
        "  #  broadcast = lambda x: jnp.broadcast_to(x, (cores_count, config[\"n_batches\"]) + x.shape)\n",
        "  # params = jax.tree_map(broadcast, params)  # broadcast to cores and batch.\n",
        "  # opt_state = jax.tree_map(broadcast, opt_state)  # broadcast to cores and batch\n",
        "  # mean_loss = jax.tree_map(broadcast, mean_loss)\n",
        "  # mean_abs_loss = jax.tree_map(broadcast, mean_abs_loss)\n",
        "\n",
        "  # rng, *env_rngs = jax.random.split(rng, cores_count * config[\"n_batches\"]+ 1)\n",
        "  # env_obs = jax.vmap(env.initial_obs)(jnp.stack(env_rngs))  # init envs.\n",
        "  # rng, *step_rngs = jax.random.split(rng, cores_count * config[\"n_batches\"] + 1)\n",
        "  # rng, *eval_rngs = jax.random.split(rng, cores_count * config[\"n_batches\"] + 1)\n",
        "  rng, env_rngs = jax.random.split(rng, cores_count * config[\"n_batches\"]+ 1)\n",
        "  env_obs = env.initial_obs(env_rngs)  # init envs.\n",
        "  rng, step_rngs = jax.random.split(rng, cores_count * config[\"n_batches\"] + 1)\n",
        "  rng, eval_rngs = jax.random.split(rng, cores_count * config[\"n_batches\"] + 1)\n",
        "\n",
        "  # reshape = lambda x: x.reshape((cores_count, config[\"n_batches\"]) + x.shape[1:])\n",
        "  # step_rngs = reshape(jnp.stack(step_rngs))  # add dimension to pmap over.\n",
        "  # eval_rngs = reshape(jnp.stack(eval_rngs))  # add dimension to pmap over.\n",
        "  # env_obs = reshape(env_obs)  # add dimension to pmap over.\n",
        "\n",
        "  mean_losses = []\n",
        "  mean_accuracy = []\n",
        "  num_steps = cores_count * config[\"epoque_iters\"] * config[\"batch_size\"] * config[\"n_batches\"]\n",
        "\n",
        "  with TimeIt(tag='COMPILATION'):\n",
        "    learn(params, opt_state, step_rngs, env_obs, mean_loss)  # compiles\n",
        "\n",
        "  #First run, we calculate periods per second\n",
        "  with TimeIt(tag='EXECUTION', steps=num_steps):\n",
        "    params, opt_state, step_rngs, env_obs, mean_loss = learn(\n",
        "        params, opt_state, step_rngs, env_obs, mean_loss)\n",
        "  \n",
        "  #Rest of the runs\n",
        "  for i in range(2,config[\"n_epoques\"]+1):\n",
        "    rng, step_rngs = jax.random.split(rng, cores_count * config[\"n_batches\"] + 1)\n",
        "    # step_rngs = reshape(jnp.stack(step_rngs))\n",
        "    params, opt_state, step_rngs, env_obs, mean_loss = learn( \n",
        "        params, opt_state, step_rngs, env_obs, mean_loss) \n",
        "    \n",
        "    mean_losses.append(jnp.mean(mean_loss)) \n",
        "    # mean_accuracy.append((1- jnp.mean(mean_abs_loss))*100)\n",
        "      \n",
        "    print('Iteration:', i*config[\"epoque_iters\"], \n",
        "          \", Mean_loss:\", jnp.mean(mean_loss),\n",
        "          \", Learning rate:\", config[\"learning_rate\"](i*config[\"epoque_iters\"]), \n",
        "          # \", Mean accuracy (%):\", (1- jnp.mean(mean_abs_loss))*100\n",
        "          )\n",
        "    \n",
        "    if i%config[\"reset_env_nepoques\"]==0:\n",
        "      env_obs = jnp.zeros_like(env_obs)\n",
        "      print(\"ENV RESET\")\n",
        "\n",
        "  # Print best result\n",
        "  # print(\"Maximum accuracy attained in training:\", max(mean_accuracy))\n",
        "  \n",
        "  #Checkpoint\n",
        "  checkpoints.save_checkpoint(ckpt_dir=config['working_dir']+config['run_name'], target=params, step=config[\"n_epoques\"]*config[\"epoque_iters\"])\n",
        "\n",
        "  # Plots\n",
        "  plt.plot([i for i in range(len(mean_losses[100:]))], mean_losses[100:])\n",
        "  plt.xlabel('Steps')\n",
        "  plt.ylabel('Mean Losses')\n",
        "  plt.savefig(config['working_dir']+config['run_name']+'/mean_losses.jpg')\n",
        "  plt.close()\n",
        "\n",
        " \n",
        "  return params, optim, nn_policy, mean_losses, mean_accuracy"
      ],
      "metadata": {
        "id": "gOC1kNvM92Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_epoque_learner_fn(\n",
        "    env, nn_forward, opt_update, batch_size, epoque_iters):\n",
        "  \"\"\"It runs and epoque with learing. This is what the compiler reads and parallelize (the minimal unit of computation).\"\"\"\n",
        "\n",
        "  def period_loss_fn(nn_params, loss_rng, env_obs):\n",
        "    \n",
        "    # get rngs\n",
        "    loss_rng, mc_rng = random.split(loss_rng,2)\n",
        "\n",
        "    # Step the environemnt\n",
        "    period_shock = jax.random.multivariate_normal(loss_rng, jnp.zeros((env.n_sectors,)), env.Sigma_A/100)\n",
        "    policy = nn_forward(nn_params, env_obs)\n",
        "    obs_final = env.step(env_obs, policy, period_shock)  # apply period steps for each row shock in shocks.\n",
        "    \n",
        "    # calculate exp\n",
        "    mc_shocks = jax.random.multivariate_normal(mc_rng, jnp.zeros((env.n_sectors,)), env.Sigma_A, shape=(200000,))\n",
        "    mc_nextobs = jax.vmap(env.step, in_axes=(None, None, 0))(env_obs, policy, mc_shocks)\n",
        "    del mc_shocks\n",
        "    mc_nextpols = nn_forward(nn_params,jnp.stack(mc_nextobs))\n",
        "    exp = jnp.mean(jax.vmap(env.exp_realization)(mc_nextobs, mc_nextpols))\n",
        "\n",
        "    loss = env.loss(env_obs, exp, policy)\n",
        "    \n",
        "    return loss, (obs_final, jnp.array([loss]))\n",
        "\n",
        "  def update_fn(nn_params, opt_state, rng, env_obs, mean_loss):\n",
        "    \"\"\"Compute a gradient update from a single trajectory.\"\"\"\n",
        "    rng, loss_rng = random.split(rng,2)\n",
        "    grads, aux_info  = jax.grad(  # compute gradient on a single trajectory.\n",
        "        period_loss_fn, has_aux=True)(nn_params, loss_rng, env_obs)\n",
        "    new_env_obs, mean_loss = aux_info\n",
        "    # grads = lax.pmean(grads, axis_name='j')  # reduce mean (average grads) across cores.\n",
        "    # grads = lax.pmean(grads, axis_name='i')  # reduce mean (average grads) across batch.\n",
        "    updates, new_opt_state = opt_update(grads, opt_state)  # transform grads.\n",
        "    new_params = optax.apply_updates(nn_params, updates)  # update parameters.\n",
        "    return new_params, new_opt_state, rng, new_env_obs, mean_loss\n",
        "\n",
        "  # def learner_fn(params, opt_state, rngs, env_obs, mean_loss):\n",
        "  #   \"\"\"Vectorise and repeat the update.\"\"\"\n",
        "  #   batched_update_fn = jax.vmap(update_fn, axis_name='j')  # vectorize across batch.\n",
        "  #   def iterate_fn(_, val):  # repeat many times to avoid going back to Python.\n",
        "  #     params, opt_state, rngs, env_obs, mean_loss = val\n",
        "  #     return batched_update_fn(params, opt_state, rngs, env_obs, mean_loss)\n",
        "  #   return lax.fori_loop(0, epoque_iters, iterate_fn, (\n",
        "  #       params, opt_state, rngs, env_obs, mean_loss))\n",
        "\n",
        "  return update_fn"
      ],
      "metadata": {
        "id": "E5O7oVye_I7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AxcdZawR-2L0"
      ],
      "toc_visible": true,
      "mount_file_id": "1QW4LOfJ21C_dgXh3C3UXHawi1YgzYTjE",
      "authorship_tag": "ABX9TyO+ryn0dJUjLtf4iKt9skhx",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}