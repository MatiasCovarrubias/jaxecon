{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasCovarrubias/jaxecon/blob/main/RbcProdNet_Sep13_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmNe6o_4tTEy"
      },
      "source": [
        "# DEQN Solver in Jax: Prelims\n",
        "\n",
        "This notebook trains a neural net to output the optimal policy of a nonlinear Rbc model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue0dX6Q14A1Q",
        "outputId": "cef5d4c2-f102-4cd0-db57-c2ca8dbbb64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: jax 0.5.3\n",
            "Uninstalling jax-0.5.3:\n",
            "  Successfully uninstalled jax-0.5.3\n",
            "Found existing installation: jaxlib 0.5.3\n",
            "Uninstalling jaxlib-0.5.3:\n",
            "  Successfully uninstalled jaxlib-0.5.3\n",
            "Collecting jax==0.6.0\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib==0.6.0\n",
            "  Downloading jaxlib-0.6.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax==0.6.0) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from jax==0.6.0) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax==0.6.0) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax==0.6.0) (1.16.2)\n",
            "Downloading jax-0.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.6.0-cp312-cp312-manylinux2014_x86_64.whl (87.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.8/87.8 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jaxlib, jax\n",
            "Successfully installed jax-0.6.0 jaxlib-0.6.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.5.3 is installed, but it is not compatible with the installed jaxlib version 0.6.0, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICES:  [CudaDevice(id=0)]\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade jax jaxlib\n",
        "\n",
        "# PRECISION\n",
        "from jax import numpy as jnp, lax, random, config\n",
        "double_precision = True\n",
        "if double_precision:\n",
        "  config.update(\"jax_enable_x64\", True)\n",
        "  precision = jnp.float64\n",
        "else:\n",
        "  precision = jnp.float32\n",
        "\n",
        "# IMPORTS\n",
        "import matplotlib.pyplot as plt, numpy as np, pandas as pd, jax, flax, optax, os, json\n",
        "import flax.linen as nn\n",
        "from flax.training.train_state import TrainState  # Useful dataclass to keep train state\n",
        "from flax.training import checkpoints\n",
        "from flax.core import freeze, unfreeze\n",
        "import optax\n",
        "from time import time\n",
        "from typing import Sequence\n",
        "config.update(\"jax_debug_nans\", True)\n",
        "\n",
        "print(\"DEVICES: \", jax.devices())\n",
        "\n",
        "# MOUNT GOOGLE DRIVE (a pop up will appear, follow instructions)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# DEFINE WORKING DIR\n",
        "working_dir = \"/content/drive/MyDrive/Jaxecon/RbcProdNet/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2jUK8OalDss"
      },
      "source": [
        "The first time we run this colab, we need to create a daframe that will sstore the results of the different experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlz3RnHYlOV1",
        "outputId": "9e503fde-e790-49df-808c-a68107d26194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The experiments csv already exists\n"
          ]
        }
      ],
      "source": [
        "# Create an empty DataFrame with the desired columns\n",
        "columns = [\n",
        "    \"exper_name\",\n",
        "    \"comment\",\n",
        "    \"min_loss\",\n",
        "    \"max_mean_acc\",\n",
        "    \"max_min_acc\",\n",
        "    \"Time for Full Experiment (m)\",\n",
        "    \"Time for epoch (s)\",\n",
        "    \"Time for Compilation (s)\",\n",
        "    \"Steps per second\",\n",
        "    \"config\",\n",
        "    \"Losses_list\",\n",
        "    \"mean_accuracy_list\",\n",
        "    \"min_accuracy_list\",\n",
        "]\n",
        "\n",
        "# Create an empty DataFrame with the specified columns\n",
        "df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Specify the file path for the CSV file in your working directory\n",
        "csv_filename = os.path.join(working_dir, 'experiment_results.csv')\n",
        "# Save the DataFrame to a CSV file if it doesn't exist\n",
        "if not os.path.isfile(csv_filename):\n",
        "  df.to_csv(csv_filename, index=False)\n",
        "  print(f\"New experiments csv saved as {csv_filename}\")\n",
        "else:\n",
        "  print(\"The experiments csv already exists\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIQLSPJLvaPE"
      },
      "source": [
        "# Create Neural Net Policy\n",
        "\n",
        "First, we use Flax to create the Neural Net, Notice that we activate the last layer using Softplus to guarantee that we get possitive outputs.\n",
        "\n",
        "See https://flax.readthedocs.io/en/latest/getting_started.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVGY1ZCnvtXh"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  features: Sequence[int]\n",
        "  C: jnp.ndarray  # shape (n_states, n_states)\n",
        "  policies_sd: jnp.ndarray  # shape (n_policies,)\n",
        "  param_dtype = precision\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # Ensure 2D for consistent slicing\n",
        "    x_2d = x.reshape(-1, x.shape[-1])  # Always (batch, 111)\n",
        "\n",
        "    # Baseline loglinear policy\n",
        "    baseline = x @ self.C.T\n",
        "    baseline = baseline * self.policies_sd[None, :]  # (batch, n_states)\n",
        "\n",
        "    # Residual MLP\n",
        "    h = x_2d\n",
        "    for feat in self.features:\n",
        "        h = nn.relu(nn.Dense(feat, param_dtype=self.param_dtype)(h))\n",
        "\n",
        "    residual = nn.Dense(\n",
        "        self.C.shape[0],\n",
        "        kernel_init=nn.initializers.zeros,\n",
        "        bias_init=nn.initializers.zeros,\n",
        "        param_dtype=self.param_dtype\n",
        "    )(h)\n",
        "\n",
        "    output = baseline + residual\n",
        "\n",
        "    # Reshape output to match input shape structure\n",
        "    if x.ndim == 1:\n",
        "        output = output.reshape(-1)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LXEca1NwAiC"
      },
      "source": [
        "# Create Economic Model\n",
        "\n",
        "We will represent our model as clas with four main methods (or functions): initial_obs to get first observation; step to advance a period, expectation to get the expectation term given a state, policy, and shock; and a loss funciton that gets as the loss given a state, policy and expectation term."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYFQPBn3Lucz"
      },
      "source": [
        "## Import Parameters of the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC0AHN5BTDQP",
        "outputId": "8b7895f0-de8a-4199-f2fc-c14cf7b2fd0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'parn_sectors': 37, 'parbeta': 0.96, 'pareps_c': 0.5, 'pareps_l': 0.5, 'parphi': 2, 'partheta': 282791.12733419565, 'parsigma_c': 0.5, 'parsigma_m': 0.1, 'parsigma_q': 0.5, 'parsigma_y': 0.8, 'parsigma_I': 0.5, 'parsigma_l': 0.5, 'paralpha': array([0.76246851, 0.81447933, 0.2173877 , 0.35041499, 0.51072498,\n",
            "       0.49043508, 0.43432306, 0.48266322, 0.46770221, 0.45357891,\n",
            "       0.50864942, 0.45744396, 0.30623142, 0.49284854, 0.61563524,\n",
            "       0.33601341, 0.27585449, 0.55552307, 0.23069028, 0.65399183,\n",
            "       0.72390174, 0.48730601, 0.54020845, 0.47152072, 0.42829889,\n",
            "       0.70186202, 0.59354188, 0.96764559, 0.57588724, 0.20876475,\n",
            "       0.33779216, 0.16464859, 0.45550776, 0.57440692, 0.4837216 ,\n",
            "       0.4069147 , 0.41336966]), 'pardelta': array([0.07113207, 0.03325417, 0.12865278, 0.08514028, 0.07672778,\n",
            "       0.05764028, 0.07075   , 0.08320417, 0.13533333, 0.10309861,\n",
            "       0.15317917, 0.11067917, 0.08234722, 0.0942625 , 0.06625972,\n",
            "       0.06867222, 0.06729028, 0.08019444, 0.09138194, 0.073125  ,\n",
            "       0.08682083, 0.10151111, 0.10221806, 0.04677639, 0.04802375,\n",
            "       0.07467899, 0.07716097, 0.03244109, 0.11301604, 0.048125  ,\n",
            "       0.07008811, 0.03102361, 0.04592889, 0.07246535, 0.03796806,\n",
            "       0.07057639, 0.03986667]), 'parmu': array([0.5987729 , 0.66798151, 0.22943181, 0.29958969, 0.43999103,\n",
            "       0.26990444, 0.34064709, 0.41093444, 0.4467065 , 0.43367542,\n",
            "       0.24227738, 0.43897372, 0.38804311, 0.40666965, 0.26331014,\n",
            "       0.24679222, 0.26719642, 0.35585532, 0.35254154, 0.17918909,\n",
            "       0.44495804, 0.40456639, 0.60246306, 0.45671343, 0.38409397,\n",
            "       0.63528725, 0.48091896, 0.87688844, 0.49715331, 0.43336792,\n",
            "       0.48455498, 0.36854315, 0.45829807, 0.61382554, 0.62073198,\n",
            "       0.34256064, 0.46428268]), 'parrho': array([0.88622525, 0.63630922, 0.90547934, 0.81078663, 0.68797083,\n",
            "       0.64406222, 0.58967978, 0.86863773, 0.7991582 , 0.56406776,\n",
            "       0.3503659 , 0.72835564, 0.60382004, 0.65948395, 0.25316946,\n",
            "       0.58523282, 0.68595472, 0.4787086 , 0.77832382, 0.66198059,\n",
            "       0.62875339, 0.69839426, 0.72707346, 0.81387911, 0.7193329 ,\n",
            "       0.7998806 , 0.59552462, 0.78888751, 0.85221058, 0.72848257,\n",
            "       0.84271451, 0.91784638, 0.7437725 , 0.91644303, 0.81458763,\n",
            "       0.60959122, 0.89782365]), 'parxi': array([5.22929259e-03, 4.70399119e-02, 1.09788511e-05, 1.08045893e-03,\n",
            "       2.32964271e-03, 2.85767021e-03, 4.15204604e-03, 9.60650372e-03,\n",
            "       1.34180743e-02, 8.90215884e-03, 3.51494760e-02, 9.12037397e-03,\n",
            "       6.25947546e-03, 1.26926006e-02, 1.00004265e-01, 6.99332709e-03,\n",
            "       2.87872706e-02, 5.23204340e-03, 7.25126391e-04, 3.62075849e-02,\n",
            "       3.37985739e-02, 5.43279557e-03, 3.60030917e-02, 1.16873246e-01,\n",
            "       2.98989057e-02, 4.48113965e-02, 5.95210268e-02, 3.84149945e-01,\n",
            "       1.52317300e-02, 6.68960222e-04, 4.50012914e-03, 1.78172066e-02,\n",
            "       1.00765250e-01, 1.84970439e-02, 9.16758897e-03, 5.28561681e-02,\n",
            "       4.79314253e-02]), 'parGamma_I': array([[7.44894045e-01, 5.24545544e-03, 1.89634460e-03, ...,\n",
            "        2.04893206e-03, 1.93333781e-03, 1.97736195e-03],\n",
            "       [1.20716121e-03, 1.96398014e-03, 1.88098691e-03, ...,\n",
            "        2.03233863e-03, 1.91768053e-03, 1.96134813e-03],\n",
            "       [4.72342999e-02, 4.52000312e-01, 7.20162408e-02, ...,\n",
            "        6.33532068e-01, 1.98086544e-01, 4.10734346e-01],\n",
            "       ...,\n",
            "       [8.06479816e-04, 1.31209512e-03, 1.25664903e-03, ...,\n",
            "        1.35776403e-03, 1.28116329e-03, 1.31033673e-03],\n",
            "       [6.55195022e-04, 1.06596368e-03, 1.02091853e-03, ...,\n",
            "        1.10306572e-03, 1.04083425e-03, 1.06453514e-03],\n",
            "       [6.02069821e-04, 9.79532107e-04, 9.38139360e-04, ...,\n",
            "        1.01362580e-03, 9.56440247e-04, 9.78219399e-04]]), 'parGamma_M': array([[2.66107361e-01, 5.62484187e-01, 3.87432773e-02, ...,\n",
            "        2.35857222e-03, 1.97263257e-03, 2.55374862e-03],\n",
            "       [5.42599712e-02, 9.96459669e-02, 1.24211222e-02, ...,\n",
            "        1.46942456e-01, 7.36658424e-02, 6.18780787e-02],\n",
            "       [3.11244322e-03, 8.56549674e-03, 5.38888665e-04, ...,\n",
            "        6.48454267e-03, 2.24529904e-03, 6.55436682e-03],\n",
            "       ...,\n",
            "       [9.97967948e-04, 2.02760653e-03, 1.65386952e-03, ...,\n",
            "        1.49306682e-02, 1.75097223e-03, 9.46865438e-03],\n",
            "       [1.51435979e-03, 4.15102794e-03, 3.37941669e-03, ...,\n",
            "        2.76912960e-02, 3.06460124e-03, 7.89036775e-03],\n",
            "       [2.32383721e-03, 4.17932879e-03, 1.22317260e-02, ...,\n",
            "        3.49745538e-02, 1.05345082e-02, 3.13578679e-02]]), 'parSigma_A': array([[ 6.27552599e-03, -5.48014181e-04,  4.17998623e-04, ...,\n",
            "        -6.88113170e-04,  2.22252783e-04,  3.68897033e-04],\n",
            "       [-5.48014181e-04,  1.99306869e-03,  5.97624024e-06, ...,\n",
            "         2.78627974e-04,  2.39868593e-04,  2.67718338e-04],\n",
            "       [ 4.17998623e-04,  5.97624024e-06,  8.66206938e-04, ...,\n",
            "         1.37602897e-04, -8.69078637e-05,  2.12953055e-04],\n",
            "       ...,\n",
            "       [-6.88113170e-04,  2.78627974e-04,  1.37602897e-04, ...,\n",
            "         1.27503503e-03,  3.11218481e-04,  3.61169746e-04],\n",
            "       [ 2.22252783e-04,  2.39868593e-04, -8.69078637e-05, ...,\n",
            "         3.11218481e-04,  1.44389200e-03, -8.36946224e-05],\n",
            "       [ 3.68897033e-04,  2.67718338e-04,  2.12953055e-04, ...,\n",
            "         3.61169746e-04, -8.36946224e-05,  6.69542987e-04]])}\n"
          ]
        }
      ],
      "source": [
        "import scipy.io as sio\n",
        "model_name = \"Feb21_24_baselinev3.mat\"\n",
        "model_data = sio.loadmat(working_dir + \"Model_Data/RbcProdNet_SolData_\" + model_name, simplify_cells=True)\n",
        "modparams = model_data[\"SolData\"][\"parameters\"]  # dictionary with model parameters of the model\n",
        "policies_ss = model_data[\"SolData\"][\"policies_ss\"]  # dictionary with model parameters of the model\n",
        "k_ss = model_data[\"SolData\"][\"k_ss\"]\n",
        "policies_sd = model_data[\"SolData\"][\"policies_sd\"]  # dictionary with model parameters of the model\n",
        "states_sd = model_data[\"SolData\"][\"states_sd\"]  # dictionary with model parameters of the model\n",
        "shocks_sd = model_data[\"SolData\"][\"shocks_sd\"]  # dictionary with model parameters of the model\n",
        "A = model_data[\"SolData\"][\"A\"]\n",
        "B = model_data[\"SolData\"][\"B\"]\n",
        "C = model_data[\"SolData\"][\"C\"]\n",
        "D = model_data[\"SolData\"][\"D\"]\n",
        "n_sectors = modparams[\"parn_sectors\"]\n",
        "print(modparams)\n",
        "del model_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(states_sd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gtA2pDCyA-J",
        "outputId": "06a4ae48-65c0-4fb4-b95a-dcfcb45d217c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.04515133 0.03403735 0.08149502 0.10456063 0.05156925 0.05730716\n",
            " 0.05469752 0.03207556 0.05441389 0.04682447 0.04630113 0.05072055\n",
            " 0.06580466 0.04258033 0.05302759 0.07543571 0.05762596 0.05643485\n",
            " 0.07725362 0.05537577 0.03626287 0.0572563  0.05564264 0.04074429\n",
            " 0.04199542 0.03781637 0.03965085 0.03402221 0.03517175 0.04551916\n",
            " 0.0584624  0.02805981 0.04183397 0.03928181 0.04694842 0.05608965\n",
            " 0.03324947 0.1965228  0.05434032 0.05390058 0.10868998 0.07450848\n",
            " 0.08911767 0.06404581 0.16582568 0.0904109  0.07320349 0.14056454\n",
            " 0.10458124 0.06094005 0.07292626 0.04934042 0.07508559 0.040071\n",
            " 0.0742004  0.05010725 0.29439812 0.08114295 0.07334797 0.03927505\n",
            " 0.04591136 0.0449825  0.07102711 0.04402118 0.02309391 0.0373194\n",
            " 0.05784268 0.03522574 0.05849283 0.02823388 0.04865723 0.04147002\n",
            " 0.04459083 0.05860165]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgRHYZULMIJv"
      },
      "source": [
        "## Create Model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ueaQPW4GbcR"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "  \"\"\"A JAX implementation of an RBC model.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "    modparams = modparams, k_ss=k_ss, policies_ss=policies_ss, state_sd=states_sd,\n",
        "    shocks_sd=shocks_sd, policies_sd=policies_sd, A=A, B=B, C=C, D=D):\n",
        "\n",
        "    self.alpha = jnp.array(modparams[\"paralpha\"], dtype = precision)\n",
        "    self.beta = jnp.array(modparams[\"parbeta\"], dtype = precision)\n",
        "    self.delta = jnp.array(modparams[\"pardelta\"], dtype = precision)\n",
        "    self.rho = jnp.array(modparams[\"parrho\"], dtype = precision)\n",
        "    self.eps_c = jnp.array(modparams[\"pareps_c\"], dtype = precision)\n",
        "    self.eps_l = jnp.array(modparams[\"pareps_l\"], dtype = precision)\n",
        "    self.phi = jnp.array(modparams[\"parphi\"], dtype = precision)\n",
        "    self.theta = jnp.array(modparams[\"partheta\"], dtype = precision)\n",
        "    self.sigma_c = jnp.array(modparams[\"parsigma_c\"], dtype = precision)\n",
        "    self.sigma_m = jnp.array(modparams[\"parsigma_m\"], dtype = precision)\n",
        "    self.sigma_q = jnp.array(modparams[\"parsigma_q\"], dtype = precision)\n",
        "    self.sigma_y = jnp.array(modparams[\"parsigma_y\"], dtype = precision)\n",
        "    self.sigma_I = jnp.array(modparams[\"parsigma_I\"], dtype = precision)\n",
        "    self.sigma_l = jnp.array(modparams[\"parsigma_l\"], dtype = precision)\n",
        "    self.xi = jnp.array(modparams[\"parxi\"], dtype = precision)\n",
        "    self.mu = jnp.array(modparams[\"parmu\"], dtype = precision)\n",
        "    self.Gamma_M = jnp.array(modparams[\"parGamma_M\"], dtype = precision)\n",
        "    self.Gamma_I = jnp.array(modparams[\"parGamma_I\"], dtype = precision)\n",
        "    self.Sigma_A = jnp.array(modparams[\"parSigma_A\"], dtype = precision)\n",
        "    self.n_sectors = modparams[\"parn_sectors\"]\n",
        "    self.state_ss = jnp.concatenate([k_ss,jnp.zeros(shape=(1*self.n_sectors,), dtype = precision)])\n",
        "    self.policies_ss = jnp.array(policies_ss, dtype = precision)\n",
        "\n",
        "    self.A = A\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "    self.D = D\n",
        "    self.state_sd = jnp.array(state_sd, dtype = precision)\n",
        "    self.policies_sd = jnp.array(policies_sd, dtype = precision)\n",
        "    self.dim_policies = len(policies_ss)\n",
        "    self.dim_states = len(self.state_ss)\n",
        "    self.n_actions = len(policies_ss)\n",
        "    self.L_cholesky = jnp.linalg.cholesky(self.Sigma_A)\n",
        "\n",
        "  def initial_obs(self, rng, range=1):\n",
        "    \"\"\" Get initial obs given first shock \"\"\"\n",
        "\n",
        "    rng_k, rng_a, rng_e, rng_c = random.split(rng,4)\n",
        "    e = self.sample_shock(rng)                                                  # sample a realization of the shock\n",
        "    k_ss = self.state_ss[:self.n_sectors]                                         # get log K in StSt\n",
        "    a_ss = self.state_ss[self.n_sectors:]                         # get log A in StSt\n",
        "    K_init = random.uniform(rng_k, shape=(self.n_sectors,), minval=(1-range/100)*jnp.exp(self.state_ss[:self.n_sectors]),maxval = (1+range/300)*jnp.exp(self.state_ss[:self.n_sectors]))\n",
        "    A_init = random.uniform(rng_a, shape=(self.n_sectors,), minval=(1-range/100),maxval = (1+range/100))\n",
        "    state_init_notnorm = jnp.concatenate([jnp.log(K_init),jnp.log(A_init)])\n",
        "    state_init = (state_init_notnorm-self.state_ss)/self.state_sd                       # normalize\n",
        "    return random.choice(rng_c,jnp.array([state_init]))\n",
        "\n",
        "\n",
        "  def step(self, state, policy, shock):\n",
        "    \"\"\" A period step of the model, given current obs, the shock and policy_params \"\"\"\n",
        "\n",
        "    state_notnorm = state*self.state_sd + self.state_ss                                 # denormalize obs\n",
        "    K = jnp.exp(state_notnorm[:self.n_sectors])                                   # extract k and put in levels\n",
        "    a = state_notnorm[self.n_sectors:]\n",
        "    a_next = self.rho*a + shock\n",
        "    policy_notnorm = jnp.exp(policy*self.policies_sd+self.policies_ss)                           # denormalize policy\n",
        "\n",
        "    I = policy_notnorm[6*self.n_sectors:7*self.n_sectors]\n",
        "    K_tplus1 = (1-self.delta)*K + I - (self.phi/2) * (I/K - self.delta)**2 * K     # update K\n",
        "    state_next_notnorm = jnp.concatenate([jnp.log(K_tplus1),a_next])            # calculate next obs not notrm\n",
        "    state_next = (state_next_notnorm-self.state_ss)/self.state_sd                       # normalize\n",
        "\n",
        "    return state_next\n",
        "\n",
        "\n",
        "  def expect_realization(self, obs_next, policy_next):\n",
        "    \"\"\" A realization (given a shock) of the expectation terms in system of equation \"\"\"\n",
        "\n",
        "    # Process observation\n",
        "    obs_next_notnorm = obs_next*self.state_sd + self.state_ss# denormalize\n",
        "    K_next = jnp.exp(obs_next_notnorm[:self.n_sectors]) # put in levels\n",
        "    a_next = obs_next_notnorm[self.n_sectors:2*self.n_sectors]\n",
        "    A_next = jnp.exp(a_next)\n",
        "\n",
        "    # Calculate tplus1 policies\n",
        "    policy_next_notnorm = jnp.exp(policy_next*self.policies_sd+self.policies_ss)                           # denormalize policy\n",
        "    Pk_next = policy_next_notnorm[2*self.n_sectors:3*self.n_sectors]\n",
        "    I_next = policy_next_notnorm[6*self.n_sectors:7*self.n_sectors]\n",
        "    P_next = policy_next_notnorm[8*self.n_sectors:9*self.n_sectors]\n",
        "    Q_next = policy_next_notnorm[9*self.n_sectors:10*self.n_sectors]\n",
        "    Y_next = policy_next_notnorm[10*self.n_sectors:11*self.n_sectors]\n",
        "\n",
        "    # Solve for the expectation term in the FOC for Ktplus1\n",
        "    expect_realization = (P_next*A_next**((self.sigma_y-1)/self.sigma_y) * (self.mu*Q_next/Y_next)**(1/self.sigma_q) *(self.alpha*Y_next/K_next)**(1/self.sigma_y)\n",
        "      + Pk_next*((1-self.delta) + self.phi/2*(I_next**2 / K_next**2-self.delta**2)))\n",
        "\n",
        "    return jax.lax.stop_gradient(expect_realization)\n",
        "\n",
        "  def loss(self, state, expect, policy):\n",
        "    \"\"\" Calculate loss associated with observing obs, having policy_params, and expectation exp \"\"\"\n",
        "\n",
        "    # Process observation\n",
        "    state_notnorm = state*self.state_sd + self.state_ss# denormalize\n",
        "    K = jnp.exp(state_notnorm[:self.n_sectors]) # put in levels\n",
        "    a = state_notnorm[self.n_sectors:]\n",
        "    A = jnp.exp(a)\n",
        "\n",
        "    # Process policy\n",
        "    policy_notnorm = jnp.exp(policy*self.policies_sd+self.policies_ss)                           # denormalize policy\n",
        "    C = policy_notnorm[:self.n_sectors]\n",
        "    L = policy_notnorm[self.n_sectors:2*self.n_sectors]\n",
        "    Pk = policy_notnorm[2*self.n_sectors:3*self.n_sectors]\n",
        "    Pm = policy_notnorm[3*self.n_sectors:4*self.n_sectors]\n",
        "    M = policy_notnorm[4*self.n_sectors:5*self.n_sectors]\n",
        "    Mout = policy_notnorm[5*self.n_sectors:6*self.n_sectors]\n",
        "    I = policy_notnorm[6*self.n_sectors:7*self.n_sectors]\n",
        "    Iout = policy_notnorm[7*self.n_sectors:8*self.n_sectors]\n",
        "    P = policy_notnorm[8*self.n_sectors:9*self.n_sectors]\n",
        "    Q = policy_notnorm[9*self.n_sectors:10*self.n_sectors]\n",
        "    Y = policy_notnorm[10*self.n_sectors:11*self.n_sectors]\n",
        "    Cagg = policy_notnorm[11*self.n_sectors]\n",
        "    Lagg = policy_notnorm[11*self.n_sectors+1]\n",
        "    Yagg = policy_notnorm[11*self.n_sectors+2]\n",
        "    Iagg = policy_notnorm[11*self.n_sectors+3]\n",
        "    Magg = policy_notnorm[11*self.n_sectors+4]\n",
        "\n",
        "    # get steady state prices to aggregate Y, I and M\n",
        "    Pss = jnp.exp(policies_ss[8*self.n_sectors:9*self.n_sectors])\n",
        "    Pkss = jnp.exp(policies_ss[2*self.n_sectors:3*self.n_sectors])\n",
        "    Pmss = jnp.exp(policies_ss[3*self.n_sectors:4*self.n_sectors])\n",
        "    capadj_term = 1-self.phi*(I/K-self.delta)\n",
        "\n",
        "    # auxialiry variables\n",
        "    Pagg = (self.xi.T @ P ** (1 - self.sigma_c)) ** (1 / (1 - self.sigma_c))\n",
        "    MgUtCagg = (Cagg - self.theta * 1 / (1 + self.eps_l ** (-1)) * Lagg ** (1 + self.eps_l ** (-1))) ** (-self.eps_c ** (-1))\n",
        "\n",
        "    # key variables for loss function\n",
        "    MgUtCmod = MgUtCagg * (Cagg * self.xi / C) ** (1 / self.sigma_c)\n",
        "    MgUtLmod = MgUtCagg * self.theta * Lagg ** (self.eps_l ** -1) * (L / Lagg) ** (1 / self.sigma_l)\n",
        "    MPLmod = P * A**((self.sigma_y-1)/self.sigma_y) * (self.mu * Q / Y) ** (1 / self.sigma_q) * ((1 - self.alpha) * Y / L) ** (1 / self.sigma_y)\n",
        "    MPKmod = self.beta * expect\n",
        "    Pmdef = (self.Gamma_M.T @ P ** (1 - self.sigma_m)) ** (1 / (1 - self.sigma_m))\n",
        "    Mmod = (1 - self.mu) * (Pm / P) ** (-self.sigma_q) * Q\n",
        "    Moutmod = P ** (-self.sigma_m) * jnp.dot(self.Gamma_M, Pm**self.sigma_m * M)\n",
        "    Pkdef = (self.Gamma_I.T @ P ** (1 - self.sigma_I)) ** (1 / (1 - self.sigma_I)) * capadj_term**(-1)\n",
        "    Ioutmod = P ** (-self.sigma_I) * jnp.dot( self.Gamma_I,Pk**self.sigma_I * I * capadj_term**(self.sigma_I) )\n",
        "    Qrc = C + Mout + Iout\n",
        "    Qdef = ( self.mu**(1/self.sigma_q) * Y**((self.sigma_q-1)/self.sigma_q) + (1-self.mu)**(1/self.sigma_q) * M**((self.sigma_q-1)/self.sigma_q) ) ** (self.sigma_q/(self.sigma_q-1))\n",
        "    Ydef = A * ( self.alpha**(1/self.sigma_y) * K**((self.sigma_y-1)/self.sigma_y) + (1-self.alpha)**(1/self.sigma_y) * L**((self.sigma_y-1)/self.sigma_y) ) ** (self.sigma_y/(self.sigma_y-1))\n",
        "    Caggdef = ( (self.xi**(1/self.sigma_c)).T @ C**((self.sigma_c-1)/self.sigma_c) ) ** (self.sigma_c/(self.sigma_c-1))\n",
        "    Laggdef = jnp.sum( L**((self.sigma_l+1)/self.sigma_l) ) ** (self.sigma_l/(self.sigma_l+1))\n",
        "    Yaggdef = jnp.sum(Y * Pss)\n",
        "    Iaggdef = jnp.sum(I * Pkss)\n",
        "    Maggdef = jnp.sum(M * Pmss)\n",
        "\n",
        "    C_loss = P/MgUtCmod - 1;\n",
        "    L_loss = MgUtLmod/MPLmod - 1;\n",
        "    K_loss = Pk/MPKmod - 1;\n",
        "    Pm_loss = Pm/Pmdef - 1;\n",
        "    M_loss = M/Mmod - 1;\n",
        "    Mout_loss = Mout/Moutmod - 1;\n",
        "    Pk_loss = Pk/Pkdef - 1;\n",
        "    Iout_loss = Iout/Ioutmod - 1;\n",
        "    Qrc_loss = Q/Qrc - 1;\n",
        "    Qdef_loss = Q/Qdef - 1;\n",
        "    Ydef_loss = Y/Ydef - 1;\n",
        "    Caggdef_loss = jnp.array([Cagg/Caggdef - 1]);\n",
        "    Laggdef_loss = jnp.array([Lagg/Laggdef - 1]);\n",
        "    Yaggdef_loss = jnp.array([Yagg/Yaggdef - 1]);\n",
        "    Iaggdef_loss = jnp.array([Iagg/Iaggdef - 1]);\n",
        "    Maggdef_loss = jnp.array([Magg/Maggdef - 1]);\n",
        "\n",
        "    losses_array = jnp.concatenate([C_loss,L_loss,K_loss,Pm_loss,M_loss,Mout_loss,Pk_loss,\n",
        "                              Iout_loss,Qrc_loss,Qdef_loss,Ydef_loss,Caggdef_loss,\n",
        "                              Laggdef_loss,Yaggdef_loss,Iaggdef_loss,Maggdef_loss], axis =0)\n",
        "\n",
        "    # Calculate aggregate losses and metrics\n",
        "    mean_loss = jnp.mean(losses_array**2)\n",
        "    mean_accuracy = jnp.mean(1-jnp.abs(losses_array))\n",
        "    min_accuracy = jnp.min(1-jnp.abs(losses_array))\n",
        "    mean_accuracies_focs = jnp.array([jnp.mean(1-jnp.abs(C_loss)),jnp.mean(1-jnp.abs(L_loss)),jnp.mean(1-jnp.abs(K_loss)),jnp.mean(1-jnp.abs(Pm_loss)),jnp.mean(1-jnp.abs(M_loss)),jnp.mean(1-jnp.abs(Mout_loss)),jnp.mean(1-jnp.abs(Pk_loss)),\n",
        "                              jnp.mean(1-jnp.abs(Iout_loss)),jnp.mean(1-jnp.abs(Qrc_loss)),jnp.mean(1-jnp.abs(Qdef_loss)),jnp.mean(1-jnp.abs(Ydef_loss)),jnp.mean(1-jnp.abs(Caggdef_loss)),\n",
        "                              jnp.mean(1-jnp.abs(Laggdef_loss)),jnp.mean(1-jnp.abs(Yaggdef_loss)),jnp.mean(1-jnp.abs(Iaggdef_loss)),jnp.mean(1-jnp.abs(Maggdef_loss))])\n",
        "\n",
        "    min_accuracies_focs = jnp.array([jnp.min(1-jnp.abs(C_loss)),jnp.min(1-jnp.abs(L_loss)),jnp.min(1-jnp.abs(K_loss)),jnp.min(1-jnp.abs(Pm_loss)),jnp.min(1-jnp.abs(M_loss)),jnp.min(1-jnp.abs(Mout_loss)),jnp.min(1-jnp.abs(Pk_loss)),\n",
        "                              jnp.min(1-jnp.abs(Iout_loss)),jnp.min(1-jnp.abs(Qrc_loss)),jnp.min(1-jnp.abs(Qdef_loss)),jnp.min(1-jnp.abs(Ydef_loss)),jnp.min(1-jnp.abs(Caggdef_loss)),\n",
        "                              jnp.min(1-jnp.abs(Laggdef_loss)),jnp.min(1-jnp.abs(Yaggdef_loss)),jnp.min(1-jnp.abs(Iaggdef_loss)),jnp.min(1-jnp.abs(Maggdef_loss))])\n",
        "\n",
        "    return mean_loss, mean_accuracy, min_accuracy, mean_accuracies_focs, min_accuracies_focs\n",
        "\n",
        "  # def sample_shock(self, rng):\n",
        "  #   \"\"\" sample one realization of the shock \"\"\"\n",
        "  #   z = jax.random.normal(rng, shape=(self.n_sectors,))\n",
        "  #   shock = self.L_cholesky @ z\n",
        "  #   return shock\n",
        "\n",
        "  # def mc_shocks(self, rng=random.PRNGKey(0), mc_draws=8):\n",
        "  #   \"\"\" sample mc_draws realizations of the shock (for monte-carlo) \"\"\"\n",
        "  #   z = jax.random.normal(rng, shape=(mc_draws, self.n_sectors))\n",
        "  #   mc_shocks = jax.vmap(lambda zi: self.L_cholesky @ zi)(z)\n",
        "  #   return mc_shocks\n",
        "\n",
        "  def sample_shock(self, rng):\n",
        "    \"\"\" sample one realization of the shock \"\"\"\n",
        "    return jax.random.multivariate_normal(rng, jnp.zeros((self.n_sectors,)), self.Sigma_A)\n",
        "\n",
        "  def mc_shocks(self, rng=random.PRNGKey(0), mc_draws=8):\n",
        "    \"\"\"\n",
        "    Optimized for highly nonlinear functions using:\n",
        "    - Antithetic variates (always helps with symmetric distributions)\n",
        "    - Latin Hypercube Sampling (better than simple stratification for nonlinear cases)\n",
        "    - Optional importance sampling direction (if you know where nonlinearity is strongest)\n",
        "    \"\"\"\n",
        "\n",
        "    # Latin Hypercube Sampling for better space-filling with nonlinear functions\n",
        "    def latin_hypercube_sample(key, n_samples, n_dims):\n",
        "        \"\"\"Generate Latin Hypercube samples\"\"\"\n",
        "        keys = random.split(key, n_dims)\n",
        "\n",
        "        # Create permutations for each dimension\n",
        "        perms = jnp.stack([\n",
        "            random.permutation(keys[i], n_samples)\n",
        "            for i in range(n_dims)\n",
        "        ], axis=1)\n",
        "\n",
        "        # Add uniform noise within each cell\n",
        "        key_uniform = random.fold_in(key, 1)\n",
        "        uniform_noise = random.uniform(\n",
        "            key_uniform,\n",
        "            shape=(n_samples, n_dims)\n",
        "        )\n",
        "\n",
        "        # Create LHS samples in [0,1]^d\n",
        "        lhs_samples = (perms + uniform_noise) / n_samples\n",
        "        return lhs_samples\n",
        "\n",
        "    # Decide on sampling strategy\n",
        "    use_antithetic = (mc_draws % 2 == 0)\n",
        "\n",
        "    if use_antithetic:\n",
        "        # Generate half samples with LHS, create antithetic pairs\n",
        "        n_base = mc_draws // 2\n",
        "        key1, key2 = random.split(rng)\n",
        "\n",
        "        # Latin hypercube sampling for base samples\n",
        "        u_lhs = latin_hypercube_sample(key1, n_base, self.n_sectors)\n",
        "\n",
        "        # Transform to standard normal\n",
        "        u_lhs = jnp.clip(u_lhs, 1e-6, 1 - 1e-6)\n",
        "        z_base = jax.scipy.stats.norm.ppf(u_lhs)\n",
        "\n",
        "        # Create antithetic pairs (works well even for nonlinear functions)\n",
        "        z = jnp.vstack([z_base, -z_base])\n",
        "\n",
        "    else:\n",
        "        # Full Latin Hypercube Sampling\n",
        "        u_lhs = latin_hypercube_sample(rng, mc_draws, self.n_sectors)\n",
        "        u_lhs = jnp.clip(u_lhs, 1e-6, 1 - 1e-6)\n",
        "        z = jax.scipy.stats.norm.ppf(u_lhs)\n",
        "\n",
        "    # Optional: Add controlled noise for highly discontinuous functions\n",
        "    # This can help explore around discontinuities\n",
        "    # key_noise = random.fold_in(rng, 2)\n",
        "    # noise = 0.1 * random.normal(key_noise, shape=z.shape)\n",
        "    # z = z + noise\n",
        "\n",
        "    # Transform to target distribution\n",
        "    if hasattr(self, 'L_cholesky'):\n",
        "        mc_shocks = jax.vmap(lambda zi: self.L_cholesky @ zi)(z)\n",
        "    else:\n",
        "        L_cholesky = jnp.linalg.cholesky(self.Sigma_A)\n",
        "        mc_shocks = jax.vmap(lambda zi: L_cholesky @ zi)(z)\n",
        "\n",
        "    return mc_shocks\n",
        "\n",
        "\n",
        "  def ir_shocks(self):\n",
        "    \"\"\" (Optional) Define a set of shocks sequences that are of interest\"\"\"\n",
        "    # ir_shock_1 = jnp.array([-1]+[0 for i in range(40)])\n",
        "    # ir_shock_2 = jnp.array([1]+[0 for i in range(40)])\n",
        "    ir_shock_1 = jnp.zeros(shape=(40,self.n_sectors), dtype = precision).at[0,0].set(-1)\n",
        "    ir_shock_2 = jnp.zeros(shape=(40,self.n_sectors), dtype = precision).at[0,0].set(1)\n",
        "\n",
        "    return jnp.array([ir_shock_1, ir_shock_2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz-RH8Wt23Rz"
      },
      "source": [
        "## Test the environment\n",
        "We are going to make sure that the functions in our model are correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpWtHMnC9Lnm",
        "outputId": "522bcb1d-63ea-4760-b7cc-bbfca61411dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of zeros like ss: float64\n",
            "type of nn_policy: float64\n",
            "next obs with policy_ss [-2.45889331e-15  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  3.99054793e-15  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00]\n",
            "type of next obs float64\n",
            "type of mean_loss: float64\n",
            "test one period: \n",
            " , Mean_loss: 2.3452059120944013e-06 , Mean_accuracy: 0.9992804429916019 , Min_accuracy: 0.9856937278317419\n",
            "next obs for montecarlo with policies =1 (should be an array with multiple obs =0) [[0.47584681 0.18500831 0.25312386 ... 0.         0.         0.        ]\n",
            " [0.47584681 0.18500831 0.25312386 ... 0.         0.         0.        ]\n",
            " [0.47584681 0.18500831 0.25312386 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.47584681 0.18500831 0.25312386 ... 0.         0.         0.        ]\n",
            " [0.47584681 0.18500831 0.25312386 ... 0.         0.         0.        ]\n",
            " [0.47584681 0.18500831 0.25312386 ... 0.         0.         0.        ]]\n",
            "expectation [0.54022321 1.24316832 1.17814549 1.20084035 1.16700062 1.16238339\n",
            " 1.1642633  1.13292192 1.11627364 1.10166217 1.11501951 1.09496015\n",
            " 1.17338278 1.13527154 1.1617106  1.17870434 1.15825781 1.15221448\n",
            " 1.16367088 1.17122268 1.10077897 1.16671145 1.13344159 1.222231\n",
            " 1.0941424  0.92864408 1.12394271 1.25624113 1.02509417 1.18629672\n",
            " 1.15570158 1.27589543 1.18752323 0.89041874 1.3181667  1.19274644\n",
            " 1.24158453]\n",
            "test that StSt. policies give 0 loss in StSt. obs: \n",
            " , Mean_loss: 0.00709983709816892 , Mean_accuracy: 0.9557703222791234 , Min_accuracy: 0.24536384609019413\n"
          ]
        }
      ],
      "source": [
        "env_test = Model()\n",
        "rng_test = random.PRNGKey(4)\n",
        "\n",
        "# STEADY STATE POLICY  AND STEP WITH RANDOM NEURAL NET\n",
        "obs_ss_norm = jnp.zeros_like(env_test.state_ss)                                   # get StSt obs\n",
        "print(\"type of zeros like ss:\", obs_ss_norm.dtype)\n",
        "obs_test = obs_ss_norm                                                               # set obs that we will test to StSt\n",
        "nn_test = NeuralNet([32,32] + [env_test.dim_policies], C, policies_sd)                             # initialize NN class\n",
        "nn_policy_test = nn_test.apply                                                  # initialize policy fn\n",
        "params_test = nn_test.init(rng_test, env_test.initial_obs(rng_test))            # initialize params\n",
        "policy_test = nn_policy_test(params_test,obs_test)                              # get policy\n",
        "print(\"type of nn_policy:\", policy_test.dtype)\n",
        "policy_test_ones = jnp.zeros_like(env_test.policies_ss)                             # get policy\n",
        "# print(\"policy in steady state\", policy_test)\n",
        "next_obs_sspol = env_test.step(obs_test, policy_test, jnp.zeros_like(env_test.sample_shock(rng_test)))\n",
        "print(\"next obs with policy_ss\", next_obs_sspol)\n",
        "print(\"type of next obs\", next_obs_sspol.dtype)\n",
        "\n",
        "# print(\"next obs with policy= ones (so steady state)\", env_test.step(\n",
        "#     obs_test, policy_test_ones, jnp.zeros_like(env_test.sample_shock(rng_test))))    # get next obs for random policy\n",
        "\n",
        "# STEADY STATE LOSS WITH RANDOM NEURAL NET\n",
        "expect_test = env_test.expect_realization(obs_ss_norm, policy_test)             # calculate expectations\n",
        "mean_loss, mean_accuracy, min_accuracy, mean_acc_kfoc, min_acc_kfoc = env_test.loss(\n",
        "    obs_ss_norm, expect_test, policy_test)                                      # calculate loss given expect. and policy\n",
        "print(\"type of mean_loss:\", mean_loss.dtype)\n",
        "# print(\"loss with random policy in StSt: \\n\",\n",
        "#       \", Mean_loss:\", mean_loss,\n",
        "#       \", Mean_accuracy:\", mean_accuracy,\n",
        "#       \", Min_accuracy:\", min_accuracy)\n",
        "\n",
        "# POLICY AND STEP OVER INTIAL OBS\n",
        "obs_test = env_test.initial_obs(rng_test)                                       # get init obs (not necessariliy ==StStobs)\n",
        "# print(\"initial obs\", obs_test)\n",
        "\n",
        "# # apply a step\n",
        "policy_test = nn_policy_test(params_test,obs_test)                              # get policy for first step\n",
        "# print(\"policy in first step\", policy_test)\n",
        "shock_test = env_test.sample_shock(rng_test)                                    # get shock\n",
        "# print(\"Realization of shock\", shock_test)\n",
        "next_obs_test = env_test.step(obs_test, policy_test, shock_test)                # make a step\n",
        "# print(\"next obs first step\", next_obs_test)\n",
        "\n",
        "#  LOSS IN INITAL OBS\n",
        "# First, we calculate expectations\n",
        "mc_shocks_test = env_test.mc_shocks(rng_test, mc_draws = 1280)                  # get mc shock\n",
        "mc_nextobs_test = jax.vmap(env_test.step, in_axes = (None,None,0))(\n",
        "    obs_test, policy_test, mc_shocks_test)                                      # next obs given policy and for each shock in mc_shocks\n",
        "# mc_nextobs_test = jax.vmap(env_test.step, in_axes = (None,0))(obs_ss_norm, mc_shocks_test)\n",
        "mc_nextpols_test = nn_policy_test(params_test, mc_nextobs_test)\n",
        "expect_test = jnp.mean(jax.vmap(env_test.expect_realization)(\n",
        "        mc_nextobs_test, mc_nextpols_test), axis=0)                             # calculate expectations\n",
        "# print(\"expect in oneperiodtest\", expect_test)\n",
        "# Second, we calculate loss given expectations and policy\n",
        "mean_loss, mean_accuracy, min_accuracy, mean_acc_kfoc, min_acc_kfoc = env_test.loss(\n",
        "    obs_ss_norm, expect_test, policy_test)                                         # calculate loss\n",
        "print(\"test one period: \\n\",\n",
        "      \", Mean_loss:\", mean_loss,\n",
        "      \", Mean_accuracy:\", mean_accuracy,\n",
        "      \", Min_accuracy:\", min_accuracy)\n",
        "\n",
        "# LOSS IN STST WITH STST POLICY\n",
        "obs_test = obs_ss_norm\n",
        "policy_test = jnp.ones_like(policy_test)                                        # policies=1 pass the StSt policies to the env\n",
        "mc_nextobs_test = jax.vmap(env_test.step, in_axes = (None,None,0))(\n",
        "    obs_test, policy_test, jnp.zeros_like(mc_shocks_test))                      # next obs given policy and for each shock in mc_shocks\n",
        "print(\"next obs for montecarlo with policies =1\",\n",
        "      \"(should be an array with multiple obs =0)\", mc_nextobs_test)\n",
        "mc_nextpols_test = jnp.ones_like(mc_nextpols_test)                              # policies for mc_nextobs\n",
        "expect_test = jnp.mean(jax.vmap(env_test.expect_realization)(\n",
        "    mc_nextobs_test, mc_nextpols_test), axis=0)                                 # calculate expectation\n",
        "print(\"expectation\", expect_test)\n",
        "mean_loss, mean_accuracy, min_accuracy, mean_acc_kfoc, min_acc_kfoc = env_test.loss(\n",
        "    obs_test, expect_test, policy_test)                                         # calculate loss\n",
        "print(\"test that StSt. policies give 0 loss in StSt. obs: \\n\",\n",
        "      \", Mean_loss:\", mean_loss,\n",
        "      \", Mean_accuracy:\", mean_accuracy,\n",
        "      \", Min_accuracy:\", min_accuracy)\n",
        "\n",
        "# DELETE VARIABLES\n",
        "del env_test\n",
        "del rng_test\n",
        "del obs_ss_norm\n",
        "del obs_test\n",
        "del next_obs_test\n",
        "del nn_test\n",
        "del nn_policy_test\n",
        "del params_test\n",
        "del policy_test\n",
        "del expect_test\n",
        "del mc_shocks_test\n",
        "del mc_nextobs_test\n",
        "del mc_nextpols_test\n",
        "del mean_loss\n",
        "del mean_accuracy\n",
        "del min_accuracy\n",
        "del mean_acc_kfoc\n",
        "del min_acc_kfoc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf6Ond64MCsp"
      },
      "source": [
        "# Create Simulation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K6CZbSNMIWy"
      },
      "outputs": [],
      "source": [
        "def create_episode_simul_fn(env, config):\n",
        "\n",
        "  if config[\"proxy_sampler\"]:\n",
        "    def sample_epis_obs(train_state, epis_rng):\n",
        "      \"sample obs of an episode\"\n",
        "      init_obs = env.initial_obs(epis_rng, config[\"init_range\"])\n",
        "      period_rngs = random.split(epis_rng, config[\"periods_per_epis\"])\n",
        "      def period_step(env_obs, period_rng):\n",
        "        period_shock = config[\"simul_vol_scale\"]*env.sample_shock(period_rng)     # Sample next obs\n",
        "        obs_next = env.step_loglinear(env_obs, period_shock)                      # apply period steps.\n",
        "        return obs_next, obs_next # we pass it two times because of the syntax of the lax.scan loop\n",
        "      _, epis_obs = lax.scan(period_step, init_obs, jnp.stack(period_rngs)) # we get the obs_batch\n",
        "      return epis_obs\n",
        "\n",
        "  else:\n",
        "    def sample_epis_obs(train_state, epis_rng):\n",
        "      \"sample obs of an episode\"\n",
        "      init_obs = env.initial_obs(epis_rng, config[\"init_range\"])\n",
        "      period_rngs = random.split(epis_rng, config[\"periods_per_epis\"])\n",
        "      def period_step(env_obs, period_rng):\n",
        "        policy = train_state.apply_fn(train_state.params, env_obs)\n",
        "        period_shock = config[\"simul_vol_scale\"]*env.sample_shock(period_rng)     # Sample next obs\n",
        "        obs_next = env.step(env_obs, policy, period_shock)  # apply period steps.\n",
        "        return obs_next, obs_next # we pass it two times because of the syntax of the lax.scan loop\n",
        "      _, epis_obs = lax.scan(period_step, init_obs, jnp.stack(period_rngs)) # we get the obs_batch\n",
        "      return epis_obs\n",
        "\n",
        "  return sample_epis_obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cygQcsHYIiDN"
      },
      "source": [
        "## Test simulation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwozvI7BImR3"
      },
      "outputs": [],
      "source": [
        "#CREATE ENV,  TRAIN_STATE AND RNG\n",
        "env_test = Model()\n",
        "nn_test = NeuralNet([2,2] + [env_test.n_actions])\n",
        "rng_test = random.PRNGKey(1)\n",
        "\n",
        "# CREATE CONFIG\n",
        "config_test = {\n",
        "    \"periods_per_epis\": 100,      # periods per episode\n",
        "    \"simul_vol_scale\": 1,        # scale of volatility while simul\n",
        "    \"proxy_sampler\": True,\n",
        "    \"init_range\": 5,\n",
        "}\n",
        "\n",
        "# GET FUNCTIONS\n",
        "episode_simul_fn = create_episode_simul_fn(env_test, config_test)\n",
        "train_state_test = TrainState.create(apply_fn=nn_test.apply, params=nn_test.init(rng_test, env_test.initial_obs(rng_test)), tx=optax.adam(0.05))\n",
        "epis_rng, loss_rng = random.split(rng_test, 2)\n",
        "epis_obs = episode_simul_fn(train_state_test, epis_rng)\n",
        "print(\"last observation of simulation: \\n\", epis_obs[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6qLf-4qM4GN"
      },
      "source": [
        "# Create Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcoKxR5xM8CL"
      },
      "outputs": [],
      "source": [
        "def create_batch_loss_fn(env, config):\n",
        "\n",
        "  if config[\"proxy_mcsampler\"] and config[\"proxy_futurepol\"]:\n",
        "    def batch_loss_fn(params, train_state, batch_obs, loss_rng):\n",
        "      \"\"\"Loss function of a batch of obs.\"\"\"\n",
        "      period_mc_rngs = random.split(loss_rng, batch_obs.shape[0])\n",
        "      batch_policies = train_state.apply_fn(params, batch_obs) # get the policies for the entire obs batch.\n",
        "      # batch_policies = jax.vmap(env.policy_loglinear)(batch_obs) # get the policies for the entire obs batch.\n",
        "      def period_loss(obs, policy, period_mc_rng):\n",
        "        \"\"\"Loss function for an individual period.\"\"\"\n",
        "        mc_shocks = env.mc_shocks(period_mc_rng, config[\"mc_draws\"])\n",
        "        mc_nextobs = jax.vmap(env.step_loglinear, in_axes = (None,0))(obs, mc_shocks)\n",
        "        # print(\"shape of mc_nextobs\")\n",
        "        mc_nextpols = jax.vmap(env.policy_loglinear)(mc_nextobs)\n",
        "        # print(\"shape of mc_nexpols\", )\n",
        "        expect = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols), axis=0)\n",
        "        mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc = env.loss(obs, expect, policy) # calculate loss\n",
        "        return mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc\n",
        "\n",
        "      # parallelize callculation of period_loss for the entire batch\n",
        "      mean_losses, mean_accuracies, min_accuracies, mean_accs_foc, min_accs_foc = jax.vmap(period_loss)(batch_obs, batch_policies, jnp.stack(period_mc_rngs))\n",
        "      mean_loss = jnp.mean(mean_losses)                   # average accross periods\n",
        "      mean_accuracy = jnp.mean(mean_accuracies)           # average accross periods\n",
        "      min_accuracy = jnp.min(min_accuracies)              # min accross periods and across eqs within period\n",
        "      mean_accs_foc = jnp.mean(mean_accs_foc,axis=0)\n",
        "      min_accs_foc = jnp.min(min_accs_foc,axis=0)\n",
        "      metrics = mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc # pass as auxiliary info\n",
        "      # metrics = jnp.array([mean_losses, mean_accuracies, min_accuracies]) # pass as auxiliary info\n",
        "      return mean_loss, metrics\n",
        "\n",
        "  elif config[\"proxy_mcsampler\"] and not config[\"proxy_futurepol\"]:\n",
        "    def batch_loss_fn(params, train_state, batch_obs, loss_rng):\n",
        "      \"\"\"Loss function of a batch of obs.\"\"\"\n",
        "      period_mc_rngs = random.split(loss_rng, batch_obs.shape[0])\n",
        "      batch_policies = train_state.apply_fn(params, batch_obs) # get the policies for the entire obs batch.\n",
        "\n",
        "      def period_loss(obs, policy, period_mc_rng):\n",
        "        \"\"\"Loss function for an individual period.\"\"\"\n",
        "        mc_shocks = env.mc_shocks(period_mc_rng, config[\"mc_draws\"])\n",
        "        mc_nextobs = jax.vmap(env.step_loglinear, in_axes = (None,0))(obs, mc_shocks)\n",
        "        mc_nextpols = train_state.apply_fn(params, mc_nextobs)\n",
        "        expect = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols), axis=0)\n",
        "        mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc = env.loss(obs, expect, policy) # calculate loss\n",
        "        return mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc\n",
        "\n",
        "      # parallelize callculation of period_loss for the entire batch\n",
        "      mean_losses, mean_accuracies, min_accuracies, mean_accs_foc, min_accs_foc = jax.vmap(period_loss)(batch_obs, batch_policies, jnp.stack(period_mc_rngs))\n",
        "      mean_loss = jnp.mean(mean_losses)                   # average accross periods\n",
        "      mean_accuracy = jnp.mean(mean_accuracies)           # average accross periods\n",
        "      min_accuracy = jnp.min(min_accuracies)              # min accross periods and across eqs within period\n",
        "      mean_accs_foc = jnp.mean(mean_accs_foc,axis=0)\n",
        "      min_accs_foc = jnp.min(min_accs_foc,axis=0)\n",
        "      metrics = mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc # pass as auxiliary info\n",
        "      return mean_loss, metrics\n",
        "\n",
        "  elif not config[\"proxy_mcsampler\"] and config[\"proxy_futurepol\"]:\n",
        "    def batch_loss_fn(params, train_state, batch_obs, loss_rng):\n",
        "      \"\"\"Loss function of a batch of obs.\"\"\"\n",
        "      period_mc_rngs = random.split(loss_rng, batch_obs.shape[0])\n",
        "      batch_policies = train_state.apply_fn(params, batch_obs) # get the policies for the entire obs batch.\n",
        "\n",
        "      def period_loss(obs, policy, period_mc_rng):\n",
        "        \"\"\"Loss function for an individual period.\"\"\"\n",
        "        mc_shocks = env.mc_shocks(period_mc_rng, config[\"mc_draws\"])\n",
        "        mc_nextobs = jax.vmap(env.step, in_axes = (None,None,0))(obs, policy, mc_shocks)\n",
        "        mc_nextpols = jax.vmap(env.policy_loglinear)(mc_nextobs)\n",
        "        expect = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols), axis=0)\n",
        "        mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc = env.loss(obs, expect, policy) # calculate loss\n",
        "        return mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc\n",
        "\n",
        "     # parallelize callculation of period_loss for the entire batch\n",
        "      mean_losses, mean_accuracies, min_accuracies, mean_accs_foc, min_accs_foc = jax.vmap(period_loss)(batch_obs, batch_policies, jnp.stack(period_mc_rngs))\n",
        "      mean_loss = jnp.mean(mean_losses)                   # average accross periods\n",
        "      mean_accuracy = jnp.mean(mean_accuracies)           # average accross periods\n",
        "      min_accuracy = jnp.min(min_accuracies)              # min accross periods and across eqs within period\n",
        "      mean_accs_foc = jnp.mean(mean_accs_foc,axis=0)\n",
        "      min_accs_foc = jnp.min(min_accs_foc,axis=0)\n",
        "      metrics = mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc # pass as auxiliary info\n",
        "      return mean_loss, metrics\n",
        "  else:\n",
        "    def batch_loss_fn(params, train_state, batch_obs, loss_rng):\n",
        "      \"\"\"Loss function of a batch of obs.\"\"\"\n",
        "      period_mc_rngs = random.split(loss_rng, batch_obs.shape[0])\n",
        "      batch_policies = train_state.apply_fn(params, batch_obs) # get the policies for the entire obs batch.\n",
        "\n",
        "      def period_loss(obs, policy, period_mc_rng):\n",
        "        \"\"\"Loss function for an individual period.\"\"\"\n",
        "        mc_shocks = env.mc_shocks(period_mc_rng, config[\"mc_draws\"])\n",
        "        mc_nextobs = jax.vmap(env.step, in_axes = (None,None,0))(obs, policy, mc_shocks)\n",
        "        mc_nextpols = train_state.apply_fn(params, mc_nextobs)\n",
        "        expect = jnp.mean(jax.vmap(env.expect_realization)(mc_nextobs, mc_nextpols), axis=0)\n",
        "        mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc = env.loss(obs, expect, policy) # calculate loss\n",
        "        return mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc\n",
        "\n",
        "      # parallelize callculation of period_loss for the entire batch\n",
        "      mean_losses, mean_accuracies, min_accuracies, mean_accs_foc, min_accs_foc = jax.vmap(period_loss)(batch_obs, batch_policies, jnp.stack(period_mc_rngs))\n",
        "      mean_loss = jnp.mean(mean_losses)                   # average accross periods\n",
        "      mean_accuracy = jnp.mean(mean_accuracies)           # average accross periods\n",
        "      min_accuracy = jnp.min(min_accuracies)              # min accross periods and across eqs within period\n",
        "      mean_accs_foc = jnp.mean(mean_accs_foc,axis=0)\n",
        "      min_accs_foc = jnp.min(min_accs_foc,axis=0)\n",
        "      metrics = mean_loss, mean_accuracy, min_accuracy, mean_accs_foc, min_accs_foc # pass as auxiliary info\n",
        "      return mean_loss, metrics\n",
        "\n",
        "  return batch_loss_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pceB-eSEbcG"
      },
      "source": [
        "## Test loss fn\n",
        "\n",
        "We calculate the loss and metrics of a single epsiode. To do so, we set batch_size equal to periods_per_epis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MLBUEcHEgdY"
      },
      "outputs": [],
      "source": [
        "#CREATE ENV,  TRAIN_STATE AND RNG\n",
        "env_test = Model()\n",
        "nn_test = NeuralNet([32,32] + [env_test.n_actions])\n",
        "rng_test = random.PRNGKey(1)\n",
        "\n",
        "# CREATE CONFIG\n",
        "config_test = {\n",
        "    \"periods_per_epis\": 8,      # periods per episode\n",
        "    \"simul_vol_scale\": 1,        # scale of volatility while simul\n",
        "    \"mc_draws\": 32,             # only applies if shock is continuous\n",
        "    \"proxy_sampler\": True,\n",
        "    \"proxy_mcsampler\": True,\n",
        "    \"proxy_futurepol\": True,\n",
        "    \"init_range\": 5,\n",
        "}\n",
        "config_test[\"batch_size\"] = config_test[\"periods_per_epis\"]\n",
        "\n",
        "# GET FUNCTIONS\n",
        "episode_simul_fn = create_episode_simul_fn(env_test, config_test)\n",
        "episode_loss_fn = create_batch_loss_fn(env_test, config_test)\n",
        "\n",
        "train_state_test = TrainState.create(apply_fn=nn_test.apply, params=nn_test.init(rng_test, env_test.initial_obs(rng_test)), tx=optax.adam(0.05))\n",
        "epis_rng, loss_rng = random.split(rng_test, 2)\n",
        "epis_obs = episode_simul_fn(train_state_test, epis_rng)\n",
        "\n",
        "loss, epis_metrics = episode_loss_fn(train_state_test.params, train_state_test, epis_obs, loss_rng)\n",
        "print(\"loss:\", loss)\n",
        "print(\"epis_metrics:\", epis_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjl96ePdNvjb"
      },
      "source": [
        "# Create Epoch Training function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lacb6dyKN7hi"
      },
      "outputs": [],
      "source": [
        "def get_epoch_train_fn(env, config):\n",
        "  episode_simul_fn = create_episode_simul_fn(env, config)\n",
        "  batch_loss_fn = create_batch_loss_fn(env, config)\n",
        "\n",
        "  def batch_train_fn(train_state, batch_obs, loss_rng):\n",
        "    grad_fn = jax.value_and_grad(batch_loss_fn, has_aux=True)\n",
        "    (_, batch_metrics), grads = grad_fn(train_state.params, train_state, batch_obs, loss_rng)\n",
        "    grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
        "    train_state = train_state.apply_gradients(grads=grads)\n",
        "    return train_state, batch_metrics\n",
        "\n",
        "  def step_train_fn(train_state, step_rng):\n",
        "    epis_rng = random.split(step_rng, config[\"epis_per_step\"])\n",
        "    loss_rng = random.split(step_rng, config[\"n_batches\"])\n",
        "    step_obs = jax.vmap(episode_simul_fn, in_axes=(None,0))(train_state, jnp.stack(epis_rng))\n",
        "    step_obs = step_obs.reshape(config[\"periods_per_step\"], env.state_ss.shape[0]) # combine all periods in one axis\n",
        "    step_obs = random.permutation(step_rng, step_obs, axis=0)                   # reshuffle obs at random\n",
        "    step_obs = step_obs.reshape(config[\"n_batches\"], config[\"batch_size\"] ,env.state_ss.shape[0]) # reshape to into batches\n",
        "    train_state, step_metrics = jax.vmap(batch_train_fn, in_axes=(None,0,0), out_axes=(None,0), axis_name=\"batch\")(train_state, step_obs, jnp.stack(loss_rng))\n",
        "    losses, mean_accuracies, min_accuracies,_, _ = step_metrics\n",
        "    loss = jnp.mean(losses)\n",
        "    mean_accuracy = jnp.mean(mean_accuracies)\n",
        "    min_accuracy = jnp.min(min_accuracies)\n",
        "    metrics = loss, mean_accuracy, min_accuracy\n",
        "    return train_state, metrics\n",
        "\n",
        "  def epoch_train_fn(train_state, epoch_rng):\n",
        "    \"\"\"Vectorise and repeat the update to complete an epoch, made aout of steps_per_epoch episodes.\"\"\"\n",
        "    epoch_rng, *step_rngs = random.split(epoch_rng, config[\"steps_per_epoch\"] + 1)\n",
        "    train_state, epoch_metrics = lax.scan(step_train_fn, train_state, jnp.stack(step_rngs))\n",
        "    return train_state, epoch_rng, epoch_metrics\n",
        "\n",
        "  return epoch_train_fn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYEia3OD-Lij"
      },
      "source": [
        "## Test the training function\n",
        "\n",
        "We can run one epoch and see the results. Play with the parameters of the epoch to evaluate how good is the starting point. You can also add prints inside the update function to check internal values. An important check is to print the grads inside the epis_update_fn and make sure they are not zero for an entire layer. This is especially relevant when using pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hicUBLteIjCP"
      },
      "outputs": [],
      "source": [
        "config_test = {\n",
        "    \"periods_per_epis\": 2,      # periods per episode\n",
        "    \"epis_per_step\": 2,         # epoch per steps\n",
        "    \"steps_per_epoch\": 2,       # steps per epoch\n",
        "    \"batch_size\": 2,\n",
        "    \"n_epochs\": 1,              # number of epochs\n",
        "    \"mc_draws\": 2,             # only applies if shock is continuous\n",
        "    \"simul_vol_scale\": 1,        # scale of volatility while simul\n",
        "    \"proxy_sampler\": True,\n",
        "    \"proxy_mcsampler\": True,\n",
        "    \"proxy_futurepol\": True,\n",
        "    \"config_eval\": {\n",
        "      \"periods_per_epis\": 8,      # periods per episode\n",
        "      \"mc_draws\": 8,         # number of mc draws\n",
        "      \"simul_vol_scale\": 1,        # scale of volatility while simul\n",
        "      \"eval_n_epis\": 8,           # episodes to sample for eval\n",
        "      \"proxy_sampler\": True,\n",
        "      \"proxy_mcsampler\": True,\n",
        "      \"proxy_futurepol\": True,\n",
        "    }\n",
        "}\n",
        "\n",
        "config_test[\"periods_per_step\"] =config_test[\"periods_per_epis\"]*config_test[\"epis_per_step\"]\n",
        "config_test[\"n_batches\"] = config_test[\"periods_per_step\"]//config_test[\"batch_size\"]\n",
        "env_test = Model()\n",
        "epoch_update_test = get_epoch_train_fn(env_test, config_test)\n",
        "\n",
        "#CREATE TRAIN_STATE AND RNG\n",
        "nn_test = NeuralNet([32,32] + [env_test.n_actions])\n",
        "rng_test_init = random.PRNGKey(1)\n",
        "train_state_test = TrainState.create(apply_fn=nn_test.apply, params=nn_test.init(rng_test_init, env_test.initial_obs(rng_test_init)), tx=optax.adam(0.001))\n",
        "\n",
        "# RUN UPDATE FUNCTION\n",
        "new_train_state, new_rng_test, metrics_test = epoch_update_test(train_state_test, rng_test_init)\n",
        "print(metrics_test)\n",
        "print(len(metrics_test[0]))\n",
        "\n",
        "print(\"test epoch: \\n\",\n",
        "      \"Mean_loss:\", metrics_test[-1][0], \"\\n\",\n",
        "      \"Mean_accuracy:\", metrics_test[-1][1], \"\\n\",\n",
        "      \"Min_accuracy:\", metrics_test[-1][2],)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ5nxqwWwPTU"
      },
      "source": [
        "# Create Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt-0bBX9wUIe"
      },
      "outputs": [],
      "source": [
        "def get_eval_fn(env, config):\n",
        "  config = config[\"config_eval\"]\n",
        "  episode_simul_fn = create_episode_simul_fn(env, config)\n",
        "  batch_loss_fn = create_batch_loss_fn(env, config)\n",
        "\n",
        "  def episode_eval_fn(train_state, epis_rng):\n",
        "    epis_rng, loss_rng = random.split(epis_rng, 2)\n",
        "    epis_obs = episode_simul_fn(train_state, epis_rng)\n",
        "    _, epis_metrics = batch_loss_fn(train_state.params, train_state, epis_obs, loss_rng)\n",
        "    return epis_metrics\n",
        "\n",
        "  def eval_fn(train_state, step_rng):\n",
        "    epis_rng = random.split(step_rng, config[\"eval_n_epis\"])\n",
        "    losses, mean_accuracies, min_accuracies, mean_accs_focs, min_accs_focs = jax.vmap(episode_eval_fn, in_axes=(None,0))(train_state, jnp.stack(epis_rng))\n",
        "    loss = jnp.mean(losses)\n",
        "    mean_accuracy = jnp.mean(mean_accuracies)\n",
        "    min_accuracy = jnp.min(min_accuracies)\n",
        "    mean_accs_focs = jnp.mean(mean_accs_focs, axis=0)\n",
        "    min_accs_focs = jnp.min(min_accs_focs, axis=0)\n",
        "    return loss, mean_accuracy, min_accuracy, mean_accs_focs, min_accs_focs\n",
        "\n",
        "  return eval_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEG-xLNZQJLY"
      },
      "source": [
        "## Test Evaluation function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGKZvG6AQTnA"
      },
      "outputs": [],
      "source": [
        "config_test = {\n",
        "    \"periods_per_epis\": 2,      # periods per episode\n",
        "    \"epis_per_step\": 2,         # epoch per steps\n",
        "    \"steps_per_epoch\": 2,       # steps per epoch\n",
        "    \"batch_size\": 2,\n",
        "    \"n_epochs\": 1,              # number of epochs\n",
        "    \"mc_draws\": 2,             # only applies if shock is continuous\n",
        "    \"simul_vol_scale\": 1,        # scale of volatility while simul\n",
        "    \"proxy_sampler\": True,\n",
        "    \"proxy_mcsampler\": True,\n",
        "    \"proxy_futurepol\": True,\n",
        "    \"config_eval\": {\n",
        "      \"periods_per_epis\": 8,      # periods per episode\n",
        "      \"mc_draws\": 8,         # number of mc draws\n",
        "      \"simul_vol_scale\": 1,        # scale of volatility while simul\n",
        "      \"eval_n_epis\": 8,           # episodes to sample for eval\n",
        "      \"proxy_sampler\": True,\n",
        "      \"proxy_mcsampler\": True,\n",
        "      \"proxy_futurepol\": True,\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "config_test[\"periods_per_step\"] =config_test[\"periods_per_epis\"]*config_test[\"epis_per_step\"]\n",
        "config_test[\"n_batches\"] = config_test[\"periods_per_step\"]//config_test[\"batch_size\"]\n",
        "env_test = Model()\n",
        "epoch_train_fn_test = get_epoch_train_fn(env_test, config_test)\n",
        "eval_fn_test = get_eval_fn(env_test, config_test)\n",
        "\n",
        "#CREATE TRAIN_STATE AND RNG\n",
        "nn_test = NeuralNet([2,2] + [env_test.n_actions])\n",
        "rng_test_init = random.PRNGKey(1)\n",
        "train_state_test = TrainState.create(apply_fn=nn_test.apply, params=nn_test.init(rng_test_init, env_test.initial_obs(rng_test_init)), tx=optax.adam(0.05))\n",
        "\n",
        "# RUN UPDATE FUNCTION\n",
        "new_train_state, new_rng_test, train_metrics_test = epoch_train_fn_test(train_state_test, rng_test_init)\n",
        "print(\"Training Metrics:\", train_metrics_test)\n",
        "eval_metrics_test = eval_fn_test(new_train_state, rng_test_init)\n",
        "print(\"Evaluation Metrics:\", eval_metrics_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlXarixBjTC2"
      },
      "source": [
        "# Configure experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zWgbr0HjQua",
        "outputId": "9f7face3-610f-4c15-a333-9b421c1f636f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters:\n",
            "\n",
            "\u001b[3m                               NeuralNet Summary                                \u001b[0m\n",
            "┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams              \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│         │ NeuralNet │ \u001b[2mfloat64\u001b[0m[74]    │ \u001b[2mfloat64\u001b[0m[412]   │                      │\n",
            "├─────────┼───────────┼────────────────┼────────────────┼──────────────────────┤\n",
            "│ Dense_0 │ Dense     │ \u001b[2mfloat64\u001b[0m[1,74]  │ \u001b[2mfloat64\u001b[0m[1,512] │ bias: \u001b[2mfloat64\u001b[0m[512]   │\n",
            "│         │           │                │                │ kernel:              │\n",
            "│         │           │                │                │ \u001b[2mfloat64\u001b[0m[74,512]      │\n",
            "│         │           │                │                │                      │\n",
            "│         │           │                │                │ \u001b[1m38,400 \u001b[0m\u001b[1;2m(307.2 KB)\u001b[0m    │\n",
            "├─────────┼───────────┼────────────────┼────────────────┼──────────────────────┤\n",
            "│ Dense_1 │ Dense     │ \u001b[2mfloat64\u001b[0m[1,512] │ \u001b[2mfloat64\u001b[0m[1,512] │ bias: \u001b[2mfloat64\u001b[0m[512]   │\n",
            "│         │           │                │                │ kernel:              │\n",
            "│         │           │                │                │ \u001b[2mfloat64\u001b[0m[512,512]     │\n",
            "│         │           │                │                │                      │\n",
            "│         │           │                │                │ \u001b[1m262,656 \u001b[0m\u001b[1;2m(2.1 MB)\u001b[0m     │\n",
            "├─────────┼───────────┼────────────────┼────────────────┼──────────────────────┤\n",
            "│ Dense_2 │ Dense     │ \u001b[2mfloat64\u001b[0m[1,512] │ \u001b[2mfloat64\u001b[0m[1,412] │ bias: \u001b[2mfloat64\u001b[0m[412]   │\n",
            "│         │           │                │                │ kernel:              │\n",
            "│         │           │                │                │ \u001b[2mfloat64\u001b[0m[512,412]     │\n",
            "│         │           │                │                │                      │\n",
            "│         │           │                │                │ \u001b[1m211,356 \u001b[0m\u001b[1;2m(1.7 MB)\u001b[0m     │\n",
            "├─────────┼───────────┼────────────────┼────────────────┼──────────────────────┤\n",
            "│ Dense_3 │ Dense     │ \u001b[2mfloat64\u001b[0m[1,412] │ \u001b[2mfloat64\u001b[0m[1,412] │ bias: \u001b[2mfloat64\u001b[0m[412]   │\n",
            "│         │           │                │                │ kernel:              │\n",
            "│         │           │                │                │ \u001b[2mfloat64\u001b[0m[412,412]     │\n",
            "│         │           │                │                │                      │\n",
            "│         │           │                │                │ \u001b[1m170,156 \u001b[0m\u001b[1;2m(1.4 MB)\u001b[0m     │\n",
            "├─────────┼───────────┼────────────────┼────────────────┼──────────────────────┤\n",
            "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m              \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m         Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m682,568 \u001b[0m\u001b[1;2m(5.5 MB)\u001b[0m\u001b[1m    \u001b[0m\u001b[1m \u001b[0m│\n",
            "└─────────┴───────────┴────────────────┴────────────────┴──────────────────────┘\n",
            "\u001b[1m                                                                                \u001b[0m\n",
            "\u001b[1m                       Total Parameters: 682,568 \u001b[0m\u001b[1;2m(5.5 MB)\u001b[0m\u001b[1m                       \u001b[0m\n",
            "\n",
            "\n",
            "Number of steps (NN updates): \n",
            " 50000 episodes \n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''Config dictionary'''\n",
        "\n",
        "# CREATE CONFIG DICT\n",
        "config = {\n",
        "    # general\n",
        "    \"model_name\": model_name,\n",
        "    \"exper_name\": \"baseline_nostateaug_finetune\",\n",
        "    \"date\": \"Sep18_2025\",\n",
        "    \"save_dir\": working_dir + \"Experiments/\",\n",
        "    \"restore\": True,                                                            # True if start from restored checkpoint\n",
        "    \"restore_exper_name\": \"baseline_nostateaug\",\n",
        "    \"seed\": 4,\n",
        "    # defining who drive simuls and future policies\n",
        "    \"proxy_sampler\": False,    # use a proxy (e.g. loglinear policy) to control simulation\n",
        "    \"proxy_mcsampler\": False, # use a proxy to control monte-carlo sampling\n",
        "    \"proxy_futurepol\": False,  # use a proxy to control future policy inside monte-carlo sampling\n",
        "    \"comment\": \"This is an experiment where we use the original state.\",\n",
        "    \"comment_at_end\": False,\n",
        "\n",
        "    # neural net\n",
        "    \"layers\": [512,512],              # layers of the NN\n",
        "    # \"layers\": [6*37,9*32,11*37+5],              # layers of the NN\n",
        "    # \"layers\": [512,512],              # layers of the NN\n",
        "\n",
        "    # learning rate schedule\n",
        "    \"lr_sch_values\": [0.00001,0.00001],                                        # values (from the last, we do cosine decay to 0)\n",
        "    \"lr_sch_transitions\": [1000],\n",
        "    \"warmup_steps\": 0,\n",
        "    \"lr_end_value\": 0.0000001,\n",
        "\n",
        "    # simulation\n",
        "    \"periods_per_epis\": 128,\n",
        "    \"simul_vol_scale\": 1,        # scale of volatility while simul\n",
        "    \"init_range\": 2,\n",
        "\n",
        "    # loss calculation\n",
        "    \"mc_draws\": 256,               # monte-carlo draws\n",
        "\n",
        "    # training\n",
        "    \"epis_per_step\": 64,         # epoch per steps\n",
        "    \"steps_per_epoch\": 100,       # steps per epoch\n",
        "    \"n_epochs\": 100,               # number of epochs\n",
        "    \"batch_size\": 16,             # size of batch of obs to calculate grad\n",
        "\n",
        "\n",
        "    \"config_eval\": {\n",
        "      \"periods_per_epis\": 64,      # periods per episode\n",
        "      \"mc_draws\": 128,         # number of mc draws\n",
        "      \"simul_vol_scale\": 1,        # scale of volatility while simul\n",
        "      \"eval_n_epis\": 64,           # episodes to sample for eval\n",
        "      \"init_range\": 0,      # range (in pp) around steady state for intialization\n",
        "      \"proxy_sampler\": False,    # use a proxy (e.g. loglinear policy) to control simulation\n",
        "      \"proxy_mcsampler\": False, # use a proxy to control monte-carlo sampling\n",
        "      \"proxy_futurepol\": False,  # use a proxy to control future policy inside monte-carlo sampling\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "#create auxiliary config variables for readability\n",
        "config[\"periods_per_step\"] =config[\"periods_per_epis\"]*config[\"epis_per_step\"]\n",
        "config[\"n_batches\"] = config[\"periods_per_step\"]//config[\"batch_size\"]\n",
        "\n",
        "# PRINT AND PLOT KEY CONFIGS\n",
        "print(\"Number of parameters:\")\n",
        "print(NeuralNet(config[\"layers\"] + [Model().n_actions],C,policies_sd).tabulate(\n",
        "    random.PRNGKey(0),\n",
        "    Model().initial_obs(random.PRNGKey(0))\n",
        "    ))\n",
        "\n",
        "print(\"Number of steps (NN updates): \\n\", config[\"steps_per_epoch\"]*config[\"n_epochs\"], \"episodes \\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEAiCNRxC1AJ"
      },
      "source": [
        "# Create experiment\n",
        "Now we the entire experiment workflow as a function to call later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw69_Ic-dcVr"
      },
      "outputs": [],
      "source": [
        "def run_experiment(env, config):\n",
        "  \"\"\"Runs experiment.\"\"\"\n",
        "\n",
        "  n_cores = len(jax.devices())\n",
        "\n",
        "  # CREATE NN, RNGS, TRAIN_STATE AND EPOQUE UPDATE\n",
        "  neural_net = NeuralNet(config[\"layers\"] + [env.n_actions],C,policies_sd)\n",
        "  rng, rng_pol, rng_env, rng_epoch, rng_eval = random.split(random.PRNGKey(config[\"seed\"]), num=5)  # random number generator\n",
        "\n",
        "  # CREATE LR SCHEDULE\n",
        "  lr_schedule = optax.join_schedules(\n",
        "    schedules= [optax.constant_schedule(i) for i in config[\"lr_sch_values\"][:-1]]\n",
        "                  + [optax.warmup_cosine_decay_schedule(\n",
        "                    init_value=config[\"lr_sch_values\"][-1],\n",
        "                    peak_value=config[\"lr_sch_values\"][-1],\n",
        "                    warmup_steps=config[\"warmup_steps\"],\n",
        "                    decay_steps=config[\"n_epochs\"]*config[\"steps_per_epoch\"]-config[\"lr_sch_transitions\"][-1],\n",
        "                    end_value=config[\"lr_end_value\"],)],\n",
        "      boundaries=config[\"lr_sch_transitions\"] # the number of episodes at which to switch\n",
        "      )\n",
        "\n",
        "  # lr_schedule = optax.join_schedules(\n",
        "  #   schedules= [optax.constant_schedule(i) for i in config[\"lr_sch_values\"][:-1]]\n",
        "  #                 + [optax.warmup_cosine_decay_schedule(\n",
        "  #                   init_value=config[\"lr_sch_values\"][-1],\n",
        "  #                   peak_value=config[\"lr_sch_values\"][-1],\n",
        "  #                   warmup_steps=0,\n",
        "  #                   decay_steps=config[\"n_epochs\"]*config[\"steps_per_epoch\"]-config[\"lr_sch_transitions\"][-1],\n",
        "  #                   end_value=0.00000005,)],\n",
        "  #     boundaries=config[\"lr_sch_transitions\"] # the number of episodes at which to switch\n",
        "  #     )\n",
        "\n",
        "  # INITIALIZE OR RESTORE FULL NN TRAIN STATE\n",
        "  if not config[\"restore\"]:\n",
        "    params=neural_net.init(rng_pol, jnp.zeros_like(env.initial_obs(rng_env)))\n",
        "    train_state = TrainState.create(apply_fn=neural_net.apply, params=params, tx=optax.adam(lr_schedule))\n",
        "\n",
        "  else:\n",
        "    train_state_restored = checkpoints.restore_checkpoint(ckpt_dir=config[\"save_dir\"]+config[\"restore_exper_name\"], target = None)\n",
        "    params = train_state_restored[\"params\"]\n",
        "    opt_state = train_state_restored[\"opt_state\"]\n",
        "    train_state = TrainState.create(apply_fn=neural_net.apply, params=params, tx=optax.adam(lr_schedule))\n",
        "    train_state.replace(opt_state=opt_state)\n",
        "\n",
        "  # GET TRAIN AND EVAL FUNCTIONS\n",
        "  train_epoch_fn  = jax.jit(get_epoch_train_fn(env, config))\n",
        "  eval_fn  = jax.jit(get_eval_fn(env, config))\n",
        "\n",
        "  # COMPILE CODE\n",
        "  time_start = time()\n",
        "  train_epoch_fn(train_state, rng_epoch)  # compiles\n",
        "  eval_fn(train_state, rng_epoch) # compiles\n",
        "  time_compilation = time() - time_start\n",
        "  print(\"Time Elapsed for Compilation:\", time_compilation, \"seconds\")\n",
        "\n",
        "  # RUN AN EPOCH TO GET TIME STATS\n",
        "  time_start = time()\n",
        "  train_epoch_fn(train_state, rng_epoch) # run one epoque\n",
        "  time_epoch = time() - time_start\n",
        "  print(\"Time Elapsed for epoch:\", time_epoch, \"seconds\")\n",
        "\n",
        "  time_start = time()\n",
        "  eval_fn(train_state, rng_epoch) # run one epoque\n",
        "  time_eval = time() - time_start\n",
        "  print(\"Time Elapsed for eval:\", time_eval, \"seconds\")\n",
        "\n",
        "  time_experiment = (time_epoch + time_eval)*config[\"n_epochs\"]/60\n",
        "  print(\"Estimated time for full experiment\", time_experiment, \"minutes\")\n",
        "\n",
        "  steps_per_second = config[\"steps_per_epoch\"]*config[\"periods_per_step\"]/time_epoch\n",
        "  print(\"Steps per second:\", steps_per_second, \"st/s\")\n",
        "\n",
        "  # CREATE LISTS TO STORE METRICS\n",
        "  mean_losses, mean_accuracy, min_accuracy = [], [], []\n",
        "\n",
        "  # RUN ALL THE EPOCHS\n",
        "  time_start = time()\n",
        "  for i in range(1,config[\"n_epochs\"]+1):\n",
        "\n",
        "    # eval\n",
        "    eval_metrics = eval_fn(train_state, rng_eval)\n",
        "    print('EVALUATION:\\n',\n",
        "      'Iteration:', train_state.step,\n",
        "      \"Mean_loss:\", eval_metrics[0],\n",
        "      \", Mean Acc:\", eval_metrics[1],\n",
        "      \", Min Acc:\", eval_metrics[2], \"\\n\"\n",
        "      \", Mean Accs Foc\", eval_metrics[3], \"\\n\"\n",
        "      \", Min Accs Foc:\", eval_metrics[4],\n",
        "      \"\\n\")\n",
        "\n",
        "    # run epoch\n",
        "    train_state, rng_epoch, epoch_metrics = train_epoch_fn(train_state, rng_epoch)\n",
        "    print('TRAINING:\\n',\n",
        "          'Iteration:', train_state.step,\n",
        "          \", Mean_loss:\", jnp.mean(epoch_metrics[0]),\n",
        "          \", Mean_accuracy:\", jnp.mean(epoch_metrics[1]),\n",
        "          \", Min_accuracy:\", jnp.min(epoch_metrics[2]),\n",
        "          # \", Mean_acc_Kfoc:\", jnp.mean(epoch_metrics[:,3]),\n",
        "          # \", Min_acc_Kfoc:\", jnp.min(epoch_metrics[:,4]),\n",
        "          \", Learning rate:\", lr_schedule(train_state.step),\n",
        "          \"\\n\"\n",
        "          )\n",
        "\n",
        "    # checkpoint\n",
        "    if train_state.step>=1000 and train_state.step%1000==0:\n",
        "      checkpoints.save_checkpoint(ckpt_dir=config['save_dir']+config['exper_name'], target=train_state, step=train_state.step)\n",
        "\n",
        "      # store results\n",
        "      mean_losses.append(float(eval_metrics[0]))\n",
        "      mean_accuracy.append(float(eval_metrics[1]))\n",
        "      min_accuracy.append(float(eval_metrics[2]))\n",
        "\n",
        "    #end of inner loop\n",
        "\n",
        "\n",
        "  # PRINT RESULTS\n",
        "  print(\"Minimum loss attained in evaluation:\", min(mean_losses))\n",
        "  print(\"Maximum mean accuracy attained in evaluation:\", max(mean_accuracy))\n",
        "  print(\"Maximum min accuracy attained in evaluation:\", max(min_accuracy))\n",
        "  time_fullexp = (time() - time_start)/60\n",
        "  print(\"Time Elapsed for Full Experiment:\", time_fullexp, \"minutes\")\n",
        "\n",
        "  # ASK FOR A COMMENT:\n",
        "  # comment = input(\"Enter a comment for the researcher: \")\n",
        "  if config[\"comment_at_end\"]:\n",
        "    comment_result = input(\"Enter a comment for the researcher: \")\n",
        "  else:\n",
        "    comment_result = \"\"\n",
        "\n",
        "  # STORE RESULTS\n",
        "  results = {\n",
        "    \"exper_name\": config[\"exper_name\"],\n",
        "    \"comment_preexp\": config[\"comment\"],\n",
        "    \"comment_result\": comment_result,\n",
        "    \"min_loss\":  min(mean_losses),\n",
        "    \"max_mean_acc\": max(mean_accuracy),\n",
        "    \"max_min_acc\": max(min_accuracy),\n",
        "    \"Time for Full Experiment (m)\": time_fullexp,\n",
        "    \"Time for epoch (s)\": time_epoch,\n",
        "    \"Time for Compilation (s)\": time_compilation,\n",
        "    \"Steps per second\": steps_per_second,\n",
        "    \"config\": config,\n",
        "    \"Losses_list\": mean_losses,\n",
        "    \"mean_accuracy_list\": mean_accuracy,\n",
        "    \"min_accuracy_list\": min_accuracy,\n",
        "  }\n",
        "\n",
        "  # store to json\n",
        "  if not os.path.exists(config['save_dir']+config['exper_name']):\n",
        "    os.mkdir(config['save_dir']+config['exper_name'])\n",
        "  with open(config['save_dir']+config['exper_name']+\"/results.json\", \"w\") as write_file:\n",
        "    json.dump(results, write_file)\n",
        "\n",
        "  # store to experiments csv\n",
        "  csv_filename = os.path.join(working_dir, 'experiment_results.csv')\n",
        "  df = pd.read_csv(csv_filename)\n",
        "  df = pd.concat([df, pd.DataFrame([results])], ignore_index=True)\n",
        "  df.to_csv(csv_filename, index=False)\n",
        "\n",
        "  # PLOT LEARNING\n",
        "\n",
        "  # Mean Losses\n",
        "  plt.plot([(i+1)*1000 for i in range(len(mean_losses))], mean_losses)\n",
        "  plt.xlabel('Steps (NN updates)')\n",
        "  plt.ylabel('Mean Losses')\n",
        "  plt.savefig(config['save_dir']+config['exper_name']+'/mean_losses.jpg')\n",
        "  plt.close()\n",
        "\n",
        "  # Mean Accuracy\n",
        "  plt.plot([(i+1)*1000 for i in range(len(mean_accuracy))], mean_accuracy)\n",
        "  plt.xlabel('Steps (NN updates)')\n",
        "  plt.ylabel('Mean Accuracy (%)')\n",
        "  plt.savefig(config['save_dir']+config['exper_name']+'/mean_accuracy.jpg')\n",
        "  plt.close()\n",
        "\n",
        "  # Min Accuracy\n",
        "  plt.plot([(i+1)*1000 for i in range(len(min_accuracy))], min_accuracy)\n",
        "  plt.xlabel('Steps (NN updates)')\n",
        "  plt.ylabel('Minimum Accuracy (%)')\n",
        "  plt.savefig(config['save_dir']+config['exper_name']+'/min_accuracy.jpg')\n",
        "  plt.close()\n",
        "\n",
        "  # Learning rate schedule\n",
        "  plt.plot([(i+1)*1000 for i in range(len(mean_losses))], [lr_schedule((i+1)*1000) for i in range(len(mean_losses))])\n",
        "  plt.xlabel('Steps (NN updates)')\n",
        "  plt.ylabel('Learning Rate')\n",
        "  plt.savefig(config['save_dir']+config['exper_name']+'/learning_rate.jpg')\n",
        "  plt.close()\n",
        "\n",
        "  return train_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdkzmdmHk_Il"
      },
      "source": [
        "# Run experiment\n",
        "*Finally*, we run the experiment abd get the trained parameter plus useful info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "awmY1xXgDOcU",
        "outputId": "30303fb7-d71d-40df-c5f7-bd33affbcd45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1269: UserWarning: Sharding info not provided when restoring. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time Elapsed for Compilation: 96.08792734146118 seconds\n",
            "Time Elapsed for epoch: 30.74180006980896 seconds\n",
            "Time Elapsed for eval: 0.09713602066040039 seconds\n",
            "Estimated time for full experiment 51.398226817448936 minutes\n",
            "Steps per second: 26647.75641438523 st/s\n",
            "EVALUATION:\n",
            " Iteration: 0 Mean_loss: 9.225874777917741e-07 , Mean Acc: 0.9994172325290587 , Min Acc: 0.9683098374862518 \n",
            ", Mean Accs Foc [0.99978942 0.99936081 0.9991298  0.99942101 0.99977917 0.99988794\n",
            " 0.99897378 0.99947289 0.99979318 0.99948837 0.99849257 0.99889272\n",
            " 0.99906906 0.99980508 0.9994019  0.99994034] \n",
            ", Min Accs Foc: [0.99318577 0.99105117 0.9888942  0.98038409 0.9933519  0.99687089\n",
            " 0.96830984 0.98630983 0.99547693 0.97229663 0.98085633 0.99444021\n",
            " 0.99536036 0.99740343 0.9956177  0.9994219 ] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 100 , Mean_loss: 1.3965336315845857e-06 , Mean_accuracy: 0.9992718802467316 , Min_accuracy: 0.9389780938541722 , Learning rate: 1e-05 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 100 Mean_loss: 9.093505587935597e-07 , Mean Acc: 0.9994205735722527 , Min Acc: 0.9685143925966665 \n",
            ", Mean Accs Foc [0.99979173 0.99936512 0.99913643 0.99942889 0.99977443 0.99988618\n",
            " 0.99897715 0.99948488 0.99979174 0.99948487 0.99850488 0.99889032\n",
            " 0.99904468 0.99980724 0.99941948 0.99994184] \n",
            ", Min Accs Foc: [0.99323059 0.99111269 0.98862511 0.98070151 0.99425368 0.99666201\n",
            " 0.96851439 0.98573685 0.99542541 0.97232375 0.98125833 0.99452473\n",
            " 0.99521541 0.99733356 0.9954353  0.99941118] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 200 , Mean_loss: 9.952104684727232e-07 , Mean_accuracy: 0.9993947614768561 , Min_accuracy: 0.9272852724657602 , Learning rate: 1e-05 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 200 Mean_loss: 8.990663604774269e-07 , Mean Acc: 0.9994225264576768 , Min Acc: 0.9687388248780272 \n",
            ", Mean Accs Foc [0.99979236 0.99936891 0.99914165 0.99942691 0.9997736  0.99988736\n",
            " 0.99897978 0.99948741 0.99979142 0.99948229 0.998516   0.99889182\n",
            " 0.99905406 0.9998082  0.99942182 0.99994038] \n",
            ", Min Accs Foc: [0.99303405 0.99088192 0.98841063 0.98114548 0.99409095 0.99671498\n",
            " 0.96873882 0.98554926 0.99544546 0.97224864 0.98128889 0.99446336\n",
            " 0.995288   0.99736504 0.99538787 0.99944627] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 300 , Mean_loss: 9.855495559476927e-07 , Mean_accuracy: 0.9993963950900849 , Min_accuracy: 0.9360966738037917 , Learning rate: 1e-05 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 300 Mean_loss: 8.927646536839105e-07 , Mean Acc: 0.9994232700512873 , Min Acc: 0.9687121712162561 \n",
            ", Mean Accs Foc [0.99979149 0.99937138 0.99914567 0.99942673 0.99977388 0.99988695\n",
            " 0.99897751 0.99948897 0.99979176 0.99947838 0.99852301 0.99889268\n",
            " 0.99905907 0.9998075  0.99942383 0.99994208] \n",
            ", Min Accs Foc: [0.99310772 0.99119834 0.9882502  0.98142474 0.99399281 0.99667194\n",
            " 0.96871217 0.98560756 0.9954751  0.97241094 0.98105294 0.99444815\n",
            " 0.99532186 0.99741041 0.99537547 0.9994097 ] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 400 , Mean_loss: 9.762882964451433e-07 , Mean_accuracy: 0.9993985478956396 , Min_accuracy: 0.9230346562142963 , Learning rate: 1e-05 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 400 Mean_loss: 8.879350295674019e-07 , Mean Acc: 0.9994236365443991 , Min Acc: 0.9687053719007283 \n",
            ", Mean Accs Foc [0.99979238 0.99936822 0.99914898 0.99942517 0.99976911 0.99988731\n",
            " 0.99898455 0.9994861  0.99978989 0.99947864 0.99852933 0.99889337\n",
            " 0.99906209 0.99980971 0.99942439 0.99994033] \n",
            ", Min Accs Foc: [0.99320629 0.99123231 0.98822695 0.98164536 0.9939837  0.99661215\n",
            " 0.96870537 0.98557342 0.99546559 0.97248809 0.98129581 0.99444479\n",
            " 0.9953221  0.99733797 0.99531164 0.99946281] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 500 , Mean_loss: 9.664220128836173e-07 , Mean_accuracy: 0.999400696889284 , Min_accuracy: 0.9222771363267519 , Learning rate: 1e-05 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 500 Mean_loss: 8.814683121775987e-07 , Mean Acc: 0.9994260030412502 , Min Acc: 0.968602820584875 \n",
            ", Mean Accs Foc [0.99979298 0.99937495 0.99915158 0.9994247  0.99977276 0.99988521\n",
            " 0.99898545 0.99949283 0.99979094 0.99947959 0.99853481 0.99889477\n",
            " 0.99906645 0.99981026 0.9994271  0.99994031] \n",
            ", Min Accs Foc: [0.9930442  0.99108609 0.98825275 0.98174695 0.99383294 0.99667723\n",
            " 0.96860282 0.985569   0.99546654 0.97238618 0.98119764 0.99445948\n",
            " 0.99532533 0.99739489 0.99539436 0.99945838] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 600 , Mean_loss: 9.56101684034131e-07 , Mean_accuracy: 0.9994030498820212 , Min_accuracy: 0.9118878028820946 , Learning rate: 1e-05 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 600 Mean_loss: 8.772886523998244e-07 , Mean Acc: 0.9994263016258098 , Min Acc: 0.9686485137148754 \n",
            ", Mean Accs Foc [0.99978951 0.99937195 0.99915394 0.9994256  0.99977218 0.99988593\n",
            " 0.99898555 0.99949385 0.99979089 0.99948007 0.99853942 0.99889548\n",
            " 0.99907066 0.99981052 0.99942799 0.99994238] \n",
            ", Min Accs Foc: [0.99275989 0.9908356  0.98835038 0.98166649 0.99390559 0.99680956\n",
            " 0.96864851 0.98567935 0.99543773 0.972634   0.98147088 0.9944619\n",
            " 0.99532399 0.997404   0.9954276  0.99943854] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 700 , Mean_loss: 9.466318175650811e-07 , Mean_accuracy: 0.9994054643222282 , Min_accuracy: 0.9311764781627552 , Learning rate: 1e-05 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 700 Mean_loss: 8.737272220405176e-07 , Mean Acc: 0.9994277067457189 , Min Acc: 0.9685271772989682 \n",
            ", Mean Accs Foc [0.99979079 0.99937487 0.99915623 0.99942721 0.99977056 0.99988768\n",
            " 0.99899174 0.99949098 0.99979009 0.99947964 0.99854465 0.99889636\n",
            " 0.99907544 0.99981123 0.99942568 0.99994235] \n",
            ", Min Accs Foc: [0.99277415 0.99062616 0.98838178 0.9815669  0.9938711  0.99670724\n",
            " 0.96852718 0.98546921 0.99544062 0.97256358 0.98164855 0.99448621\n",
            " 0.99531452 0.9973872  0.99528566 0.99943892] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 800 , Mean_loss: 9.451258589633205e-07 , Mean_accuracy: 0.9994059159441195 , Min_accuracy: 0.9236695365531423 , Learning rate: 1e-05 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 800 Mean_loss: 8.669444031808574e-07 , Mean Acc: 0.9994300717980429 , Min Acc: 0.9684498452117374 \n",
            ", Mean Accs Foc [0.9997946  0.99937977 0.99915855 0.99942824 0.99977364 0.99988718\n",
            " 0.99899011 0.99949589 0.99979172 0.9994817  0.99854903 0.99889693\n",
            " 0.99908319 0.99981095 0.99943101 0.99994195] \n",
            ", Min Accs Foc: [0.99288644 0.99097264 0.98844282 0.98169115 0.99392049 0.99671654\n",
            " 0.96844985 0.98545381 0.9954782  0.97269347 0.98188219 0.99449795\n",
            " 0.995328   0.99741975 0.99542952 0.9994549 ] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 900 , Mean_loss: 9.36605909391318e-07 , Mean_accuracy: 0.9994081479624461 , Min_accuracy: 0.9296155482530859 , Learning rate: 1e-05 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 900 Mean_loss: 8.643087783511205e-07 , Mean Acc: 0.999428711443857 , Min Acc: 0.9687878403576283 \n",
            ", Mean Accs Foc [0.99978978 0.99937723 0.99916032 0.99942475 0.99977057 0.9998826\n",
            " 0.9989884  0.99949694 0.99979179 0.99947982 0.99855296 0.99889783\n",
            " 0.9990883  0.99981023 0.99943168 0.99994025] \n",
            ", Min Accs Foc: [0.99288534 0.99083859 0.98848231 0.98177158 0.99384218 0.99676006\n",
            " 0.96878784 0.98558954 0.99544053 0.97268031 0.9826708  0.99450659\n",
            " 0.99531738 0.99734247 0.99538036 0.99941294] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 1000 , Mean_loss: 9.319139114803116e-07 , Mean_accuracy: 0.9994078739355833 , Min_accuracy: 0.9109773716687499 , Learning rate: 1e-05 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 1000 Mean_loss: 8.632007751121967e-07 , Mean Acc: 0.999430049436065 , Min Acc: 0.9685783945752537 \n",
            ", Mean Accs Foc [0.99979384 0.99938252 0.99916198 0.99942476 0.99977446 0.99988806\n",
            " 0.99899106 0.99948273 0.99979229 0.99948154 0.99855651 0.99889843\n",
            " 0.99909652 0.99981179 0.99943179 0.999941  ] \n",
            ", Min Accs Foc: [0.9929684  0.99121885 0.98857479 0.98163838 0.9937891  0.99681092\n",
            " 0.96857839 0.9854078  0.99546973 0.97294254 0.98266747 0.99452635\n",
            " 0.99532769 0.99739749 0.9953737  0.99945302] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 1100 , Mean_loss: 9.283014668185293e-07 , Mean_accuracy: 0.9994086596731506 , Min_accuracy: 0.9323462407681147 , Learning rate: 9.996984593744525e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 1100 Mean_loss: 8.555814707465297e-07 , Mean Acc: 0.9994330733928874 , Min Acc: 0.9688337483280602 \n",
            ", Mean Accs Foc [0.99979764 0.99938143 0.99916396 0.99942924 0.99977421 0.99988628\n",
            " 0.99899502 0.99949905 0.99979213 0.99948339 0.99856107 0.9988993\n",
            " 0.99910391 0.99980605 0.99943221 0.99993808] \n",
            ", Min Accs Foc: [0.99310745 0.99137833 0.98856166 0.9818568  0.99375331 0.99679217\n",
            " 0.96883375 0.98571943 0.99548313 0.9728807  0.98268496 0.99452127\n",
            " 0.99533778 0.99730303 0.99552636 0.99939332] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 1200 , Mean_loss: 9.311434698184058e-07 , Mean_accuracy: 0.9994050584228462 , Min_accuracy: 0.9226804221594329 , Learning rate: 9.98794204878613e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 1200 Mean_loss: 8.607937105732199e-07 , Mean Acc: 0.9994257135464257 , Min Acc: 0.968830228229983 \n",
            ", Mean Accs Foc [0.99976953 0.99936867 0.99916471 0.99941041 0.99976554 0.99988255\n",
            " 0.9989843  0.99949922 0.99979339 0.99947944 0.99856324 0.99890017\n",
            " 0.99911078 0.99981159 0.9994337  0.99994097] \n",
            ", Min Accs Foc: [0.99299192 0.99127857 0.9885785  0.9820393  0.99362731 0.99692141\n",
            " 0.96883023 0.98563316 0.99547699 0.97284715 0.98304123 0.99453037\n",
            " 0.99534248 0.99742656 0.99551496 0.99940856] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 1300 , Mean_loss: 9.229840626459367e-07 , Mean_accuracy: 0.9994088513194939 , Min_accuracy: 0.9380174487782221 , Learning rate: 9.972883382072953e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 1300 Mean_loss: 8.505940951448257e-07 , Mean Acc: 0.9994349493057384 , Min Acc: 0.9689242395350803 \n",
            ", Mean Accs Foc [0.99979847 0.99938756 0.99916623 0.99942653 0.99977575 0.99988823\n",
            " 0.99899668 0.99949996 0.99979258 0.99948468 0.998567   0.99890112\n",
            " 0.99911479 0.99980946 0.99943583 0.99994239] \n",
            ", Min Accs Foc: [0.99303373 0.99124837 0.98865928 0.98189486 0.99349504 0.99677085\n",
            " 0.96892424 0.98568893 0.99551848 0.97298279 0.98345491 0.99454331\n",
            " 0.99532495 0.99734685 0.99548921 0.99944833] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 1400 , Mean_loss: 9.261037107855062e-07 , Mean_accuracy: 0.999405176853217 , Min_accuracy: 0.8949909358313483 , Learning rate: 9.951826940270774e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 1400 Mean_loss: 8.679864845068013e-07 , Mean Acc: 0.9994195391896488 , Min Acc: 0.9684837653927599 \n",
            ", Mean Accs Foc [0.99974331 0.99932121 0.99916671 0.9994304  0.9997446  0.99987111\n",
            " 0.99900032 0.99948287 0.99979272 0.99948878 0.9985708  0.9989013\n",
            " 0.99912264 0.99980118 0.99942997 0.99992037] \n",
            ", Min Accs Foc: [0.99233545 0.99033863 0.98882807 0.98174866 0.99410983 0.99721697\n",
            " 0.96848377 0.98591266 0.99551756 0.97259725 0.98356962 0.99456492\n",
            " 0.99534905 0.99754435 0.99561253 0.99953403] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 1500 , Mean_loss: 9.141047254354871e-07 , Mean_accuracy: 0.9994129171741771 , Min_accuracy: 0.9155627261478023 , Learning rate: 9.92479837741043e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 1500 Mean_loss: 8.456151580480814e-07 , Mean Acc: 0.9994357677320085 , Min Acc: 0.9680976600599406 \n",
            ", Mean Accs Foc [0.99978956 0.99938617 0.9991689  0.99943114 0.9997763  0.99988772\n",
            " 0.99900184 0.99949678 0.99979397 0.99948664 0.99857342 0.9989021\n",
            " 0.99912649 0.99981286 0.9994322  0.99994278] \n",
            ", Min Accs Foc: [0.99285028 0.99091364 0.98882775 0.98208627 0.9937208  0.9968491\n",
            " 0.96809766 0.98595451 0.99547277 0.97287578 0.98358288 0.99456561\n",
            " 0.9953375  0.9974065  0.99561045 0.99943017] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 1600 , Mean_loss: 9.191868888194776e-07 , Mean_accuracy: 0.9994069794059126 , Min_accuracy: 0.9371896246959508 , Learning rate: 9.891830623632339e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 1600 Mean_loss: 8.5644868674206e-07 , Mean Acc: 0.9994265958332311 , Min Acc: 0.968817158079482 \n",
            ", Mean Accs Foc [0.99977423 0.99937024 0.99917012 0.99941581 0.9997665  0.999882\n",
            " 0.99899397 0.99946773 0.99979424 0.9994793  0.9985764  0.99890257\n",
            " 0.99912963 0.99981327 0.99942809 0.99993381] \n",
            ", Min Accs Foc: [0.99341562 0.99096897 0.98869082 0.98218096 0.99342078 0.99669798\n",
            " 0.96881716 0.98547058 0.99542801 0.97295798 0.9837551  0.99459285\n",
            " 0.99532174 0.99739252 0.99532134 0.99937654] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 1700 , Mean_loss: 9.04671719785703e-07 , Mean_accuracy: 0.9994152272206711 , Min_accuracy: 0.9265847633237748 , Learning rate: 9.852963845066182e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 1700 Mean_loss: 8.582353325651171e-07 , Mean Acc: 0.9994205036181711 , Min Acc: 0.9689857650907049 \n",
            ", Mean Accs Foc [0.99974638 0.99933597 0.99917107 0.99941608 0.99975545 0.99987338\n",
            " 0.99899369 0.99948105 0.99979381 0.9994761  0.99857956 0.99890355\n",
            " 0.99913246 0.9998126  0.99943102 0.99993368] \n",
            ", Min Accs Foc: [0.9933672  0.99117076 0.98875713 0.98213025 0.99369557 0.99676268\n",
            " 0.96898577 0.98579599 0.99545901 0.97274364 0.98392058 0.99460003\n",
            " 0.9953119  0.99736761 0.99531804 0.99938071] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 1800 , Mean_loss: 9.084800379814421e-07 , Mean_accuracy: 0.9994121207790678 , Min_accuracy: 0.892620220871739 , Learning rate: 9.80824539489468e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 1800 Mean_loss: 8.373606538385705e-07 , Mean Acc: 0.9994376950583551 , Min Acc: 0.9685180900893785 \n",
            ", Mean Accs Foc [0.99979676 0.99938355 0.99917247 0.99943288 0.99977384 0.99988246\n",
            " 0.99900465 0.99950178 0.99979411 0.99948861 0.99858264 0.998904\n",
            " 0.99914004 0.99980881 0.99943928 0.99993024] \n",
            ", Min Accs Foc: [0.99302213 0.99108894 0.9888239  0.98212129 0.99374015 0.99703594\n",
            " 0.96851809 0.98594669 0.99544976 0.97273919 0.98426768 0.99461301\n",
            " 0.99534937 0.99750476 0.99547747 0.99952479] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 1900 , Mean_loss: 9.054402577472962e-07 , Mean_accuracy: 0.9994138678540944 , Min_accuracy: 0.9369827930001231 , Learning rate: 9.757729755661012e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 1900 Mean_loss: 9.029012401371578e-07 , Mean Acc: 0.9993997236323641 , Min Acc: 0.9682048494686761 \n",
            ", Mean Accs Foc [0.99967303 0.99924686 0.99917152 0.99941412 0.99969723 0.99985257\n",
            " 0.99900253 0.9994656  0.99979388 0.99949    0.99858448 0.99890466\n",
            " 0.99914422 0.99980304 0.99943831 0.99989875] \n",
            ", Min Accs Foc: [0.99117735 0.988598   0.98889414 0.98145849 0.99483703 0.99738454\n",
            " 0.96820485 0.9861468  0.9955064  0.97222551 0.98439043 0.99462913\n",
            " 0.99536119 0.99753728 0.99557165 0.99959442] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 2000 , Mean_loss: 8.986921038821691e-07 , Mean_accuracy: 0.999416187173788 , Min_accuracy: 0.9337540774223758 , Learning rate: 9.701478472890248e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 2000 Mean_loss: 8.36214022267129e-07 , Mean Acc: 0.9994387796558917 , Min Acc: 0.9688966464478632 \n",
            ", Mean Accs Foc [0.99979413 0.99938599 0.99917405 0.99943172 0.99977127 0.99988847\n",
            " 0.99900526 0.99950113 0.99979416 0.99949135 0.99858761 0.99890565\n",
            " 0.99914902 0.99981191 0.9994391  0.99994122] \n",
            ", Min Accs Foc: [0.99216063 0.9899714  0.98894016 0.98209746 0.99411908 0.99711443\n",
            " 0.96889665 0.98591583 0.99552606 0.97263945 0.98458701 0.99463906\n",
            " 0.9953756  0.99746673 0.99543332 0.99947165] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 2100 , Mean_loss: 8.988767867549063e-07 , Mean_accuracy: 0.999414253261741 , Min_accuracy: 0.9306218497211378 , Learning rate: 9.639560080105599e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 2100 Mean_loss: 8.273859234703108e-07 , Mean Acc: 0.9994418613567169 , Min Acc: 0.9687839163336829 \n",
            ", Mean Accs Foc [0.99980261 0.9993986  0.99917521 0.99943296 0.99977883 0.99988893\n",
            " 0.99900309 0.99950522 0.99979494 0.99948822 0.99859066 0.99890663\n",
            " 0.99915126 0.99981411 0.99943991 0.99994197] \n",
            ", Min Accs Foc: [0.99271897 0.99096404 0.98899631 0.98220092 0.99371484 0.99692046\n",
            " 0.96878392 0.98611272 0.99549424 0.97278054 0.98459279 0.99464205\n",
            " 0.99537326 0.99742157 0.9955913  0.99941632] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 2200 , Mean_loss: 8.923264026153845e-07 , Mean_accuracy: 0.9994180388372118 , Min_accuracy: 0.9202381579850997 , Learning rate: 9.572050015330875e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 2200 Mean_loss: 8.333822307508734e-07 , Mean Acc: 0.9994379108707799 , Min Acc: 0.9688275759825093 \n",
            ", Mean Accs Foc [0.99979254 0.99938812 0.99917612 0.99942433 0.99977219 0.99988372\n",
            " 0.99900465 0.99950157 0.99979563 0.99948396 0.99859248 0.99890772\n",
            " 0.99915533 0.99981399 0.99944222 0.99993369] \n",
            ", Min Accs Foc: [0.99363175 0.99099134 0.98897704 0.9823653  0.99359476 0.99696442\n",
            " 0.96882758 0.98627497 0.99542889 0.97291611 0.98477772 0.99464848\n",
            " 0.99538819 0.9973954  0.99556735 0.99935845] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 2300 , Mean_loss: 8.911917142785114e-07 , Mean_accuracy: 0.9994184688539108 , Min_accuracy: 0.9296963806386453 , Learning rate: 9.499030529180876e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 2300 Mean_loss: 8.377783959875006e-07 , Mean Acc: 0.9994345461106351 , Min Acc: 0.968142889407822 \n",
            ", Mean Accs Foc [0.99977745 0.99936381 0.99917637 0.99943361 0.99976324 0.99987276\n",
            " 0.9990072  0.99950336 0.99979589 0.99948973 0.99859468 0.99890889\n",
            " 0.99915832 0.9998132  0.99944242 0.99991996] \n",
            ", Min Accs Foc: [0.99226471 0.99023159 0.9889678  0.98187169 0.99426514 0.99720946\n",
            " 0.96814289 0.98611462 0.99549854 0.97284749 0.98493695 0.99467242\n",
            " 0.99539476 0.99746038 0.99547655 0.99955222] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 2400 , Mean_loss: 8.859978335132328e-07 , Mean_accuracy: 0.9994210853849491 , Min_accuracy: 0.9228097016071454 , Learning rate: 9.42059058465169e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 2400 Mean_loss: 8.338405976636382e-07 , Mean Acc: 0.9994379142202345 , Min Acc: 0.968551602134945 \n",
            ", Mean Accs Foc [0.99978553 0.99937269 0.99917751 0.999433   0.99976556 0.99988439\n",
            " 0.99901066 0.99950071 0.99979624 0.99949187 0.99859781 0.99890975\n",
            " 0.99916237 0.99980263 0.99944004 0.99991504] \n",
            ", Min Accs Foc: [0.99214596 0.99000255 0.98901083 0.98195847 0.99441178 0.99723072\n",
            " 0.9685516  0.98609771 0.99551331 0.97263947 0.98485763 0.99468888\n",
            " 0.99541231 0.99755247 0.99560827 0.99955889] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 2500 , Mean_loss: 8.860932345890492e-07 , Mean_accuracy: 0.9994193989336247 , Min_accuracy: 0.9388393524239267 , Learning rate: 9.336825748732973e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 2500 Mean_loss: 8.220056755049536e-07 , Mean Acc: 0.9994424292493111 , Min Acc: 0.9688644021651778 \n",
            ", Mean Accs Foc [0.99979732 0.99939722 0.99917873 0.99943581 0.99977407 0.99988719\n",
            " 0.99901091 0.99950055 0.99979613 0.99948815 0.99859929 0.99891106\n",
            " 0.99916438 0.99981109 0.99943433 0.9999412 ] \n",
            ", Min Accs Foc: [0.99335565 0.99129798 0.98902706 0.98223816 0.99362018 0.99703441\n",
            " 0.9688644  0.98619244 0.99554459 0.97273357 0.98516733 0.99469127\n",
            " 0.99541055 0.99734096 0.99572536 0.99946232] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 2600 , Mean_loss: 8.759354044649273e-07 , Mean_accuracy: 0.999424392295131 , Min_accuracy: 0.9418427524830697 , Learning rate: 9.247838075974309e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 2600 Mean_loss: 8.218049592792554e-07 , Mean Acc: 0.9994405267723756 , Min Acc: 0.9691296590113402 \n",
            ", Mean Accs Foc [0.99979025 0.99939428 0.99917989 0.9994272  0.99977264 0.99988941\n",
            " 0.99900429 0.9995039  0.9997966  0.99948357 0.99860192 0.99891241\n",
            " 0.99916739 0.99981261 0.99943531 0.99994343] \n",
            ", Min Accs Foc: [0.99339464 0.99119353 0.98897445 0.98233266 0.99359674 0.99699842\n",
            " 0.96912966 0.98605805 0.99549936 0.97288676 0.98527649 0.99470818\n",
            " 0.99541951 0.99747011 0.99571166 0.9994364 ] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 2700 , Mean_loss: 8.838500833343033e-07 , Mean_accuracy: 0.9994187747686653 , Min_accuracy: 0.9243961979524092 , Learning rate: 9.153735984147457e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 2700 Mean_loss: 8.165912063382596e-07 , Mean Acc: 0.99944425561111 , Min Acc: 0.9687828393821684 \n",
            ", Mean Accs Foc [0.99979811 0.99940031 0.99918105 0.99943112 0.99978123 0.99988795\n",
            " 0.99900917 0.99950973 0.99979591 0.99948558 0.99860493 0.99891365\n",
            " 0.9991709  0.99981393 0.99944477 0.99994175] \n",
            ", Min Accs Foc: [0.99346522 0.99153127 0.98894836 0.98262354 0.99367258 0.99696938\n",
            " 0.96878284 0.98608053 0.99556575 0.97292007 0.98515238 0.99472269\n",
            " 0.99543773 0.99746681 0.99554887 0.99946155] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 2800 , Mean_loss: 8.75247100158683e-07 , Mean_accuracy: 0.9994236998586737 , Min_accuracy: 0.9466893170964533 , Learning rate: 9.054634122155991e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 2800 Mean_loss: 8.131183265972818e-07 , Mean Acc: 0.9994460716248204 , Min Acc: 0.9688949440821346 \n",
            ", Mean Accs Foc [0.9998018  0.99940668 0.99918162 0.99943    0.99978148 0.99989012\n",
            " 0.99901105 0.99950976 0.99979671 0.99948934 0.99860666 0.99891511\n",
            " 0.99917294 0.99981541 0.99944419 0.99994086] \n",
            ", Min Accs Foc: [0.99318107 0.99122654 0.98893643 0.9824536  0.99375358 0.99688849\n",
            " 0.96889494 0.98602997 0.99553334 0.97279979 0.98542043 0.99473108\n",
            " 0.99543629 0.99741198 0.99543917 0.99940646] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 2900 , Mean_loss: 8.713059883214241e-07 , Mean_accuracy: 0.9994259204529281 , Min_accuracy: 0.9039397491282559 , Learning rate: 8.950653230353273e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 2900 Mean_loss: 8.154593650432154e-07 , Mean Acc: 0.9994439314645485 , Min Acc: 0.968365430028657 \n",
            ", Mean Accs Foc [0.99979707 0.99939559 0.99918194 0.99943562 0.9997769  0.99987999\n",
            " 0.99901342 0.99950306 0.99979729 0.99949312 0.99860782 0.99891632\n",
            " 0.99917766 0.99981119 0.99944273 0.99992465] \n",
            ", Min Accs Foc: [0.99298322 0.99137219 0.98909808 0.98231386 0.99393218 0.9973118\n",
            " 0.96836543 0.98630706 0.99551508 0.97264029 0.98542041 0.99474596\n",
            " 0.99547021 0.99749076 0.99566401 0.99953969] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 3000 , Mean_loss: 8.758024905541632e-07 , Mean_accuracy: 0.9994215893331436 , Min_accuracy: 0.9256789240011039 , Learning rate: 8.841919993438941e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 3000 Mean_loss: 8.098776357669354e-07 , Mean Acc: 0.9994471864684915 , Min Acc: 0.9682338411478099 \n",
            ", Mean Accs Foc [0.99980397 0.99940668 0.99918296 0.99943425 0.99978158 0.99988979\n",
            " 0.99901518 0.99950462 0.99979746 0.99948998 0.99861088 0.99891777\n",
            " 0.99917907 0.99981473 0.99944417 0.99994317] \n",
            ", Min Accs Foc: [0.99274635 0.99064996 0.98909054 0.98234205 0.99393455 0.99710481\n",
            " 0.96823384 0.98615239 0.99547928 0.97279757 0.98571434 0.99475274\n",
            " 0.99546669 0.99738169 0.99566001 0.99943531] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 3100 , Mean_loss: 8.657853357120349e-07 , Mean_accuracy: 0.9994281626164008 , Min_accuracy: 0.9301769539367795 , Learning rate: 8.728566886113103e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 3100 Mean_loss: 8.094773805492153e-07 , Mean Acc: 0.9994455196752363 , Min Acc: 0.9686478812551593 \n",
            ", Mean Accs Foc [0.99979597 0.99940221 0.99918373 0.99942974 0.99977822 0.99988718\n",
            " 0.99901142 0.99951022 0.99979812 0.99948927 0.99861256 0.99891927\n",
            " 0.99918153 0.99981529 0.99944546 0.99994287] \n",
            ", Min Accs Foc: [0.99301052 0.99117315 0.98903937 0.98251896 0.99389368 0.99692335\n",
            " 0.96864788 0.98640193 0.9954934  0.9729234  0.98567306 0.99475823\n",
            " 0.99547371 0.997435   0.99563754 0.99941614] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 3200 , Mean_loss: 8.713900420113154e-07 , Mean_accuracy: 0.9994236170522481 , Min_accuracy: 0.9347754047527546 , Learning rate: 8.610732011676323e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 3200 Mean_loss: 8.073079258472858e-07 , Mean Acc: 0.9994475073586344 , Min Acc: 0.9687835540574812 \n",
            ", Mean Accs Foc [0.99980462 0.99940965 0.99918425 0.99943599 0.99978146 0.99989089\n",
            " 0.99901501 0.99949604 0.99979706 0.99949112 0.99861474 0.99892073\n",
            " 0.9991848  0.99981482 0.9994391  0.99994309] \n",
            ", Min Accs Foc: [0.99277133 0.99082673 0.98910483 0.98246425 0.99402814 0.99706374\n",
            " 0.96878355 0.98616728 0.99555942 0.97296596 0.98594798 0.99476977\n",
            " 0.99549228 0.99737503 0.99575177 0.99944562] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 3300 , Mean_loss: 8.593251350361584e-07 , Mean_accuracy: 0.9994299083779167 , Min_accuracy: 0.9293365475550823 , Learning rate: 8.488558933772037e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 3300 Mean_loss: 8.122445200119308e-07 , Mean Acc: 0.9994435926195628 , Min Acc: 0.968429167868519 \n",
            ", Mean Accs Foc [0.99978158 0.99938132 0.99918495 0.99943487 0.99977415 0.99988648\n",
            " 0.99901558 0.99950816 0.99979824 0.99949539 0.99861664 0.99892183\n",
            " 0.99918789 0.99981531 0.99944705 0.9999262 ] \n",
            ", Min Accs Foc: [0.99333538 0.99158284 0.98915243 0.98263496 0.99360258 0.99710171\n",
            " 0.96842917 0.98634725 0.99551246 0.97259018 0.98605944 0.99478762\n",
            " 0.99550662 0.99744844 0.99560623 0.99952946] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 3400 , Mean_loss: 8.598605404360399e-07 , Mean_accuracy: 0.9994296043951764 , Min_accuracy: 0.9296506367608216 , Learning rate: 8.362196501476348e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 3400 Mean_loss: 8.068873406536846e-07 , Mean Acc: 0.9994482955260791 , Min Acc: 0.9687371348303864 \n",
            ", Mean Accs Foc [0.99980186 0.99940843 0.99918562 0.99943361 0.99977947 0.99988741\n",
            " 0.99901835 0.99950608 0.99979758 0.99949296 0.99861799 0.99892366\n",
            " 0.99918979 0.99981504 0.99944148 0.99994177] \n",
            ", Min Accs Foc: [0.99267811 0.99054764 0.98909582 0.9822792  0.9942738  0.9971132\n",
            " 0.96873713 0.98598938 0.99552431 0.97269759 0.98592159 0.99479367\n",
            " 0.99550395 0.99746309 0.99542344 0.99940578] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 3500 , Mean_loss: 8.597027057898896e-07 , Mean_accuracy: 0.9994298901258559 , Min_accuracy: 0.9284715943979803 , Learning rate: 8.23179866794837e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 3500 Mean_loss: 8.068831469289299e-07 , Mean Acc: 0.9994442730781521 , Min Acc: 0.96882762886523 \n",
            ", Mean Accs Foc [0.99979242 0.99939186 0.99918663 0.99943144 0.99977828 0.99988531\n",
            " 0.99901142 0.99950224 0.99979746 0.99948717 0.99862055 0.99892515\n",
            " 0.99919164 0.99981135 0.99944107 0.99993441] \n",
            ", Min Accs Foc: [0.99329824 0.9915029  0.98912923 0.98266503 0.99379946 0.99702015\n",
            " 0.96882763 0.98642377 0.99549285 0.97292641 0.98603349 0.99480376\n",
            " 0.99551299 0.99733199 0.99573844 0.99937774] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 3600 , Mean_loss: 8.62573882660179e-07 , Mean_accuracy: 0.9994266555185494 , Min_accuracy: 0.9259037571709948 , Learning rate: 8.09752430286201e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 3600 Mean_loss: 8.339797504557774e-07 , Mean Acc: 0.9994250881115753 , Min Acc: 0.9692153121330407 \n",
            ", Mean Accs Foc [0.99973633 0.99933384 0.99918718 0.99941189 0.99975438 0.99987849\n",
            " 0.99900561 0.9994654  0.99979816 0.99947951 0.99862095 0.99892638\n",
            " 0.99919381 0.99981231 0.99943044 0.99991968] \n",
            ", Min Accs Foc: [0.99377168 0.99100959 0.98904553 0.98282344 0.99351576 0.99702029\n",
            " 0.96921531 0.98585174 0.99551061 0.9730774  0.98592959 0.99481236\n",
            " 0.99551742 0.99734558 0.995353   0.99933052] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 3700 , Mean_loss: 8.545442922611941e-07 , Mean_accuracy: 0.9994314563909652 , Min_accuracy: 0.9336297978188679 , Learning rate: 7.959536998847742e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 3700 Mean_loss: 8.00031910462523e-07 , Mean Acc: 0.9994507551538967 , Min Acc: 0.9682480956601501 \n",
            ", Mean Accs Foc [0.99980353 0.99941376 0.99918792 0.99943511 0.99978239 0.99989068\n",
            " 0.99901775 0.99951043 0.99979886 0.99949199 0.99862384 0.99892771\n",
            " 0.9991967  0.99981307 0.99944873 0.99994336] \n",
            ", Min Accs Foc: [0.99358991 0.99142799 0.98918269 0.98263464 0.99382607 0.99697919\n",
            " 0.9682481  0.98628926 0.99556899 0.97299458 0.98640896 0.99482029\n",
            " 0.99553309 0.99749011 0.99563379 0.99944381] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 3800 , Mean_loss: 8.515288367190818e-07 , Mean_accuracy: 0.9994333690144944 , Min_accuracy: 0.93442211403745 , Learning rate: 7.818004872180198e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 3800 Mean_loss: 7.958617489776987e-07 , Mean Acc: 0.9994518158986396 , Min Acc: 0.9686553463712806 \n",
            ", Mean Accs Foc [0.99980663 0.99941372 0.99918841 0.9994355  0.99978092 0.99989117\n",
            " 0.99901858 0.99951433 0.99979924 0.99949347 0.9986259  0.99892904\n",
            " 0.99919975 0.99981648 0.99944903 0.9999425 ] \n",
            ", Min Accs Foc: [0.99303958 0.99114806 0.98916116 0.98247433 0.99409925 0.99715129\n",
            " 0.96865535 0.98634304 0.99550123 0.97281172 0.98635468 0.99483267\n",
            " 0.99555491 0.99742693 0.99564744 0.99945937] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 3900 , Mean_loss: 8.500080000330233e-07 , Mean_accuracy: 0.9994338703094064 , Min_accuracy: 0.9148586658685712 , Learning rate: 7.673100357954366e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 3900 Mean_loss: 7.9564538058537e-07 , Mean Acc: 0.9994513848632995 , Min Acc: 0.9687773032159434 \n",
            ", Mean Accs Foc [0.99980491 0.99941263 0.99918913 0.99943881 0.99978015 0.99989086\n",
            " 0.99901869 0.99950559 0.99979913 0.9994955  0.99862798 0.99893071\n",
            " 0.99920212 0.9998163  0.9994396  0.99993769] \n",
            ", Min Accs Foc: [0.99294217 0.99109045 0.98922012 0.98257592 0.99398578 0.99713803\n",
            " 0.9687773  0.98640694 0.99555074 0.97289921 0.9863009  0.99483624\n",
            " 0.99556537 0.99740662 0.99576252 0.99949005] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 4000 , Mean_loss: 8.496498838221384e-07 , Mean_accuracy: 0.9994334084011343 , Min_accuracy: 0.9290934858352917 , Learning rate: 7.525e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 4000 Mean_loss: 7.938792080831698e-07 , Mean Acc: 0.999452107809405 , Min Acc: 0.9684342202629088 \n",
            ", Mean Accs Foc [0.99980716 0.99941436 0.99918979 0.99943675 0.99978059 0.99988949\n",
            " 0.99902055 0.99951063 0.9997992  0.99949324 0.99862932 0.99893228\n",
            " 0.99920488 0.99981666 0.99944247 0.9999417 ] \n",
            ", Min Accs Foc: [0.99334062 0.99158106 0.98916676 0.98256725 0.99403526 0.99713052\n",
            " 0.96843422 0.98648414 0.99560581 0.97299836 0.98639003 0.99484181\n",
            " 0.99558053 0.99741332 0.9957507  0.99941451] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 4100 , Mean_loss: 8.492758485850947e-07 , Mean_accuracy: 0.9994333092213549 , Min_accuracy: 0.9267441029814589 , Learning rate: 7.373884235790159e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 4100 Mean_loss: 7.978494434099574e-07 , Mean Acc: 0.9994477816320688 , Min Acc: 0.9689407001660596 \n",
            ", Mean Accs Foc [0.99978985 0.99940251 0.99919028 0.99943154 0.99978023 0.99988641\n",
            " 0.99901358 0.99950794 0.99979982 0.99948998 0.99863061 0.99893341\n",
            " 0.99920704 0.99981405 0.99944942 0.99994021] \n",
            ", Min Accs Foc: [0.99295635 0.99125822 0.9891181  0.98258751 0.99403098 0.99721573\n",
            " 0.9689407  0.98654956 0.9954842  0.97276342 0.98631924 0.99487045\n",
            " 0.99558608 0.99735914 0.99558009 0.99941534] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 4200 , Mean_loss: 8.455833923197133e-07 , Mean_accuracy: 0.9994341485902966 , Min_accuracy: 0.9400756186108669 , Learning rate: 7.219937176605934e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 4200 Mean_loss: 7.911252398795778e-07 , Mean Acc: 0.999452999825378 , Min Acc: 0.9686717589756304 \n",
            ", Mean Accs Foc [0.99980651 0.99941765 0.99919062 0.99943751 0.99978312 0.99989116\n",
            " 0.99902207 0.9995067  0.99979975 0.99949425 0.99863165 0.99893521\n",
            " 0.99920986 0.99981513 0.99944066 0.99993846] \n",
            ", Min Accs Foc: [0.99329651 0.99140878 0.98913955 0.98259425 0.99387936 0.99691519\n",
            " 0.96867176 0.98649341 0.99554658 0.9730112  0.98650237 0.99486265\n",
            " 0.9956054  0.99739887 0.99579662 0.99938523] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 4300 , Mean_loss: 8.427599090524431e-07 , Mean_accuracy: 0.9994358973231576 , Min_accuracy: 0.9317362837096933 , Learning rate: 7.063346383225212e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 4300 Mean_loss: 7.892648743092467e-07 , Mean Acc: 0.9994540415866273 , Min Acc: 0.9688092209264281 \n",
            ", Mean Accs Foc [0.99980759 0.99941849 0.99919138 0.99943765 0.99978457 0.99989079\n",
            " 0.99902509 0.99950583 0.99980058 0.99949617 0.99863407 0.99893653\n",
            " 0.99921214 0.99981653 0.99944454 0.99994342] \n",
            ", Min Accs Foc: [0.99290331 0.99111483 0.98917793 0.9826294  0.994035   0.99708936\n",
            " 0.96880922 0.98652408 0.9955539  0.97278668 0.98654192 0.99487779\n",
            " 0.99561838 0.99739488 0.9957294  0.99943818] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 4400 , Mean_loss: 8.41013286093573e-07 , Mean_accuracy: 0.9994368017087223 , Min_accuracy: 0.9242590916580192 , Learning rate: 6.9043026374087665e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 4400 Mean_loss: 7.881225761821314e-07 , Mean Acc: 0.9994546236378101 , Min Acc: 0.9687769257804866 \n",
            ", Mean Accs Foc [0.99980599 0.99941778 0.99919173 0.99943715 0.99978442 0.99989119\n",
            " 0.99902266 0.99951786 0.99980036 0.999494   0.99863529 0.99893783\n",
            " 0.9992144  0.99981665 0.9994516  0.99994311] \n",
            ", Min Accs Foc: [0.99316474 0.99121725 0.98926329 0.98267867 0.9940367  0.99705849\n",
            " 0.96877693 0.98634932 0.99558196 0.972764   0.9867276  0.99489435\n",
            " 0.99562728 0.99743278 0.99558935 0.99942052] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 4500 , Mean_loss: 8.404469244325443e-07 , Mean_accuracy: 0.999437152113652 , Min_accuracy: 0.9340038599431556 , Learning rate: 6.742999709462062e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 4500 Mean_loss: 7.867479803109145e-07 , Mean Acc: 0.9994555105215149 , Min Acc: 0.9688026462373214 \n",
            ", Mean Accs Foc [0.99980968 0.99941938 0.9991924  0.99943759 0.99978437 0.99989166\n",
            " 0.99902443 0.99951562 0.99980043 0.99949569 0.99863698 0.99893952\n",
            " 0.9992163  0.99981711 0.99944989 0.99994325] \n",
            ", Min Accs Foc: [0.99298405 0.99080769 0.98919106 0.98277224 0.99413061 0.99720085\n",
            " 0.96880265 0.98653866 0.99564188 0.97285479 0.98654849 0.99489493\n",
            " 0.99563475 0.99741014 0.99564108 0.99945185] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 4600 , Mean_loss: 8.371252423387911e-07 , Mean_accuracy: 0.9994377014289103 , Min_accuracy: 0.9201053816013132 , Learning rate: 6.57963412215599e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 4600 Mean_loss: 7.859812097269397e-07 , Mean Acc: 0.9994557219199549 , Min Acc: 0.9685689923487839 \n",
            ", Mean Accs Foc [0.99980874 0.99942123 0.99919312 0.99944042 0.99978439 0.99988843\n",
            " 0.99902705 0.9995122  0.99980072 0.99949657 0.99863804 0.9989409\n",
            " 0.9992179  0.99981415 0.99944818 0.9999325 ] \n",
            ", Min Accs Foc: [0.99276273 0.9908481  0.98924237 0.98219188 0.99419711 0.99718246\n",
            " 0.96856899 0.98650125 0.99549993 0.97277093 0.98702059 0.99490522\n",
            " 0.99563737 0.99734234 0.99575061 0.99937275] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 4700 , Mean_loss: 8.376074510203064e-07 , Mean_accuracy: 0.999437684068993 , Min_accuracy: 0.9280323234590223 , Learning rate: 6.414404911294146e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 4700 Mean_loss: 7.863341052994128e-07 , Mean Acc: 0.9994550364502983 , Min Acc: 0.9685388686973462 \n",
            ", Mean Accs Foc [0.99980158 0.99941556 0.99919349 0.99944039 0.99978055 0.99989227\n",
            " 0.99902568 0.99951585 0.99980085 0.99949689 0.99863956 0.99894257\n",
            " 0.99922102 0.99981708 0.99945237 0.99994332] \n",
            ", Min Accs Foc: [0.99304833 0.99122437 0.98923261 0.98264848 0.99409906 0.9971886\n",
            " 0.96853887 0.98658291 0.99557551 0.97309074 0.98658611 0.99491235\n",
            " 0.99565884 0.99739465 0.99563356 0.99943071] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 4800 , Mean_loss: 8.356958306257442e-07 , Mean_accuracy: 0.9994382902946296 , Min_accuracy: 0.9097887845204728 , Learning rate: 6.247513383218356e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 4800 Mean_loss: 7.852676283060047e-07 , Mean Acc: 0.9994549302709652 , Min Acc: 0.968519877473129 \n",
            ", Mean Accs Foc [0.99980345 0.99941634 0.99919384 0.99943942 0.99977672 0.99989231\n",
            " 0.99902748 0.999512   0.99980039 0.99949867 0.99864089 0.99894428\n",
            " 0.99922335 0.9998169  0.99945055 0.99994008] \n",
            ", Min Accs Foc: [0.99315135 0.99120067 0.9892618  0.98273235 0.993946   0.99702951\n",
            " 0.96851988 0.98647724 0.9956289  0.97281516 0.98701372 0.99491834\n",
            " 0.99567215 0.99743369 0.99571815 0.99939833] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 4900 , Mean_loss: 8.339435648154298e-07 , Mean_accuracy: 0.9994391278095216 , Min_accuracy: 0.9362964589655307 , Learning rate: 6.079162869547908e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 4900 Mean_loss: 7.832618626055527e-07 , Mean Acc: 0.9994566098988229 , Min Acc: 0.9686401188303512 \n",
            ", Mean Accs Foc [0.99980775 0.99942084 0.99919429 0.99943831 0.99978433 0.99989104\n",
            " 0.99902577 0.9995171  0.99980104 0.99949817 0.99864144 0.99894535\n",
            " 0.9992244  0.999815   0.99945221 0.99994341] \n",
            ", Min Accs Foc: [0.99250448 0.99057416 0.98926332 0.9825437  0.9941934  0.99729878\n",
            " 0.96864012 0.98647836 0.99558925 0.97266447 0.98670046 0.99492567\n",
            " 0.99566882 0.99737324 0.99568819 0.99943017] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 5000 , Mean_loss: 8.302872083424984e-07 , Mean_accuracy: 0.9994408079930708 , Min_accuracy: 0.9409993699838072 , Learning rate: 5.909558479451306e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 5000 Mean_loss: 7.808219741460201e-07 , Mean Acc: 0.9994574945441248 , Min Acc: 0.9685431098435431 \n",
            ", Mean Accs Foc [0.99980921 0.99942198 0.99919519 0.99943908 0.99978485 0.99989187\n",
            " 0.99902732 0.99951936 0.99980119 0.99949615 0.99864358 0.99894663\n",
            " 0.99922725 0.99981708 0.99945238 0.99994328] \n",
            ", Min Accs Foc: [0.99285286 0.99107238 0.98927002 0.98259303 0.99414561 0.99723707\n",
            " 0.96854311 0.98649746 0.99560218 0.97301913 0.98659501 0.99493539\n",
            " 0.99568795 0.99741683 0.99558869 0.99944869] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 5100 , Mean_loss: 8.302208034537622e-07 , Mean_accuracy: 0.9994397197069388 , Min_accuracy: 0.932935793949903 , Learning rate: 5.7389068497523245e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 5100 Mean_loss: 7.796338840796158e-07 , Mean Acc: 0.9994571694584022 , Min Acc: 0.9688849788257115 \n",
            ", Mean Accs Foc [0.99981089 0.99942244 0.99919552 0.99943941 0.99978502 0.99989254\n",
            " 0.99902595 0.9995109  0.99980151 0.99949727 0.9986448  0.99894804\n",
            " 0.9992295  0.99981739 0.99944512 0.99994294] \n",
            ", Min Accs Foc: [0.99311135 0.99131782 0.98930178 0.98246886 0.9941528  0.99710305\n",
            " 0.96888498 0.98654165 0.99559127 0.97282279 0.98697248 0.99494037\n",
            " 0.99570103 0.9974359  0.99576733 0.99943691] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 5200 , Mean_loss: 8.297064861790713e-07 , Mean_accuracy: 0.9994407634977875 , Min_accuracy: 0.9355786575240135 , Learning rate: 5.567415893174885e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 5200 Mean_loss: 7.803578366551894e-07 , Mean Acc: 0.999457844739648 , Min Acc: 0.9684751772183536 \n",
            ", Mean Accs Foc [0.99981029 0.99942047 0.999196   0.9994406  0.99978337 0.99989348\n",
            " 0.99902728 0.99951805 0.99980086 0.99949758 0.99864561 0.99894957\n",
            " 0.99923127 0.99981557 0.99944996 0.99994297] \n",
            ", Min Accs Foc: [0.99297907 0.99094932 0.98926751 0.98239748 0.9941902  0.99701071\n",
            " 0.96847518 0.98638801 0.99559789 0.97263852 0.9871701  0.99496638\n",
            " 0.99570734 0.99748376 0.99574638 0.99941785] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 5300 , Mean_loss: 8.254067581630247e-07 , Mean_accuracy: 0.9994422774277097 , Min_accuracy: 0.9379105410690649 , Learning rate: 5.395294545033421e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 5300 Mean_loss: 7.771299437202877e-07 , Mean Acc: 0.9994589450707735 , Min Acc: 0.9686258178546081 \n",
            ", Mean Accs Foc [0.99981041 0.99942432 0.99919639 0.99943992 0.99978567 0.99989226\n",
            " 0.99902939 0.99952005 0.9998015  0.99949842 0.99864727 0.99895081\n",
            " 0.9992333  0.99981711 0.99945355 0.99994332] \n",
            ", Min Accs Foc: [0.99293091 0.99100038 0.98928795 0.98249674 0.99413464 0.99713076\n",
            " 0.96862582 0.98647768 0.9955967  0.97282921 0.98710715 0.99496731\n",
            " 0.99571514 0.99741049 0.995679   0.99943922] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 5400 , Mean_loss: 8.278217360078573e-07 , Mean_accuracy: 0.999441708817546 , Min_accuracy: 0.939827241756462 , Learning rate: 5.222752508677381e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 5400 Mean_loss: 7.768922701632227e-07 , Mean Acc: 0.9994584752314046 , Min Acc: 0.968681062974518 \n",
            ", Mean Accs Foc [0.99980757 0.99942327 0.99919699 0.99944056 0.99978404 0.9998928\n",
            " 0.99903002 0.99951591 0.99980194 0.99949842 0.99864881 0.99895219\n",
            " 0.99923548 0.99981666 0.99945224 0.99994333] \n",
            ", Min Accs Foc: [0.99305643 0.99121815 0.98924709 0.98262955 0.99411037 0.99719712\n",
            " 0.96868106 0.98673481 0.99565089 0.9729293  0.9868913  0.99496911\n",
            " 0.99572917 0.99738798 0.99571415 0.99943362] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 5500 , Mean_loss: 8.24109963073077e-07 , Mean_accuracy: 0.9994429430435038 , Min_accuracy: 0.9210506670294704 , Learning rate: 5.050000000000001e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 5500 Mean_loss: 7.768623734088013e-07 , Mean Acc: 0.9994583329451963 , Min Acc: 0.9687157289904675 \n",
            ", Mean Accs Foc [0.99980607 0.99942071 0.99919718 0.99944068 0.99978338 0.99989076\n",
            " 0.9990301  0.99951955 0.99980181 0.99949905 0.99864955 0.99895334\n",
            " 0.99923802 0.99981728 0.99945347 0.99993398] \n",
            ", Min Accs Foc: [0.99298689 0.99090931 0.9892289  0.98262103 0.99402901 0.99709665\n",
            " 0.96871573 0.98658976 0.9955644  0.9727886  0.98696799 0.99497688\n",
            " 0.99574568 0.99743919 0.99564918 0.99949973] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 5600 , Mean_loss: 8.218463345888574e-07 , Mean_accuracy: 0.9994435525912024 , Min_accuracy: 0.9280891417841042 , Learning rate: 4.877247491322622e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 5600 Mean_loss: 7.761417519194884e-07 , Mean Acc: 0.9994590703585626 , Min Acc: 0.9687903021633301 \n",
            ", Mean Accs Foc [0.99981037 0.99942574 0.99919797 0.99943894 0.99978716 0.99989255\n",
            " 0.99902915 0.99951305 0.9998024  0.99949897 0.99865064 0.99895479\n",
            " 0.99923944 0.99981755 0.9994488  0.99993884] \n",
            ", Min Accs Foc: [0.99298795 0.99103903 0.98926553 0.98267859 0.99420538 0.99717474\n",
            " 0.9687903  0.98627173 0.99558203 0.97286204 0.98689674 0.99497872\n",
            " 0.99574761 0.99739329 0.99552823 0.99939796] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 5700 , Mean_loss: 8.226708909159772e-07 , Mean_accuracy: 0.9994434388826835 , Min_accuracy: 0.9315749180297035 , Learning rate: 4.704705454966581e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 5700 Mean_loss: 7.755846189923844e-07 , Mean Acc: 0.9994586360613407 , Min Acc: 0.9686252950367764 \n",
            ", Mean Accs Foc [0.99981044 0.99942521 0.99919801 0.99944143 0.99978547 0.9998933\n",
            " 0.99903098 0.99950508 0.9998023  0.99949775 0.99865205 0.99895589\n",
            " 0.99924147 0.99981562 0.99944823 0.99994183] \n",
            ", Min Accs Foc: [0.99269248 0.99071871 0.98929922 0.98250613 0.99437963 0.99719032\n",
            " 0.9686253  0.98665965 0.99561012 0.97288161 0.98701523 0.99499707\n",
            " 0.99576273 0.99738353 0.99577879 0.99946622] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 5800 , Mean_loss: 8.210289613578245e-07 , Mean_accuracy: 0.9994441922120043 , Min_accuracy: 0.9213424717557627 , Learning rate: 4.532584106825116e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 5800 Mean_loss: 7.745298281445415e-07 , Mean Acc: 0.9994589678380175 , Min Acc: 0.9683338324691898 \n",
            ", Mean Accs Foc [0.99980887 0.99942534 0.99919847 0.99944134 0.99978575 0.99989229\n",
            " 0.99902932 0.99951    0.99980259 0.99949926 0.99865248 0.99895751\n",
            " 0.99924352 0.99981546 0.99944833 0.99993887] \n",
            ", Min Accs Foc: [0.99308923 0.99129524 0.98928516 0.98278278 0.99406236 0.99716059\n",
            " 0.96833383 0.98669752 0.99562791 0.97292051 0.98695327 0.99499637\n",
            " 0.99577199 0.99748957 0.99580357 0.99938989] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 5900 , Mean_loss: 8.201775669329706e-07 , Mean_accuracy: 0.9994446064316335 , Min_accuracy: 0.9061225991635751 , Learning rate: 4.361093150247677e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 5900 Mean_loss: 7.739372356336037e-07 , Mean Acc: 0.9994583219723766 , Min Acc: 0.9690229113705775 \n",
            ", Mean Accs Foc [0.99980383 0.99941771 0.99919885 0.99943786 0.99978484 0.99989385\n",
            " 0.99902954 0.99952006 0.99980243 0.99949602 0.99865319 0.9989586\n",
            " 0.99924558 0.99981758 0.99945106 0.99994292] \n",
            ", Min Accs Foc: [0.99289644 0.99097157 0.98930716 0.98257506 0.99409935 0.99711697\n",
            " 0.96902291 0.98661477 0.99550075 0.97293698 0.98714162 0.99500368\n",
            " 0.99578416 0.99740782 0.99579299 0.99941986] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 6000 , Mean_loss: 8.208112611582958e-07 , Mean_accuracy: 0.9994440288075032 , Min_accuracy: 0.9340294662942159 , Learning rate: 4.190441520548696e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 6000 Mean_loss: 7.727747654758357e-07 , Mean Acc: 0.9994591740382187 , Min Acc: 0.9686894921244813 \n",
            ", Mean Accs Foc [0.99981202 0.99942857 0.99919912 0.99943972 0.99978687 0.99989379\n",
            " 0.99903131 0.99950203 0.99980288 0.99949743 0.99865421 0.99895987\n",
            " 0.99924615 0.99981772 0.99943912 0.99994282] \n",
            ", Min Accs Foc: [0.99340587 0.99163167 0.98929826 0.98279104 0.99401701 0.99711551\n",
            " 0.96868949 0.98661895 0.99564871 0.97283895 0.98706834 0.99500199\n",
            " 0.99577888 0.99742988 0.99588286 0.99945192] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 6100 , Mean_loss: 8.182648286481966e-07 , Mean_accuracy: 0.9994451396052566 , Min_accuracy: 0.9097939524033491 , Learning rate: 4.0208371304520915e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 6100 Mean_loss: 7.709449861925728e-07 , Mean Acc: 0.9994611997061859 , Min Acc: 0.9685914785663636 \n",
            ", Mean Accs Foc [0.99981233 0.99942837 0.99919976 0.99943958 0.99978715 0.9998939\n",
            " 0.99903156 0.99951788 0.99980256 0.99950132 0.99865569 0.99896112\n",
            " 0.99924795 0.99981621 0.99945155 0.9999432 ] \n",
            ", Min Accs Foc: [0.99302816 0.99106677 0.98928657 0.98268196 0.99424829 0.99710424\n",
            " 0.96859148 0.98642231 0.99565087 0.97283745 0.98722103 0.99502245\n",
            " 0.99578562 0.99738367 0.99559374 0.99944913] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 6200 , Mean_loss: 8.18082152901309e-07 , Mean_accuracy: 0.9994452656132911 , Min_accuracy: 0.9217684094837512 , Learning rate: 3.852486616781646e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 6200 Mean_loss: 7.71193923959138e-07 , Mean Acc: 0.9994602291588002 , Min Acc: 0.9685854958423566 \n",
            ", Mean Accs Foc [0.99980936 0.99942667 0.99920011 0.99943672 0.99978616 0.99989299\n",
            " 0.99902859 0.99951914 0.99980291 0.99949966 0.99865685 0.99896222\n",
            " 0.99924969 0.99981705 0.99945384 0.99994315] \n",
            ", Min Accs Foc: [0.9930224  0.99118985 0.98932323 0.98272882 0.99415833 0.99714081\n",
            " 0.9685855  0.98638154 0.99562256 0.97267351 0.98720143 0.99503477\n",
            " 0.99579626 0.99745476 0.9956067  0.99944279] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 6300 , Mean_loss: 8.182202299418918e-07 , Mean_accuracy: 0.9994456154072708 , Min_accuracy: 0.9150116731289684 , Learning rate: 3.685595088705854e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 6300 Mean_loss: 7.691709634360089e-07 , Mean Acc: 0.9994617590218097 , Min Acc: 0.9687660014903556 \n",
            ", Mean Accs Foc [0.99981229 0.99942886 0.99920013 0.99944022 0.99978715 0.999894\n",
            " 0.99903143 0.99952181 0.99980348 0.99950027 0.99865639 0.99896369\n",
            " 0.99925274 0.99981718 0.99945518 0.99994316] \n",
            ", Min Accs Foc: [0.99297482 0.99104136 0.98930786 0.98278506 0.99427996 0.99721002\n",
            " 0.968766   0.98651008 0.99561776 0.97281429 0.98708506 0.99503386\n",
            " 0.99581989 0.99740588 0.99570203 0.99943331] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 6400 , Mean_loss: 8.156932148814777e-07 , Mean_accuracy: 0.9994460857944097 , Min_accuracy: 0.9346568674557577 , Learning rate: 3.52036587784401e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 6400 Mean_loss: 7.697619963635959e-07 , Mean Acc: 0.9994612901817022 , Min Acc: 0.9687241370491739 \n",
            ", Mean Accs Foc [0.99980765 0.9994258  0.99920053 0.9994412  0.99978507 0.99989207\n",
            " 0.99903248 0.99952208 0.99980348 0.99950252 0.99865791 0.99896463\n",
            " 0.99925361 0.99981738 0.99945523 0.99994087] \n",
            ", Min Accs Foc: [0.99269293 0.9907008  0.98934881 0.98285195 0.99425225 0.99723055\n",
            " 0.96872414 0.98640378 0.99567114 0.97275092 0.98717902 0.99504471\n",
            " 0.99581989 0.9974497  0.99568208 0.99947208] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 6500 , Mean_loss: 8.156010331299659e-07 , Mean_accuracy: 0.9994463956538957 , Min_accuracy: 0.9330111076054455 , Learning rate: 3.357000290537942e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 6500 Mean_loss: 7.674836460300644e-07 , Mean Acc: 0.999461973028682 , Min Acc: 0.9687316754550285 \n",
            ", Mean Accs Foc [0.99981227 0.99942828 0.99920109 0.99944032 0.999788   0.99989274\n",
            " 0.99903234 0.99952228 0.99980312 0.99949932 0.99865871 0.99896568\n",
            " 0.99925483 0.99981158 0.99945545 0.99994227] \n",
            ", Min Accs Foc: [0.99306203 0.99124359 0.98936396 0.98268956 0.99428738 0.99707556\n",
            " 0.96873168 0.98660137 0.99563741 0.97288584 0.98733802 0.99504735\n",
            " 0.99582465 0.99730971 0.99568071 0.99942522] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 6600 , Mean_loss: 8.116538913794372e-07 , Mean_accuracy: 0.9994475387610419 , Min_accuracy: 0.9181976592154226 , Learning rate: 3.1956973625912367e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 6600 Mean_loss: 7.667864180451263e-07 , Mean Acc: 0.9994620912985481 , Min Acc: 0.9688155744050017 \n",
            ", Mean Accs Foc [0.99981234 0.99943089 0.9992011  0.99943604 0.99978833 0.99989347\n",
            " 0.99903166 0.9995224  0.99980355 0.99950094 0.99865879 0.99896666\n",
            " 0.9992566  0.99981784 0.99945507 0.9999433 ] \n",
            ", Min Accs Foc: [0.99310857 0.99114355 0.98935608 0.98267963 0.99415284 0.99716306\n",
            " 0.96881557 0.98661185 0.99561253 0.97284893 0.98738354 0.99506507\n",
            " 0.99583545 0.99742418 0.99573662 0.99943048] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 6700 , Mean_loss: 8.123016425445944e-07 , Mean_accuracy: 0.9994473572980173 , Min_accuracy: 0.934539271768164 , Learning rate: 3.0366536167747895e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 6700 Mean_loss: 7.663646757307877e-07 , Mean Acc: 0.9994624797500458 , Min Acc: 0.9686686880484657 \n",
            ", Mean Accs Foc [0.99981351 0.99943086 0.99920119 0.99944058 0.99978766 0.99989436\n",
            " 0.99903135 0.99952027 0.99980299 0.99950137 0.99865968 0.99896783\n",
            " 0.99925846 0.99981736 0.99945329 0.99994324] \n",
            ", Min Accs Foc: [0.99278888 0.99082169 0.98938691 0.98263065 0.99428476 0.99711883\n",
            " 0.96866869 0.98661045 0.99563337 0.97282826 0.98745767 0.99505793\n",
            " 0.99584565 0.99744589 0.9957583  0.9994411 ] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 6800 , Mean_loss: 8.121143336206057e-07 , Mean_accuracy: 0.9994475682188088 , Min_accuracy: 0.9315599545982268 , Learning rate: 2.880062823394068e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 6800 Mean_loss: 7.656016006868461e-07 , Mean Acc: 0.999462662603883 , Min Acc: 0.9685296982758869 \n",
            ", Mean Accs Foc [0.99981271 0.99943132 0.99920155 0.99944064 0.99978708 0.99989317\n",
            " 0.99903351 0.99951976 0.99980406 0.99950139 0.99866055 0.99896873\n",
            " 0.9992603  0.99981757 0.99945498 0.99994306] \n",
            ", Min Accs Foc: [0.99301659 0.99114176 0.98936774 0.98269818 0.99427777 0.99727704\n",
            " 0.9685297  0.98661182 0.99562179 0.97283202 0.98756159 0.99506593\n",
            " 0.9958575  0.99744927 0.99574733 0.99945044] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 6900 , Mean_loss: 8.089744978689867e-07 , Mean_accuracy: 0.9994483997623212 , Min_accuracy: 0.9193698865675429 , Learning rate: 2.7261157642098423e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 6900 Mean_loss: 7.652973566582313e-07 , Mean Acc: 0.9994625150378889 , Min Acc: 0.968560995662503 \n",
            ", Mean Accs Foc [0.99981249 0.99943089 0.99920184 0.99944023 0.99978774 0.99989406\n",
            " 0.99903495 0.99951454 0.99980419 0.99950159 0.99866162 0.99896973\n",
            " 0.99926157 0.99981797 0.99945069 0.99994315] \n",
            ", Min Accs Foc: [0.9931309  0.99129062 0.98938377 0.98274576 0.99407786 0.99717544\n",
            " 0.968561   0.98669181 0.99564189 0.97295638 0.98752614 0.99507107\n",
            " 0.99586224 0.99742558 0.99580455 0.99942596] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 7000 , Mean_loss: 8.107475078858814e-07 , Mean_accuracy: 0.9994479377511395 , Min_accuracy: 0.9385496376317692 , Learning rate: 2.575000000000001e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 7000 Mean_loss: 7.638652888229285e-07 , Mean Acc: 0.999463250864765 , Min Acc: 0.9685772010304575 \n",
            ", Mean Accs Foc [0.9998126  0.99943179 0.99920226 0.99944082 0.99978918 0.99989453\n",
            " 0.99903314 0.99952093 0.99980408 0.99950066 0.99866228 0.99897066\n",
            " 0.99926232 0.99981773 0.99945227 0.99994319] \n",
            ", Min Accs Foc: [0.99288891 0.9909304  0.98943191 0.98250809 0.99423943 0.99718538\n",
            " 0.9685772  0.98664712 0.99562416 0.97288946 0.98750041 0.99508008\n",
            " 0.99586181 0.99740287 0.99578536 0.99943864] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 7100 , Mean_loss: 8.108066145133962e-07 , Mean_accuracy: 0.9994483065993006 , Min_accuracy: 0.9233072370722633 , Learning rate: 2.4268996420456366e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 7100 Mean_loss: 7.643983340075237e-07 , Mean Acc: 0.9994636850125663 , Min Acc: 0.9686066835095006 \n",
            ", Mean Accs Foc [0.99981317 0.99943139 0.99920238 0.99944219 0.99978805 0.9998942\n",
            " 0.99903556 0.99952286 0.99980428 0.9995009  0.99866196 0.99897175\n",
            " 0.99926415 0.99981686 0.99945561 0.99994324] \n",
            ", Min Accs Foc: [0.99284171 0.99095049 0.98936462 0.98272417 0.99426288 0.99716558\n",
            " 0.96860668 0.98647006 0.99564702 0.97287394 0.98764429 0.99508299\n",
            " 0.9958742  0.99746236 0.99571288 0.99944047] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 7200 , Mean_loss: 8.087737759726306e-07 , Mean_accuracy: 0.9994488588055725 , Min_accuracy: 0.9384446120388248 , Learning rate: 2.281995127819804e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 7200 Mean_loss: 7.63211073572968e-07 , Mean Acc: 0.9994633478940507 , Min Acc: 0.9689142834938519 \n",
            ", Mean Accs Foc [0.99981376 0.99943293 0.99920261 0.99944151 0.99978901 0.99989465\n",
            " 0.99903331 0.99951684 0.99980405 0.99950127 0.99866331 0.9989726\n",
            " 0.9992654  0.99981788 0.99944974 0.99994312] \n",
            ", Min Accs Foc: [0.99291615 0.99103424 0.98930904 0.98269039 0.99422204 0.99711074\n",
            " 0.96891428 0.98668538 0.99560815 0.97283426 0.98740948 0.99509211\n",
            " 0.99588073 0.99741065 0.9958361  0.9994461 ] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 7300 , Mean_loss: 8.101195399122566e-07 , Mean_accuracy: 0.9994484957814543 , Min_accuracy: 0.9400124229245912 , Learning rate: 2.140463001152259e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 7300 Mean_loss: 7.635714096589782e-07 , Mean Acc: 0.9994636914498807 , Min Acc: 0.9686233360860318 \n",
            ", Mean Accs Foc [0.99981342 0.99943287 0.9992029  0.99944012 0.99978955 0.99989488\n",
            " 0.99903408 0.99952023 0.99980429 0.99950113 0.99866343 0.99897345\n",
            " 0.99926658 0.99981759 0.99945534 0.9999426 ] \n",
            ", Min Accs Foc: [0.99294278 0.99108048 0.98936751 0.9828015  0.99421331 0.99712874\n",
            " 0.96862334 0.98640609 0.99559301 0.97291101 0.98754418 0.9951025\n",
            " 0.99588445 0.99745684 0.99564352 0.99946036] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 7400 , Mean_loss: 8.084197834817001e-07 , Mean_accuracy: 0.9994491413333453 , Min_accuracy: 0.9278188213160551 , Learning rate: 2.002475697137992e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 7400 Mean_loss: 7.621725391198466e-07 , Mean Acc: 0.999464246792378 , Min Acc: 0.968751599437687 \n",
            ", Mean Accs Foc [0.99981412 0.99943341 0.99920306 0.99944099 0.99978933 0.99989468\n",
            " 0.99903609 0.99952038 0.99980484 0.99950171 0.99866435 0.99897437\n",
            " 0.99926824 0.99981824 0.99945557 0.99994275] \n",
            ", Min Accs Foc: [0.99297799 0.99117218 0.98940192 0.98280319 0.99425233 0.99721316\n",
            " 0.9687516  0.98666088 0.99563568 0.97292578 0.98749937 0.9950962\n",
            " 0.9958967  0.99742818 0.99572094 0.9994579 ] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 7500 , Mean_loss: 8.082039588775045e-07 , Mean_accuracy: 0.9994491865956883 , Min_accuracy: 0.9346794565166582 , Learning rate: 1.868201332051631e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 7500 Mean_loss: 7.618032447916554e-07 , Mean Acc: 0.9994642488345014 , Min Acc: 0.9686857968863425 \n",
            ", Mean Accs Foc [0.99981337 0.99943337 0.9992033  0.99943963 0.99978943 0.9998946\n",
            " 0.99903462 0.99952359 0.99980448 0.99950189 0.99866468 0.99897509\n",
            " 0.99926925 0.9998174  0.99945624 0.99994329] \n",
            ", Min Accs Foc: [0.99301082 0.99109235 0.98941888 0.98282257 0.99419273 0.997117\n",
            " 0.9686858  0.98661909 0.9956094  0.97288718 0.98740498 0.99510518\n",
            " 0.99590047 0.99739944 0.99571866 0.99944221] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 7600 , Mean_loss: 8.081534567627264e-07 , Mean_accuracy: 0.999449270790437 , Min_accuracy: 0.923986577585109 , Learning rate: 1.737803498523652e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 7600 Mean_loss: 7.61774122906925e-07 , Mean Acc: 0.9994643433630698 , Min Acc: 0.9688063374548908 \n",
            ", Mean Accs Foc [0.99981356 0.99943282 0.9992033  0.9994402  0.99978934 0.99989506\n",
            " 0.99903499 0.99952368 0.99980412 0.99950213 0.99866475 0.99897593\n",
            " 0.9992704  0.99981811 0.99945621 0.99994283] \n",
            ", Min Accs Foc: [0.9929233  0.99080414 0.98941165 0.98288445 0.9943296  0.99713554\n",
            " 0.96880634 0.98665203 0.99561592 0.97287532 0.98748901 0.99510858\n",
            " 0.99590627 0.99740585 0.99567963 0.99943132] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 7700 , Mean_loss: 8.068779359294064e-07 , Mean_accuracy: 0.9994497449639036 , Min_accuracy: 0.9245704231892221 , Learning rate: 1.6114410662279633e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 7700 Mean_loss: 7.611393098211595e-07 , Mean Acc: 0.9994643742801494 , Min Acc: 0.9686875657422183 \n",
            ", Mean Accs Foc [0.99981461 0.99943386 0.9992036  0.99944015 0.99979024 0.9998945\n",
            " 0.99903428 0.99952067 0.99980471 0.99950193 0.99866574 0.99897665\n",
            " 0.99927087 0.99981755 0.99945559 0.99994302] \n",
            ", Min Accs Foc: [0.99312659 0.99128117 0.98938022 0.98286051 0.99415195 0.99711838\n",
            " 0.96868757 0.98670008 0.9955996  0.97279216 0.98759117 0.99511875\n",
            " 0.99590581 0.99745305 0.99577879 0.99942278] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 7800 , Mean_loss: 8.075890356168623e-07 , Mean_accuracy: 0.9994495512543554 , Min_accuracy: 0.9173320396351582 , Learning rate: 1.489267988323678e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 7800 Mean_loss: 7.60922471068668e-07 , Mean Acc: 0.9994645635372504 , Min Acc: 0.9689385699109 \n",
            ", Mean Accs Foc [0.99981457 0.9994335  0.99920342 0.99944216 0.99978852 0.99989432\n",
            " 0.99903537 0.99952139 0.99980495 0.99950267 0.99866545 0.99897749\n",
            " 0.99927263 0.99981789 0.99945511 0.99994328] \n",
            ", Min Accs Foc: [0.99289206 0.99090014 0.98936952 0.98264928 0.99432783 0.99722583\n",
            " 0.96893857 0.98670536 0.99559386 0.97287131 0.98743396 0.99511849\n",
            " 0.9959189  0.997445   0.9957725  0.99943592] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 7900 , Mean_loss: 8.055770541280972e-07 , Mean_accuracy: 0.9994500716492587 , Min_accuracy: 0.9238018619960713 , Learning rate: 1.3714331138868992e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 7900 Mean_loss: 7.603483901343991e-07 , Mean Acc: 0.9994648437070704 , Min Acc: 0.9687063047540366 \n",
            ", Mean Accs Foc [0.99981342 0.99943373 0.99920386 0.9994413  0.99978926 0.99989489\n",
            " 0.99903586 0.99952394 0.99980483 0.99950214 0.99866617 0.99897824\n",
            " 0.99927343 0.99981831 0.99945594 0.99994179] \n",
            ", Min Accs Foc: [0.99298129 0.99098879 0.98938587 0.98272719 0.9941946  0.99720845\n",
            " 0.9687063  0.98661555 0.99560785 0.97280918 0.98759767 0.99511753\n",
            " 0.99592169 0.99742918 0.99574788 0.99946156] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 8000 , Mean_loss: 8.048232842662672e-07 , Mean_accuracy: 0.9994501860395383 , Min_accuracy: 0.8932019174855217 , Learning rate: 1.2580800065610596e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 8000 Mean_loss: 7.598789532785276e-07 , Mean Acc: 0.9994651691784229 , Min Acc: 0.9687321949239741 \n",
            ", Mean Accs Foc [0.99981492 0.99943396 0.99920413 0.99944102 0.99978978 0.99989506\n",
            " 0.99903728 0.99952241 0.99980487 0.9995034  0.99866611 0.99897875\n",
            " 0.99927399 0.99981823 0.99945617 0.99994331] \n",
            ", Min Accs Foc: [0.99297144 0.99095475 0.98937373 0.98266277 0.99428447 0.99719919\n",
            " 0.96873219 0.98670159 0.99562709 0.97285146 0.98760788 0.99512876\n",
            " 0.99592201 0.99742849 0.99575864 0.99943591] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 8100 , Mean_loss: 8.051489349386498e-07 , Mean_accuracy: 0.9994504346317448 , Min_accuracy: 0.9359074343449626 , Learning rate: 1.1493467696467265e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 8100 Mean_loss: 7.59562624297977e-07 , Mean Acc: 0.9994651446645437 , Min Acc: 0.9686025394502751 \n",
            ", Mean Accs Foc [0.99981427 0.9994337  0.99920415 0.99944216 0.99979    0.99989521\n",
            " 0.99903767 0.99952128 0.99980498 0.9995025  0.9986667  0.99897935\n",
            " 0.99927529 0.99981816 0.9994563  0.99994329] \n",
            ", Min Accs Foc: [0.9929988  0.991095   0.98938567 0.98272102 0.99422283 0.99712923\n",
            " 0.96860254 0.98669536 0.99562309 0.97297832 0.98767537 0.99513143\n",
            " 0.99593136 0.99742848 0.99575748 0.99944099] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 8200 , Mean_loss: 8.038238014915014e-07 , Mean_accuracy: 0.9994507111244119 , Min_accuracy: 0.9279570168917366 , Learning rate: 1.0453658778440107e-06 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 8200 Mean_loss: 7.590187952134277e-07 , Mean Acc: 0.9994653544190782 , Min Acc: 0.9687035542319851 \n",
            ", Mean Accs Foc [0.99981526 0.99943518 0.99920428 0.99944222 0.99979041 0.99989529\n",
            " 0.99903566 0.99952252 0.99980529 0.99950196 0.99866692 0.99897988\n",
            " 0.99927603 0.99981825 0.99945509 0.99994197] \n",
            ", Min Accs Foc: [0.99303467 0.99125145 0.98940803 0.98269772 0.99417569 0.99716742\n",
            " 0.96870355 0.98668646 0.99560794 0.97292963 0.98753122 0.99513295\n",
            " 0.99593529 0.9974163  0.99578356 0.99946369] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 8300 , Mean_loss: 8.038275989528588e-07 , Mean_accuracy: 0.9994509477420686 , Min_accuracy: 0.9271570908817117 , Learning rate: 9.462640158525434e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 8300 Mean_loss: 7.59036007941513e-07 , Mean Acc: 0.999465555370662 , Min Acc: 0.9686196744618547 \n",
            ", Mean Accs Foc [0.99981539 0.9994351  0.99920445 0.99944125 0.99979064 0.99989511\n",
            " 0.99903699 0.99952297 0.99980538 0.99950268 0.99866718 0.9989805\n",
            " 0.99927699 0.99981816 0.9994564  0.99994242] \n",
            ", Min Accs Foc: [0.99318187 0.99134631 0.98937559 0.982806   0.99414703 0.99720681\n",
            " 0.96861967 0.98674315 0.99561405 0.97287415 0.98748429 0.99513205\n",
            " 0.99594071 0.99743393 0.99574232 0.99945528] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 8400 , Mean_loss: 8.032936118392434e-07 , Mean_accuracy: 0.999450977460371 , Min_accuracy: 0.9460565198998323 , Learning rate: 8.521619240256915e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 8400 Mean_loss: 7.585316282886644e-07 , Mean Acc: 0.9994656919093667 , Min Acc: 0.9686900904067169 \n",
            ", Mean Accs Foc [0.99981542 0.99943544 0.99920455 0.9994415  0.99979073 0.99989544\n",
            " 0.99903665 0.99952291 0.99980522 0.99950307 0.99866769 0.99898099\n",
            " 0.99927756 0.99981823 0.99945633 0.99994318] \n",
            ", Min Accs Foc: [0.99304964 0.99118774 0.98941304 0.98276364 0.99416962 0.99716298\n",
            " 0.96869009 0.98673345 0.99560599 0.97288651 0.98758161 0.99513162\n",
            " 0.99594238 0.99741671 0.99574901 0.99944415] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 8500 , Mean_loss: 8.039124283659068e-07 , Mean_accuracy: 0.9994508815958879 , Min_accuracy: 0.9363096466562962 , Learning rate: 7.631742512670284e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 8500 Mean_loss: 7.583828175730873e-07 , Mean Acc: 0.9994656860211285 , Min Acc: 0.9687178161484277 \n",
            ", Mean Accs Foc [0.9998154  0.9994354  0.99920469 0.99944225 0.99979049 0.99989539\n",
            " 0.99903711 0.99952228 0.99980513 0.99950249 0.99866789 0.9989813\n",
            " 0.99927841 0.99981829 0.99945616 0.99994326] \n",
            ", Min Accs Foc: [0.99299266 0.99106208 0.9893648  0.98272361 0.9942704  0.9971969\n",
            " 0.96871782 0.98667992 0.9956541  0.97289189 0.98760001 0.99514257\n",
            " 0.99594809 0.99742468 0.99574638 0.99943557] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 8600 , Mean_loss: 8.038414459426634e-07 , Mean_accuracy: 0.999451098054127 , Min_accuracy: 0.9212025804909665 , Learning rate: 6.794094153483115e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 8600 Mean_loss: 7.582195535435036e-07 , Mean Acc: 0.9994659556119778 , Min Acc: 0.9686835603114204 \n",
            ", Mean Accs Foc [0.99981543 0.99943538 0.99920471 0.99944232 0.99979043 0.99989547\n",
            " 0.99903737 0.99952416 0.99980506 0.99950328 0.9986679  0.9989818\n",
            " 0.99927867 0.99981828 0.99945668 0.999943  ] \n",
            ", Min Accs Foc: [0.99293389 0.99108015 0.9893781  0.9827968  0.99427751 0.99716698\n",
            " 0.96868356 0.98662192 0.99563947 0.97289188 0.98742107 0.99514271\n",
            " 0.99594649 0.99741056 0.99571165 0.99945251] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 8700 , Mean_loss: 8.022715838111065e-07 , Mean_accuracy: 0.9994512405056716 , Min_accuracy: 0.9433219730518754 , Learning rate: 6.009694708191232e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 8700 Mean_loss: 7.579865724573641e-07 , Mean Acc: 0.999465654974555 , Min Acc: 0.9688571764287633 \n",
            ", Mean Accs Foc [0.99981544 0.99943546 0.99920465 0.99944114 0.99979086 0.99989554\n",
            " 0.99903676 0.99952293 0.99980537 0.99950205 0.99866794 0.99898224\n",
            " 0.99927928 0.99981785 0.99945601 0.99994265] \n",
            ", Min Accs Foc: [0.99291969 0.99104929 0.98937109 0.98272949 0.99422769 0.99715776\n",
            " 0.96885718 0.9866612  0.99561056 0.97289764 0.98758895 0.99514247\n",
            " 0.99594982 0.99745134 0.99574512 0.99945467] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 8800 , Mean_loss: 8.027655028396e-07 , Mean_accuracy: 0.9994515583728856 , Min_accuracy: 0.930284046338481 , Learning rate: 5.279499846691252e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 8800 Mean_loss: 7.578896227010375e-07 , Mean Acc: 0.9994659902820501 , Min Acc: 0.9687361416400675 \n",
            ", Mean Accs Foc [0.99981526 0.99943573 0.99920486 0.99944282 0.99978984 0.9998954\n",
            " 0.99903786 0.99952311 0.9998055  0.99950304 0.99866841 0.99898248\n",
            " 0.99927958 0.99981817 0.99945644 0.99994328] \n",
            ", Min Accs Foc: [0.99289032 0.99096404 0.98938683 0.98271736 0.99426997 0.99722018\n",
            " 0.96873614 0.98670138 0.9956131  0.97289296 0.98755122 0.9951456\n",
            " 0.99594961 0.99743821 0.99573514 0.9994442 ] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 8900 , Mean_loss: 8.029407857396064e-07 , Mean_accuracy: 0.9994513227569661 , Min_accuracy: 0.9156433512815488 , Learning rate: 4.6043991989440177e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 8900 Mean_loss: 7.579193684971673e-07 , Mean Acc: 0.9994661419804776 , Min Acc: 0.968697705045306 \n",
            ", Mean Accs Foc [0.99981529 0.99943562 0.99920489 0.999442   0.99979085 0.99989561\n",
            " 0.9990379  0.99952451 0.99980537 0.99950317 0.99866829 0.99898291\n",
            " 0.99928032 0.99981794 0.99945673 0.99994312] \n",
            ", Min Accs Foc: [0.99284755 0.99093002 0.98940257 0.98276986 0.99427636 0.99718532\n",
            " 0.96869771 0.98661885 0.99563094 0.97289837 0.98759466 0.99514629\n",
            " 0.99595483 0.99745032 0.99572227 0.99945026] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 9000 , Mean_loss: 8.035212131844198e-07 , Mean_accuracy: 0.999451104535537 , Min_accuracy: 0.928462177730083 , Learning rate: 3.985215271097538e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 9000 Mean_loss: 7.575577238238052e-07 , Mean Acc: 0.9994661559014029 , Min Acc: 0.9687203061875338 \n",
            ", Mean Accs Foc [0.99981569 0.99943576 0.99920497 0.99944241 0.99979083 0.99989548\n",
            " 0.99903759 0.99952394 0.99980551 0.99950298 0.99866847 0.99898311\n",
            " 0.99928082 0.9998183  0.9994566  0.99994331] \n",
            ", Min Accs Foc: [0.99286395 0.9909345  0.98938581 0.98267524 0.99427868 0.99718828\n",
            " 0.96872031 0.98670855 0.99561828 0.97291665 0.98758485 0.99515154\n",
            " 0.99595857 0.99742252 0.99575322 0.99943419] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 9100 , Mean_loss: 8.038250084133169e-07 , Mean_accuracy: 0.9994511653913375 , Min_accuracy: 0.9217108709817441 , Learning rate: 3.4227024433899004e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 9100 Mean_loss: 7.576491338921853e-07 , Mean Acc: 0.9994660930326806 , Min Acc: 0.9687314137905453 \n",
            ", Mean Accs Foc [0.99981552 0.9994353  0.99920492 0.99944232 0.99979037 0.99989555\n",
            " 0.99903796 0.99952347 0.99980551 0.99950372 0.9986683  0.99898344\n",
            " 0.99928147 0.99981835 0.99945595 0.99994305] \n",
            ", Min Accs Foc: [0.99295037 0.99102174 0.98938485 0.98276214 0.99428845 0.99718062\n",
            " 0.96873141 0.98673359 0.99562513 0.97291716 0.98761398 0.99515062\n",
            " 0.99596396 0.9974202  0.99577482 0.9994495 ] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 9200 , Mean_loss: 8.03760734055269e-07 , Mean_accuracy: 0.9994510857402281 , Min_accuracy: 0.9419279503674627 , Learning rate: 2.917546051053215e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 9200 Mean_loss: 7.575001503154373e-07 , Mean Acc: 0.9994661719482044 , Min Acc: 0.9687309130328577 \n",
            ", Mean Accs Foc [0.99981576 0.99943593 0.99920497 0.99944147 0.99979095 0.9998957\n",
            " 0.9990377  0.99952416 0.99980552 0.99950301 0.9986686  0.99898363\n",
            " 0.99928167 0.99981826 0.9994566  0.99994315] \n",
            ", Min Accs Foc: [0.99296931 0.99098184 0.98939022 0.98277124 0.99428855 0.99717566\n",
            " 0.96873091 0.9867146  0.99560949 0.97290026 0.98756166 0.99515354\n",
            " 0.99596388 0.99743423 0.99574329 0.99944653] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 9300 , Mean_loss: 8.027392005863622e-07 , Mean_accuracy: 0.999451436820109 , Min_accuracy: 0.9163611939284504 , Learning rate: 2.4703615493381745e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 9300 Mean_loss: 7.573261460561003e-07 , Mean Acc: 0.9994661874484292 , Min Acc: 0.9688357244760238 \n",
            ", Mean Accs Foc [0.99981593 0.99943585 0.99920502 0.99944178 0.99979086 0.99989557\n",
            " 0.99903732 0.99952424 0.99980542 0.99950329 0.99866868 0.99898389\n",
            " 0.99928185 0.99981823 0.99945648 0.99994315] \n",
            ", Min Accs Foc: [0.99297941 0.99108407 0.98938241 0.98272109 0.99424563 0.99718718\n",
            " 0.96883572 0.98669681 0.99562896 0.97288199 0.98761314 0.99515234\n",
            " 0.99596384 0.99743437 0.99575324 0.99944441] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 9400 , Mean_loss: 8.019713944388091e-07 , Mean_accuracy: 0.9994516843483098 , Min_accuracy: 0.9456318534259376 , Learning rate: 2.0816937636766183e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 9400 Mean_loss: 7.573899665129589e-07 , Mean Acc: 0.9994662546249697 , Min Acc: 0.9687353673205427 \n",
            ", Mean Accs Foc [0.99981592 0.99943588 0.99920501 0.99944297 0.99979054 0.99989567\n",
            " 0.99903791 0.99952334 0.99980554 0.99950324 0.99866866 0.99898403\n",
            " 0.99928215 0.99981834 0.99945636 0.99994331] \n",
            ", Min Accs Foc: [0.99291667 0.99093297 0.98939092 0.9826823  0.99430749 0.99718408\n",
            " 0.96873537 0.98669123 0.99561799 0.9729234  0.98755407 0.99515471\n",
            " 0.9959655  0.99742174 0.99575585 0.99943824] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 9500 , Mean_loss: 8.014932550291455e-07 , Mean_accuracy: 0.999451932611901 , Min_accuracy: 0.9273950847177777 , Learning rate: 1.752016225895703e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 9500 Mean_loss: 7.571380817432932e-07 , Mean Acc: 0.9994662738456747 , Min Acc: 0.9687954332018341 \n",
            ", Mean Accs Foc [0.99981597 0.99943597 0.99920511 0.99944237 0.99979091 0.99989569\n",
            " 0.99903731 0.99952397 0.99980555 0.99950337 0.99866864 0.99898413\n",
            " 0.99928248 0.99981834 0.99945644 0.9999433 ] \n",
            ", Min Accs Foc: [0.99296259 0.99104309 0.98937985 0.98273018 0.99428623 0.99716501\n",
            " 0.96879543 0.98670657 0.99562269 0.97294787 0.98757023 0.99515686\n",
            " 0.99596758 0.99742511 0.99574194 0.99944069] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 9600 , Mean_loss: 8.015651880799996e-07 , Mean_accuracy: 0.9994518165097875 , Min_accuracy: 0.9316752698707969 , Learning rate: 1.4817305972922725e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 9600 Mean_loss: 7.571251724146387e-07 , Mean Acc: 0.999466379918708 , Min Acc: 0.9687493915641542 \n",
            ", Mean Accs Foc [0.99981595 0.99943603 0.99920508 0.99944203 0.99979094 0.99989572\n",
            " 0.99903786 0.99952456 0.99980553 0.99950335 0.99866898 0.9989843\n",
            " 0.99928262 0.99981836 0.99945678 0.99994329] \n",
            ", Min Accs Foc: [0.99297018 0.99101944 0.98939708 0.98271158 0.99427592 0.99716819\n",
            " 0.96874939 0.98667597 0.9956358  0.97289802 0.98759524 0.99515768\n",
            " 0.9959681  0.99741825 0.99574125 0.99943894] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 9700 , Mean_loss: 8.017176292739552e-07 , Mean_accuracy: 0.9994517404959191 , Min_accuracy: 0.929287550940436 , Learning rate: 1.2711661792704723e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 9700 Mean_loss: 7.571278545347963e-07 , Mean Acc: 0.9994663572632406 , Min Acc: 0.9687894579520434 \n",
            ", Mean Accs Foc [0.99981594 0.99943607 0.99920514 0.99944256 0.99979087 0.99989569\n",
            " 0.99903798 0.99952389 0.99980559 0.99950325 0.9986688  0.99898439\n",
            " 0.99928278 0.99981832 0.99945647 0.99994327] \n",
            ", Min Accs Foc: [0.99284709 0.99090281 0.98938492 0.98267104 0.99433015 0.99718486\n",
            " 0.96878946 0.9866529  0.99562003 0.97292154 0.98759838 0.99515819\n",
            " 0.99596865 0.99741992 0.99574514 0.99944205] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 9800 , Mean_loss: 8.006604215166634e-07 , Mean_accuracy: 0.9994518930949766 , Min_accuracy: 0.9376846836603706 , Learning rate: 1.1205795121387023e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 9800 Mean_loss: 7.570635403112314e-07 , Mean Acc: 0.9994664683031974 , Min Acc: 0.9687041826382288 \n",
            ", Mean Accs Foc [0.99981599 0.99943608 0.99920519 0.99944236 0.99979096 0.99989573\n",
            " 0.99903845 0.99952432 0.99980551 0.99950345 0.99866897 0.99898452\n",
            " 0.99928294 0.99981833 0.9994568  0.99994327] \n",
            ", Min Accs Foc: [0.99290193 0.99097241 0.98939308 0.98270916 0.99429379 0.99718777\n",
            " 0.96870418 0.98667875 0.99562617 0.97289691 0.98761646 0.99515805\n",
            " 0.99596941 0.99741573 0.9957319  0.99944276] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 9900 , Mean_loss: 8.032821066573522e-07 , Mean_accuracy: 0.9994514403725532 , Min_accuracy: 0.9265388395860861 , Learning rate: 1.0301540625547597e-07 \n",
            "\n",
            "EVALUATION:\n",
            " Iteration: 9900 Mean_loss: 7.569422958454995e-07 , Mean Acc: 0.9994663445866256 , Min Acc: 0.9688084478922945 \n",
            ", Mean Accs Foc [0.99981599 0.99943625 0.9992052  0.99944153 0.99979106 0.99989578\n",
            " 0.99903755 0.99952431 0.99980555 0.99950334 0.99866907 0.99898463\n",
            " 0.9992831  0.99981838 0.99945623 0.99994323] \n",
            ", Min Accs Foc: [0.99295151 0.9910427  0.98937788 0.98277448 0.99426897 0.99717616\n",
            " 0.96880845 0.98665449 0.99562616 0.97290802 0.98757948 0.99515724\n",
            " 0.99597    0.99742087 0.99576155 0.99944475] \n",
            "\n",
            "TRAINING:\n",
            " Iteration: 10000 , Mean_loss: 8.000891861024309e-07 , Mean_accuracy: 0.9994520675153657 , Min_accuracy: 0.9359514766720807 , Learning rate: 1e-07 \n",
            "\n",
            "Minimum loss attained in evaluation: 7.569422958454995e-07\n",
            "Maximum mean accuracy attained in evaluation: 0.9994663445866256\n",
            "Maximum min accuracy attained in evaluation: 0.9690229113705775\n",
            "Time Elapsed for Full Experiment: 52.34950666427612 minutes\n"
          ]
        }
      ],
      "source": [
        "final_train_state = run_experiment(Model(), config)\n",
        "\n",
        "#DISCONNECT SESSION (uncomment next 2 lines if you do large run)\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Fz-RH8Wt23Rz",
        "cygQcsHYIiDN",
        "1pceB-eSEbcG",
        "JYEia3OD-Lij",
        "wEG-xLNZQJLY",
        "hIKEQKv2HtXS",
        "duUIS9EaCIZo"
      ],
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1QW4LOfJ21C_dgXh3C3UXHawi1YgzYTjE",
      "authorship_tag": "ABX9TyNnIhUoa/l7IFheh6p4tlDd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}